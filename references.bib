@misc{360i2011,
abstract = {There is so much emphasis on monitoring and analyzing written communication across the web and insocial media. Online listening has become a standard for many companies and brands, but what is thenext frontier? What are we missing? Photos that consumers upload and share amongstthemselves are a major missing piece of the online listening pie. The numbers speak for themselves. By the end of 2010, more than 80 billion photos were uploadedacross a variety of social picture platforms (source: Pixable;Facebook Photo Trends). At least once amonth, 52% of mobile users take photos with their phones and 19% upload their photos to the web atleast once a month (source: comScore MobiLens January 2011: 3 month average). Photo sharingbehavior amongst consumers is especially important to look at when creating marketing programstargeted towards women; they are twice as likely than their male counterparts to upload, tag and viewphotos (source: Pixable;Facebook Photo Trends).The analysis that follows looks specifically at photos shared by consumers relating to food, eating, andmeals. The food photo sharing phenomenon is in full swing with new tools popping up all the time, suchas Foodspotting, Fiddme, Eat.ly, and the recently added photo capabilities to Foursquare. Showing - not just telling - others what you’re eating is becoming mainstream.In this POV we explore the habits and motivations of people sharing food photos online by examining thecontent of UGC photos and what this phenomenon means for food-related marketers and brands.},
author = {360i},
howpublished = {http://blog.360i.com/reports/consumer-insights-food},
pages = {8},
title = {{Online Food and Photo Sharing Trends}},
url = {http://blog.360i.com/reports/consumer-insights-food},
year = {2011}
}
@article{Abbeel2006,
author = {Abbeel, Pieter and Quigley, Morgan and Ng, Andrew Y.},
file = {:Users/smcgregor/Documents/Mendeley Desktop/AbbeelQuigleyNg_uimirl_ICML2006.pdf:pdf},
journal = {International Conference on Machine Learning},
keywords = {modeling},
mendeley-tags = {modeling},
pages = {1--8},
title = {{Using inaccurate models in reinforcement learning}},
url = {http://dl.acm.org/citation.cfm?id=1143845},
year = {2006}
}
@article{Adams2009,
abstract = {Traditionally, computing has meant calculating results and then storing those results for later use. Unfortunately, committing large volumes of rarely used data to storage wastes space and energy, making it a very expensive strategy. Cloud computing, with its readily available and flexibly allocatable computing resources, suggests an alternative: storing the provenance data, and means to recomputing results as needed. While computation and storage are equivalent, finding the balance between the two that maximizes efficiency is difficult. One of the fundamental challenges of this issue is rooted in the knowledge gap separating the users and the cloud administratorsneither has a completely informed view. Users have a semantic understanding of their data, while administrators have an understanding of the clouds underlying structure. We detail the user knowledge and system knowledge needed to construct a comprehensive cost model for analyzing the trade-off between storing a result and regenerating a result, allowing users and administrators to make an informed costbenefit analysis. 1},
author = {Adams, Ian F and Long, Darrell D E and Miller, Ethan L and Pasupathy, Shankar and Storer, Mark W},
file = {:Users/smcgregor/Documents/Mendeley Desktop/adams-hotcloud09.pdf:pdf},
journal = {Proceedings of the 2009 conference on Hot topics in cloud computing},
pages = {17},
title = {{Maximizing efficiency by trading storage for computation}},
year = {2009}
}
@article{Adkinson-Orellana2011,
author = {Adkinson-Orellana, L},
file = {:Users/smcgregor/Documents/Mendeley Desktop/2011_CLOSER_108_SafeGDocs.pdf:pdf},
journal = {Proc. of CLOSER},
pages = {439--444},
title = {{Sharing secure documents in the cloud-a secure layer for Google Docs}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:SHARING+SECURE+DOCUMENTS+IN+THE+CLOUD+A+Secure+Layer+for+Google+Docs#0},
year = {2011}
}
@inproceedings{Afzal2011,
author = {Afzal, Shehzad and Maciejewski, Ross and Ebert, Davis S.},
booktitle = {Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on},
file = {:Users/smcgregor/Documents/Mendeley Desktop/afzal.pdf:pdf},
isbn = {9781467300131},
pages = {191--200},
publisher = {IEEE},
title = {{Visual Analytics Decision Support Environment for Epidemic Modeling and Response Evaluation}},
year = {2011}
}
@article{Akhawe2012,
author = {Akhawe, Devdatta and Saxena, Prateek and Song, Dawn},
file = {:Users/smcgregor/Documents/Mendeley Desktop/sec12-final168.pdf:pdf},
journal = {Proceedings of the 21st USENIX Conference on Security Symposium},
title = {{Privilege separation in HTML5 applications}},
url = {https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final168.pdf},
year = {2012}
}
@inproceedings{albuquerque2010improving,
abstract = {Modern visualization methods are needed to cope with very high- dimensional data. Efficient visual analytical techniques are required to extract the information content in these data. The large number of possible projections for each method, which usually grow quadrat- ically or even exponentially with the number of dimensions, urges the necessity to employ automatic reduction techniques, automatic sorting or selecting the projections, based on their information- bearing content. Different quality measures have been successfully applied for several specified user tasks and established visualiza- tion techniques, like Scatterplots, Scatterplot Matrices or Parallel Coordinates. Many other popular visualization techniques exist, but due to the structural differences, the measures are not directly applicable to them and new approaches are needed. In this paper we propose new quality measures for three popular visualization methods: Radviz, Pixel-Oriented Displays and Table Lenses. Our experiments show that these measures efficiently guide the visual analysis task.},
author = {Albuquerque, G and Eisemann, M and Lehmann, D J and Theisel, H and Magnor, M},
booktitle = {Proceedings of the IEEE Symposium on Visual Analytics Science and Technology (IEEE VAST)},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Albuquerque et al. - 2010 - Improving the visual analysis of high-dimensional datasets using quality measures.pdf:pdf},
title = {{Improving the visual analysis of high-dimensional datasets using quality measures}},
volume = {10},
year = {2010}
}
@article{Andoni2008,
abstract = {The Earth Mover Distance (EMD) between two equal-size sets of points in ℝd is defined to be the minimum cost of a bipartite matching between the two pointsets. It is a natural metric for comparing sets of features, and as such, it has received significant interest in computer vision. Motivated by recent developments in that area, we address computational problems involving EMD over high-dimensional pointsets. A natural approach is to embed the EMD metric into l1, and use the algorithms designed for the latter space. However, Khot and Naor [KN06] show that any embedding of EMD over the d-dimensional Hamming cube into l1 must incur a distortion $\Omega$(d), thus practically losing all distance information. We circumvent this roadblock by focusing on sets with cardinalities upper-bounded by a parameter s, and achieve a distortion of only O(log s · log d). Since in applications the feature sets have bounded size, the resulting distortion is much smaller than the $\Omega$(d) lower bound. Our approach is quite general and easily extends to EMD over ℝd. We then provide a strong lower bound on the multi-round communicatic complexity of estimating EMD, which in particular strengthens the known non-embeddability result of [KN06]. Our bound exhibits a smooth tradeoff between approximation and communication, and for example implies that every algorithm that estimates EMD using constant size sketches can only achieve $\omega$(log s) approximation.},
author = {Andoni, Alexandr and Indyk, Piotr and Krauthgamer, Robert},
file = {:Users/smcgregor/Documents/Mendeley Desktop/10.1.1.478.2750.pdf:pdf},
isbn = {9780898716474},
journal = {Proceedings of the nineteenth annual ACM-SIAM symposium on Discrete algorithms},
pages = {343--352},
title = {{Earth mover distance over high-dimensional spaces}},
url = {http://portal.acm.org/citation.cfm?id=1347082.1347120},
year = {2008}
}
@article{Anupam1998,
author = {Anupam, Vinod and Mayer, Alain},
file = {:Users/smcgregor/Documents/Mendeley Desktop/anupam.pdf:pdf},
journal = {Proceedings of the 7th USENIX Security Symposium},
title = {{Security of Web Browser Scripting Languages: Vulnerabilities, Attacks, and Remedies}},
url = {http://www.usenix.org/publications/library/proceedings/sec98/full_papers/anupam/anupam.pdf},
year = {1998}
}
@article{Arca2013,
abstract = {Fuel treatment is considered a suitable way to mitigate the hazard related to potential wildfires on a landscape. However, designing an optimal spatial layout of treatment units represents a difficult optimization problem. In fact, budget constraints, the probabilistic nature of fire spread and interactions among the different area units composing the whole treatment, give rise to challenging search spaces on typical landscapes. In this paper we formulate such optimization problem with the objective of minimizing the extension of land characterized by high fire hazard. Then, we propose a computational approach that leads to a spatially-optimized treatment layout exploiting Tabu Search and General-Purpose computing on Graphics Processing Units (GPGPU). Using an application example, we also show that the proposed methodology can provide high-quality design solutions in low computing time.},
author = {Arca, Bachisio and Ghisu, Tiziano and Spataro, William and Trunfio, Giuseppe a.},
doi = {10.1016/j.procs.2013.05.262},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1-s2.0-S1877050913004055-main.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {cellular automata,fuel treatment,gpgpu,optimization,simulation},
pages = {966--975},
publisher = {Elsevier B.V.},
title = {{GPU-accelerated Optimization of Fuel Treatments for Mitigating Wildfire Hazard}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050913004055},
volume = {18},
year = {2013}
}
@misc{Arthur2010,
abstract = {A piece of software called Haystack, which claimed to be an "anti-censorship" system to let people in Iran use the internet anonymously, has been withdrawn by its author after experts raised serious questions about its security. The author, Austin Heap, a 26-year-old programmer from San Francisco, has been roundly criticised by professionals who complain that he has never allowed them access to the program's code – which they say is a necessity with security software to check whether it can do what it claims. After having obtained access by other means, the experts now say that instead of making users anonymous, it could reveal key information about them to the Iranian authorities. In a post on his blog on Monday, Heap says that in the "vigorous debate" about Haystack's security "many of the points made were valid" and that users have been asked to stop using it. Daniel Colascione, who worked with Heap and says he came up with the "Haystack" name, tweeted on Tuesday that the Censorship Research Center (CRC) that he co-created with Heap to host Haystack is now being wound down. But he also maintained that the software that has been criticised was not intended for widespread use, and was only a test version. In March the US government granted Haystack an export licence, required for "sensitive" cryptographic software, following a fast-track approval process which does not seem to have included independent verification of its security. Haystack, and Heap, won plaudits from a number of organisations after the software's release last year. Its genesis followed the Iranian protests at the presidential election there in 2009, which was widely felt to have been rigged. Many people there tried to use mobile phones and services such as Twitter to organise protests, but there were also fears they could be traced by the authorities, using software in mobile transmission systems sold by western companies such as Nokia. The idea of Haystack was to make communications by its users look like innocent – rather than sensitive – information. Heap developed it so that Iranian users could use email and web services such as Twitter without the Iranian authorities being able to trace them.},
author = {Arthur, Charles},
booktitle = {The Guardian},
howpublished = {http://www.guardian.co.uk/technology/2010/sep/17/haystack-software-security-concerns},
title = {{Haystack 'Anti-Censorship' Software Withdrawn Over Security Concerns}},
url = {http://www.guardian.co.uk/technology/2010/sep/17/haystack-software-security-concerns},
year = {2010}
}
@book{Asimov1950,
address = {Garden City, NY},
author = {Asimov, Isaac},
pages = {218},
publisher = {Doubleday Science Fiction},
title = {{I, Robot.}},
year = {1950}
}
@misc{AssociatedPress2012,
abstract = {China’s two biggest microblog sites resumed normal service on Tuesday after a three-day ban on posting comments that drew complaints about censorship amid the country’s worst high-level political crisis in years. The suspension of the comment function on Sina’s Weibo.com and Tencent’s t.qq.com followed a flurry of rumors online about the downfall of a prominent Communist Party figure, Bo Xilai. The authorities have closed a dozen Web sites and detained six people for circulating rumors of a coup that rattled Beijing. The two companies said in statements on their sites that the shutdown was aimed at “cleaning up” illegal and harmful information posted on some microblogs but gave no details.},
address = {New York, New York, USA},
author = {{Associated Press}},
booktitle = {New York Times},
howpublished = {http://www.nytimes.com/2012/04/04/world/asia/china-microblog-commenting-restored.html},
month = {apr},
pages = {1},
title = {{China: Microblog Commenting Restored}},
url = {http://www.nytimes.com/2012/04/04/world/asia/china-microblog-commenting-restored.html},
year = {2012}
}
@article{bachthaler2008continuous,
abstract = {Scatterplots are well established means of visualizing discrete data values with two data variables as a collection of discrete points. We aim at generalizing the concept of scatterplots to the visualization of spatially continuous input data by a continuous and dense plot. An example of a continuous input field is data defined on an n-D spatial grid with respective interpolation or reconstruction of in-between values. We propose a rigorous, accurate, and generic mathematical model of continuous scatterplots that considers an arbitrary density defined on an input field on an n-D domain and that maps this density to m-D scatterplots. Special cases are derived from this generic model and discussed in detail: scatterplots where the n-D spatial domain and the m-D data attribute domain have identical dimension, 1-D scatterplots as a way to define continuous histograms, and 2-D scatterplots of data on 3-D spatial grids. We show how continuous histograms are related to traditional discrete histograms and to the histograms of isosurface statistics. Based on the mathematical model of continuous scatterplots, respective visualization algorithms are derived, in particular for 2-D scatterplots of data from 3-D tetrahedral grids. For several visualization tasks, we show the applicability of continuous scatterplots. Since continuous scatterplots do not only sample data at grid points but interpolate data values within cells, a dense and complete visualization of the data set is achieved that scales well with increasing data set size. Especially for irregular grids with varying cell size, improved results are obtained when compared to conventional scatterplots. Therefore, continuous scatterplots are a suitable extension of a statistics visualization technique to be applied to typical data from scientific computation.},
author = {Bachthaler, S and Weiskopf, D},
file = {:Users/smcgregor/Documents/Mendeley Desktop/04658159.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1428--1435},
publisher = {IEEE},
title = {{Continuous scatterplots}},
volume = {14},
year = {2008}
}
@article{Bale2007,
author = {Bale, Kim and Chapman, Paul and Barraclough, Nick and Purdy, Jon and Aydin, Nizamettin and Dark, Paul},
doi = {10.1145/1331931.1331935},
issn = {1473-8716},
journal = {Information Visualization},
keywords = {Kaleidomaps,cyclic graphs,data mining,information visualization,multivariate time-series data},
month = {jun},
number = {2},
pages = {155--167},
title = {{Kaleidomaps: a new technique for the visualization of multivariate time-series data}},
url = {http://dl.acm.org/citation.cfm?id=1331931.1331935},
volume = {6},
year = {2007}
}
@article{Bandhakavi2011,
author = {Bandhakavi, Sruthi and Tiku, Nandit and Pittman, Wyatt and King, Samuel T. and Madhusudan, P. and Winslett, Marianne},
doi = {10.1145/1995376.1995398},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Bandhakavi.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {sep},
number = {9},
pages = {91},
title = {{Vetting Browser Extensions for Security Vulnerabilities with VEX}},
url = {http://dl.acm.org/citation.cfm?doid=1995376.1995398},
volume = {54},
year = {2011}
}
@article{Banerjee2005,
abstract = {A wide variety of distortion functions, such as squared Euclidean distance, Mahalanobis distance, Itakura-Saito distance and relative entropy, have been used for clustering. In this paper, we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergences. The proposed algorithms unify centroid-based parametric clustering approaches, such as classical kmeans, the Linde-Buzo-Gray (LBG) algorithm and information-theoretic clustering, which arise by special choices of the Bregman divergence. The algorithms maintain the simplicity and scalability of the classical kmeans algorithm, while generalizing the method to a large class of clustering loss functions. This is achieved by first posing the hard clustering problem in terms of minimizing the loss in Bregman information, a quantity motivated by rate distortion theory, and then deriving an iterative algorithm that monotonically decreases this loss. In addition, we show that there is a bijection between regular exponential families and a large class of Bregman divergences, that we call regular Bregman divergences. This result enables the development of an alternative interpretation of an efficient EM scheme for learning mixtures of exponential family distributions, and leads to a simple soft clustering algorithm for regular Bregman divergences. Finally, we discuss the connection between rate distortion theory and Bregman clustering and present an information theoretic analysis of Bregman clustering algorithms in terms of a trade-off between compression and loss in Bregman information.},
author = {Banerjee, Arindam},
doi = {10.1007/s10994-005-5825-6},
file = {:Users/smcgregor/Documents/Mendeley Desktop/banerjee05b.pdf:pdf},
isbn = {9780898715682},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {1705--1749},
pmid = {702854},
title = {{Clustering with Bregman Divergences}},
url = {http://portal.acm.org/citation.cfm?id=1194902},
volume = {6},
year = {2005}
}
@inproceedings{barlowe2008multivariate,
abstract = {Understanding multivariate relationships is an important task in multivariate data analysis. Unfortunately, existing multivariate visualization systems lose effectiveness when analyzing relationships among variables that span more than a few dimensions. We present a novel multivariate visual explanation approach that helps users interactively discover multivariate relationships among a large number of dimensions by integrating automatic numerical differentiation techniques and multidimensional visualization techniques. The result is an efficient workflow for multivariate analysis model construction, interactive dimension reduction, and multivariate knowledge discovery leveraging both automatic multivariate analysis and interactive multivariate data visual exploration. Case studies and a formal user study with a real dataset illustrate the effectiveness of this approach.},
author = {Barlowe, S and Zhang, T and Liu, Y and Yang, J and Jacobs, D},
booktitle = {Visual Analytics Science and Technology, 2008. VAST'08. IEEE Symposium on},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Barlowe et al. - 2008 - Multivariate visual explanation for high dimensional datasets.pdf:pdf},
organization = {IEEE},
pages = {147--154},
title = {{Multivariate visual explanation for high dimensional datasets}},
year = {2008}
}
@article{Barth2009,
author = {Barth, Adam and Jackson, Collin and Mitchell, John C.},
doi = {10.1145/1516046.1516066},
file = {:Users/smcgregor/Documents/Mendeley Desktop/barth.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {jun},
number = {6},
pages = {83},
title = {{Securing frame communication in browsers}},
url = {http://portal.acm.org/citation.cfm?doid=1516046.1516066},
volume = {52},
year = {2009}
}
@article{Baxter2001a,
author = {Baxter, Jonathan and Bartlett, Peter L},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1106.0665v1.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {319--350},
title = {{Infinite-Horizon Policy-Gradient Estimation}},
volume = {15},
year = {2001}
}
@article{Baxter2001,
annote = {I need to read the GPOMDP paper cited here.},
author = {Baxter, Jonathan and Bartlett, Peter L and Weaver, Lex},
file = {:Users/smcgregor/Documents/Mendeley Desktop/10.1.1.68.4960.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
pages = {351--381},
title = {{Experiments with Infinite-Horizon, Policy-Gradient Estimation}},
volume = {15},
year = {2001}
}
@article{Beato2011,
author = {Beato, Filipe and Kohlweiss, Markulf and Wouters, Karel},
file = {:Users/smcgregor/Documents/Mendeley Desktop/article-2029.pdf:pdf},
journal = {Privacy Enhancing Technologies},
pages = {211--225},
title = {{Scramble! your social network data}},
url = {http://www.springerlink.com/index/5331728M20933700.pdf},
year = {2011}
}
@article{doi:10.1080/00401706.1987.10488204,
abstract = {A dynamic graphical method is one in which a data analyst interacts in real time with a data display on a computer graphics terminal. Using a screen input device such as a mouse, the analyst can specify, in a visual way, points or regions on the display and cause aspects of the display to change nearly instantaneously. Brushing is a collection of dynamic methods for viewing multidimensional data. It is very effective when used on a scatterplot matrix, a rectangular array of all pairwise scatterplots of the variables. Four brushing operations—highlight, shadow highlight, delete, and label—are carried out by moving a mouse-controlled rectangle, called the brush, over one of the scatterplots. The effect of an operation appears simultaneously on all scatterplots. Three paint modes—transient, lasting, and undo—and the ability to change the shape of the brush allow the analyst to specify collections of points on which the operations are carried out. Brushing can be used in various ways or on certain types of data; these usages are called brush techniques and include the following: single-point and cluster linking, conditioning on a single variable, conditioning on two variables, subsetting with categorical variables, and stationarity probing of a time series.},
annote = {Brushing definition},
author = {Becker, Richard A and Cleveland, William S},
doi = {10.1080/00401706.1987.10488204},
journal = {Technometrics},
number = {2},
pages = {127--142},
title = {{Brushing Scatterplots}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/00401706.1987.10488204},
volume = {29},
year = {1987}
}
@book{Bellman1957,
address = {New Jersey},
author = {Bellman, Richard},
publisher = {Princeton University Press},
title = {{Dynamic Programming}},
year = {1957}
}
@article{bennett2011feature,
abstract = {We present a new framework for feature-based statistical analysis of large-scale scientific data and demonstrate its effectiveness by analyzing features from Direct Numerical Simulations (DNS) of turbulent combustion. Turbulent flows are ubiquitous and account for transport and mixing processes in combustion, astrophysics, fusion, and climate modeling among other disciplines. They are also characterized by coherent structure or organized motion, i.e. nonlocal entities whose geometrical features can directly impact molecular mixing and reactive processes. While traditional multi-point statistics provide correlative information, they lack nonlocal structural information, and hence, fail to provide mechanistic causality information between organized fluid motion and mixing and reactive processes. Hence, it is of great interest to capture and track flow features and their statistics together with their correlation with relevant scalar quantities, e.g. temperature or species concentrations. In our approach we encode the set of all possible flow features by pre-computing merge trees augmented with attributes, such as statistical moments of various scalar fields, e.g. temperature, as well as length-scales computed via spectral analysis. The computation is performed in an efficient streaming manner in a pre-processing step and results in a collection of meta-data that is orders of magnitude smaller than the original simulation data. This meta-data is sufficient to support a fully flexible and interactive analysis of the features, allowing for arbitrary thresholds, providing per-feature statistics, and creating various global diagnostics such as Cumulative Density Functions (CDFs), histograms, or time-series. We combine the analysis with a rendering of the features in a linked-view browser that enables scientists to interactively explore, visualize, and analyze the equivalent of one terabyte of simulation data. We highlight the utility of this new framework for combustion s- ience; however, it is applicable to many other science domains.},
author = {Bennett, J C and Krishnamoorthy, V and Liu, S and Grout, R W and Hawkes, E R and Chen, J H and Shepherd, J and Pascucci, V and Bremer, P T},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Bennett et al. - 2011 - Feature-Based Statistical Analysis of Combustion Simulation Data.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {1822--1831},
publisher = {IEEE},
title = {{Feature-Based Statistical Analysis of Combustion Simulation Data}},
volume = {17},
year = {2011}
}
@article{Bergner2011,
abstract = {In this paper we introduce paraglide, a visualization system designed for interactive exploration of parameter spaces of multi-variate simulation models. To get the right parameter configuration, model developers frequently have to go back and forth between setting parameters and qualitatively judging the outcomes of their model. During this process, they build up a grounded understanding of the parameter effects in order to pick the right setting. Current state-of-the-art tools and practices, however, fail to provide a systematic way of exploring these parameter spaces, making informed decisions about parameter settings a tedious and workload-intensive task. Paraglide endeavors to overcome this shortcoming by assisting the sampling of the parameter space and the discovery of qualitatively different model outcomes. This results in a decomposition of the model parameter space into regions of distinct behaviour. We developed paraglide in close collaboration with experts from three different domains, who all were involved in developing new models for their domain. We first analyzed current practices of six domain experts and derived a set of design requirements, then engaged in a longitudinal user-centered design process, and finally conducted three in-depth case studies underlining the usefulness of our approach.},
archivePrefix = {arXiv},
arxivId = {1110.5181},
author = {Bergner, Steven and Sedlmair, Michael and Nabi, Sareh and Saad, Ahmed and M{\"{o}}ller, Torsten},
eprint = {1110.5181},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1110.5181v1.pdf:pdf},
pages = {1--14},
title = {{Paraglide: Interactive Parameter Space Partitioning for Computer Simulations}},
url = {http://arxiv.org/abs/1110.5181},
year = {2011}
}
@article{bertini2011quality,
abstract = {In this paper, we present a systematization of techniques that use quality metrics to help in the visual exploration of meaningful patterns in high-dimensional data. In a number of recent papers, different quality metrics are proposed to automate the demanding search through large spaces of alternative visualizations (e.g., alternative projections or ordering), allowing the user to concentrate on the most promising visualizations suggested by the quality metrics. Over the last decade, this approach has witnessed a remarkable development but few reflections exist on how these methods are related to each other and how the approach can be developed further. For this purpose, we provide an overview of approaches that use quality metrics in high-dimensional data visualization and propose a systematization based on a thorough literature review. We carefully analyze the papers and derive a set of factors for discriminating the quality metrics, visualization techniques, and the process itself. The process is described through a reworked version of the well-known information visualization pipeline. We demonstrate the usefulness of our model by applying it to several existing approaches that use quality metrics, and we provide reflections on implications of our model for future research.},
author = {Bertini, E and Tatu, A and Keim, D},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06064985.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2203--2212},
publisher = {IEEE},
title = {{Quality metrics in high-dimensional data visualization: An overview and systematization}},
volume = {17},
year = {2011}
}
@inproceedings{Bertini2006,
abstract = {The definition and usage of quality metrics for Information Visualization techniques is still an immature field. Several proposals are available but a common view and understanding of this issue is still missing. This paper attempts a first step toward a visual quality metrics systematization, providing a general classification of both metrics and usage purposes. Moreover, the paper explores a quite neglected class of visual quality metrics, namely Feature Preservation Metrics, that allow for evaluating and improving in a novel way the effectiveness of basic Infovis techniques.},
author = {Bertini, Enrico and Santucci, Giuseppe},
booktitle = {Proceedings of the 2006 AVI workshop on BEyond time and errors novel evaluation methods for information visualization},
doi = {10.1145/1168149.1168159},
isbn = {1595935622},
pages = {1--5},
publisher = {ACM},
title = {{Visual quality metrics}},
url = {http://portal.acm.org/citation.cfm?id=1168159},
year = {2006}
}
@book{bertsekas1995dynamic,
author = {Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P},
number = {2},
publisher = {Athena Scientific Belmont, MA},
title = {{Dynamic programming and optimal control}},
volume = {1},
year = {1995}
}
@inproceedings{besenyei2011stegoweb,
author = {Besenyei, T and F{\"{o}}ldes, {\'{A}} M and Guly{\'{a}}s, G G and Imre, S},
booktitle = {SECURWARE 2011, The Fifth International Conference on Emerging Security Information, Systems and Technologies},
pages = {109--114},
title = {{StegoWeb: Towards the ideal private web content publishing tool}},
year = {2011}
}
@article{Besenyei2011,
author = {Besenyei, Tam{\'{a}}s and F{\"{o}}ldes, {\'{A}}M},
file = {:Users/smcgregor/Documents/Mendeley Desktop/securware_2011_6_10_30073.pdf:pdf},
isbn = {9781612080109},
journal = {SECURWARE 2011, The {\ldots}},
keywords = {-web},
number = {c},
pages = {109--114},
title = {{StegoWeb: Towards the ideal private web content publishing tool}},
url = {http://www.thinkmind.org/index.php?view=article&articleid=securware_2011_6_10_30073},
year = {2011}
}
@inproceedings{bhaniramka2000isosurfacing,
abstract = {Visualization algorithms have seen substantial improvements in the past several years. However, very few algorithms have been developed for directly studying data in dimensions higher than three. Most algorithms require a sampling in three-dimensions before applying any visualization algorithms. This sampling typically ignores vital features that may be present when examined in oblique cross-sections, and places an undo burden on system resources when animation through additional dimensions is desired. For time-varying data of large data sets, smooth animation is desired at interactive rates. We provide a fast Marching Cubes like algorithm for hypercubes of any dimension. To support this, we have developed a new algorithm to automatically generate the isosurface and triangulation tables for any dimension. This allows the efficient calculation of 4D isosurfaces, which can be interactively sliced to provide smooth animation or slicing through oblique hyperplanes. The former allows for smooth animation in a very compressed format. The latter provide better tools to study time-evolving features as they move downstream. We also provide examples in using this technique to show interval volumes or the sensitivity of a particular isovalue threshold.},
author = {Bhaniramka, P and Wenger, R and Crawfis, R},
booktitle = {Visualization 2000. Proceedings},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Bhaniramka, Wenger, Crawfis - 2000 - Isosurfacing in higher dimensions.pdf:pdf},
organization = {IEEE},
pages = {267--273},
title = {{Isosurfacing in higher dimensions}},
year = {2000}
}
@article{Bigham2007,
author = {Bigham, JP and Ladner, RE},
file = {:Users/smcgregor/Documents/Mendeley Desktop/p25-bigham.pdf:pdf},
isbn = {1595935908},
journal = {Proceedings of the 2007 international cross-disciplinary conference on Web accessibility (W4A)},
keywords = {accessmonkey,alternative text,greasemonkey,ing,web accessibility,web scripting,web transcod-},
pages = {25--34},
title = {{Accessmonkey: a collaborative scripting framework for web users and developers}},
url = {http://dl.acm.org/citation.cfm?id=1243441.1243452},
year = {2007}
}
@article{blaas2008extensions,
abstract = {Parallel coordinate plots (PCPs) are commonly used in information visualization to provide insight into multi-variate data. These plots help to spot correlations between variables. PCPs have been successfully applied to unstructured datasets up to a few millions of points. In this paper, we present techniques to enhance the usability of PCPs for the exploration of large, multi-timepoint volumetric data sets, containing tens of millions of points per timestep. The main difficulties that arise when applying PCPs to large numbers of data points are visual clutter and slow performance, making interactive exploration infeasible. Moreover, the spatial context of the volumetric data is usually lost. We describe techniques for preprocessing using data quantization and compression, and for fast GPU-based rendering of PCPs using joint density distributions for each pair of consecutive variables, resulting in a smooth, continuous visualization. Also, fast brushing techniques are proposed for interactive data selection in multiple linked views, including a 3D spatial volume view. These techniques have been successfully applied to three large data sets: Hurricane Isabel (Vis'04 contest), the ionization front instability data set (Vis'08 design contest), and data from a large-eddy simulation of cumulus clouds. With these data, we show how PCPs can be extended to successfully visualize and interactively explore multi-timepoint volumetric datasets with an order of magnitude more data points.},
author = {Blaas, J and Botha, C P and Post, F H},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Blaas, Botha, Post - 2008 - Extensions of parallel coordinates for interactive exploration of large multi-timepoint data sets.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1436--1451},
publisher = {IEEE},
title = {{Extensions of parallel coordinates for interactive exploration of large multi-timepoint data sets}},
volume = {14},
year = {2008}
}
@inproceedings{Booshehrian2012,
abstract = {In this design study, we present an analysis and abstraction of the data and task in the domain of fisheries management, and the design and implementation of the Vismon tool to address the identified requirements. Vismon was designed to support sophisticated data analysis of simulation results by managers who are highly knowledgeable about the fisheries domain but not experts in simulation software and statistical data analysis. The previous workflow required the scientists who built the models to spearhead the analysis process. The features of Vismon include sensitivity analysis, comprehensive and global trade-offs analysis, and a staged approach to the visualization of the uncertainty of the underlying simulation model. The tool was iteratively refined through a multi-year engagement with fisheries scientists with a two-phase approach, where an initial diverging experimentation phase to test many alternatives was followed by a converging phase where the set of multiple linked views that proved effective were integrated together in a useable way. Several fisheries scientists have used Vismon to communicate with policy makers, and it is scheduled for deployment to policy makers in Alaska.},
annote = {http://www.vismon.org/index.html},
author = {Booshehrian, Maryam and M{\"{o}}ller, Torsten and Peterman, Randall M. and Munzner, Tamara},
booktitle = {Proceedings of Eurographics Conference on Visualization 2012 (EuroVis 2012)},
pages = {1235--1244},
publisher = {Computer Graphics Forum},
title = {{Vismon: Facilitating Analysis of Trade-Offs, Uncertainty, and Sensitivity In Fisheries Management Decision Making}},
year = {2012}
}
@article{Boots2011,
abstract = {A central problem in artificial intelligence is that of planning to maximize future reward under uncertainty in a partially observable environment. In this paper we propose and demonstrate a novel algorithm which accurately learns a model of such an environment directly from sequences of action-observation pairs. We then close the loop from observations to actions by planning in the learned model and recovering a policy which is near-optimal in the original environment. Specifically, we present an efficient and statistically consistent spectral algorithm for learning the parameters of a Predictive State Representation (PSR). We demonstrate the algorithm by learning a model of a simulated high-dimensional, vision-based mobile robot planning task, and then perform approximate point-based planning in the learned PSR. Analysis of our results shows that the algorithm learns a state space which efficiently captures the essential features of the environment. This representation allows accurate prediction with a small number of parameters, and enables successful and efficient planning.},
archivePrefix = {arXiv},
arxivId = {0912.2385},
author = {Boots, Byron and Siddiqi, Sajid M. and Gordon, Geoffrey J.},
eprint = {0912.2385},
file = {:Users/smcgregor/Documents/Mendeley Desktop/boots-siddiqi-gordon-closing-loop-psrs.pdf:pdf},
journal = {The International Journal of Robotics Research},
number = {7},
pages = {954--966},
title = {{Closing the Learning-Planning Loop with Predictive State Representations}},
url = {http://arxiv.org/abs/0912.2385},
volume = {30},
year = {2011}
}
@misc{Bostock2012,
author = {Bostock, Michael},
title = {{Data Driven Documents}},
url = {http://d3js.org/},
urldate = {0012-12-12},
year = {2012}
}
@article{Boyd2008,
abstract = {Not all Facebook users appreciated the September 2006 launch of the `News Feeds' feature. Concerned about privacy implications, thousands of users vocalized their discontent through the site itself, forcing the company to implement privacy tools. This essay examines the privacy concerns voiced following these events. Because the data made easily visible were already accessible with effort, what disturbed people was primarily the sense of exposure and invasion. In essence, the `privacy trainwreck' that people experienced was the cost of social convergence.},
author = {Boyd, Danah},
file = {:Users/smcgregor/Documents/Mendeley Desktop/645972.pdf:pdf},
journal = {Convergence: The International Journal of Research into New Media Technologies},
number = {1},
pages = {13--20},
title = {{Facebook's Privacy Train Wreck}},
volume = {14},
year = {2008}
}
@article{Brehmer2013,
abstract = {The considerable previous work characterizing visualization usage has focused on low-level tasks or interactions and high-level tasks, leaving a gap between them that is not addressed. This gap leads to a lack of distinction between the ends and means of a task, limiting the potential for rigorous analysis. We contribute a multi-level typology of visualization tasks to address this gap, distinguishing why and how a visualization task is performed, as well as what the task inputs and outputs are. Our typology allows complex tasks to be expressed as sequences of interdependent simpler tasks, resulting in concise and flexible descriptions for tasks of varying complexity and scope. It provides abstract rather than domain-specific descriptions of tasks, so that useful comparisons can be made between visualization systems targeted at different application domains. This descriptive power supports a level of analysis required for the generation of new designs, by guiding the translation of domain-specific problems into abstract tasks, and for the qualitative evaluation of visualization usage. We demonstrate the benefits of our approach in a detailed case study, comparing task descriptions from our typology to those derived from related work. We also discuss the similarities and differences between our typology and over two dozen extant classification systems and theoretical frameworks from the literatures of visualization, human-computer interaction, information retrieval, communications, and cartography.},
author = {Brehmer, Matthew and Munzner, Tamara},
doi = {10.1109/TVCG.2013.124},
file = {:Users/smcgregor/Documents/Mendeley Desktop/brehmer_infovis13.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Typology,qualitative evaluation,task and requirements analysis,visualization models},
number = {12},
pages = {2376--2385},
pmid = {24051804},
title = {{A multi-level typology of abstract visualization tasks}},
volume = {19},
year = {2013}
}
@article{Broeksema2013,
abstract = {We present a visual analytics solution designed to address prevalent issues in the area of Operational Decision Management (ODM). In ODM, which has its roots in Artificial Intelligence (Expert Systems) and Management Science, it is increasingly important to align business decisions with business goals. In our work, we consider decision models (executable models of the business domain) as ontologies that describe the business domain, and production rules that describe the business logic of decisions to be made over this ontology. Executing a decision model produces an accumulation of decisions made over time for individual cases. We are interested, first, to get insight in the decision logic and the accumulated facts by themselves. Secondly and more importantly, we want to see how the accumulated facts reveal potential divergences between the reality as captured by the decision model, and the reality as captured by the executed decisions. We illustrate the motivation, added value for visual analytics, and our proposed solution and tooling through a business case from the car insurance industry.},
author = {Broeksema, Bertjan and Baudel, Thomas and Telea, Alex and Crisafulli, Paolo},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06634184.pdf:pdf},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {12},
pages = {1972--1981},
title = {{Decision Exploration Lab: A Visual Analytics Solution for Decision Management}},
volume = {19},
year = {2013}
}
@article{Brooks1995,
author = {Brooks, Frederick P},
file = {:Users/smcgregor/Documents/Mendeley Desktop/no-silver-bullet.pdf:pdf},
pages = {1--16},
title = {{No Silver Bullet — Essence and Accident in Software Engineering}},
year = {1995}
}
@inproceedings{Brown2012,
abstract = {The world's corpora of data grow in size and complexity every day, making it increasingly difficult for experts to make sense out of their data. Although machine learning offers algorithms for finding patterns in data automatically, they often require algorithm-specific parameters, such as an appropriate distance function, which are outside the purview of a domain expert. We present a system that allows an expert to interact directly with a visual representation of the data to define an appropriate distance function, thus avoiding direct manipulation of obtuse model parameters. Adopting an iterative approach, our system first assumes a uniformly weighted Euclidean distance function and projects the data into a two-dimensional scatterplot view. The user can then move incorrectly-positioned data points to locations that reflect his or her understanding of the similarity of those data points relative to the other data points. Based on this input, the system performs an optimization to learn a new distance function and then re-projects the data to redraw the scatter-plot. We illustrate empirically that with only a few iterations of interaction and optimization, a user can achieve a scatterplot view and its corresponding distance function that reflect the user's knowledge of the data. In addition, we evaluate our system to assess scalability in data size and data dimension, and show that our system is computationally efficient and can provide an interactive or near-interactive user experience.},
author = {Brown, Eli T. and Liu, Jingjing and Brodley, Carla E. and Chang, Remco},
booktitle = {IEEE Conference on Visual Analytics Science and Technology 2012},
doi = {10.1109/VAST.2012.6400486},
file = {:Users/smcgregor/Documents/Mendeley Desktop/VAST2012-DisFunction.pdf:pdf},
isbn = {9781467347532},
pages = {83--92},
title = {{Dis-function: Learning distance functions interactively}},
year = {2012}
}
@article{Buja1999,
author = {Buja, A and Cook, D and Asimov, D and Hurley, C},
file = {:Users/smcgregor/Desktop/Theory and Computational Methods for Dynamic Projections in High-Dimensional Data Visualization.pdf:pdf},
journal = {ATT Labs-Research},
title = {{Theory and computational methods for dynamic projections in high-dimensional data visualization}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.9039&rep=rep1&type=pdf},
year = {1999}
}
@article{Buja1986,
author = {Buja, Andreas and Asimov, D},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Grand Tour Methods- An Outline.pdf:pdf},
journal = {Computing Science and Statistics},
pages = {63--67},
title = {{Grand tour methods: an outline}},
url = {ftp://212.219.56.134/sites/lib.stat.cmu.edu/general/XGobi/papers/outline86.ps.gz},
year = {1986}
}
@article{Burkhart2010,
author = {Burkhart, Martin and Strasser, Mario and Many, Dilip and Dimitropoulos, Xenofontas},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Burkhart.pdf:pdf},
title = {{SEPIA: Privacy-preserving aggregation of multi-domain network events and statistics}},
url = {http://www.usenix.org/event/sec10/tech/full_papers/Burkhart.pdf},
year = {2010}
}
@article{Camenisch2012,
author = {Camenisch, Jan},
doi = {10.1016/j.comnet.2012.10.012},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1-s2.0-S1389128612003660-main.pdf:pdf},
issn = {13891286},
journal = {Computer Networks},
month = {dec},
number = {18},
pages = {3834--3848},
publisher = {Elsevier B.V.},
title = {{Information privacy?!}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1389128612003660},
volume = {56},
year = {2012}
}
@article{Carlini2012,
author = {Carlini, Nicholas and Felt, AP and Wagner, David},
file = {:Users/smcgregor/Documents/Mendeley Desktop/sec12-final177.pdf:pdf},
journal = {Proceedings of the 21st USENIX Conference on Security Symposium},
title = {{An Evaluation of the Google Chrome Extension Security Architecture}},
url = {https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final177_0.pdf},
year = {2012}
}
@article{Chawla2013,
author = {Chawla, Sanjay and Gionis, Aristides},
file = {:Users/smcgregor/Documents/Mendeley Desktop/kmean--outliers.pdf:pdf},
journal = {SIAM International Conference on Data Mining (SDM)},
title = {{k -means-- : A unified approach to clustering and outlier detection}},
year = {2013}
}
@inproceedings{choo2010ivisclassifier,
abstract = {We present an interactive visual analytics system for classification, iVisClassifier, based on a supervised dimension reduction method, linear discriminant analysis (LDA). Given high-dimensional data and associated cluster labels, LDA gives their reduced dimensional representation, which provides a good overview about the cluster structure. Instead of a single two- or three-dimensional scatter plot, iVisClassifier fully interacts with all the reduced dimensions obtained by LDA through parallel coordinates and a scatter plot. Furthermore, it significantly improves the interactivity and interpretability of LDA. LDA enables users to understand each of the reduced dimensions and how they influence the data by reconstructing the basis vector into the original data domain. By using heat maps, iVisClassifier gives an overview about the cluster relationship in terms of pairwise distances between cluster centroids both in the original space and in the reduced dimensional space. Equipped with these functionalities, iVisClassifier supports users' classification tasks in an efficient way. Using several facial image data, we show how the above analysis is performed.},
author = {Choo, J and Lee, H and Kihm, J and Park, H},
booktitle = {Visual Analytics Science and Technology (VAST), 2010 IEEE Symposium on},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Choo et al. - 2010 - iVisClassifier An interactive visual analytics system for classification based on supervised dimension reduction.pdf:pdf},
organization = {IEEE},
pages = {27--34},
title = {{iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction}},
year = {2010}
}
@article{claessen2011flexible,
abstract = {Multivariate data visualization is a classic topic, for which many solutions have been proposed, each with its own strengths and weaknesses. In standard solutions the structure of the visualization is fixed, we explore how to give the user more freedom to define visualizations. Our new approach is based on the usage of Flexible Linked Axes: The user is enabled to define a visualization by drawing and linking axes on a canvas. Each axis has an associated attribute and range, which can be adapted. Links between pairs of axes are used to show data in either scatter plot- or Parallel Coordinates Plot-style. Flexible Linked Axes enable users to define a wide variety of different visualizations. These include standard methods, such as scatter plot matrices, radar charts, and PCPs [11]; less well known approaches, such as Hyperboxes [1], TimeWheels [17], and many-to-many relational parallel coordinate displays [14]; and also custom visualizations, consisting of combinations of scatter plots and PCPs. Furthermore, our method allows users to define composite visualizations that automatically support brushing and linking. We have discussed our approach with ten prospective users, who found the concept easy to understand and highly promising.},
author = {Claessen, J H T and {Van Wijk}, J J},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06064997.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2310--2316},
publisher = {IEEE},
title = {{Flexible linked axes for multivariate data visualization}},
volume = {17},
year = {2011}
}
@book{Cleveland:1988:DGS:576082,
address = {Boca Raton, FL, USA},
annote = {Introduction of the pipeline


A. Buja, D. Asimov, C. Hurley, and JA McDonald. Dynamic Graphics for Statistics, chapter Elements of a viewing pipeline for data analysis, pages 277–308. Wadsworth, Brooks, Cole, 1988.},
author = {Cleveland, William C and McGill, Marylyn E},
edition = {1st},
isbn = {053409144X},
publisher = {CRC Press, Inc.},
title = {{Dynamic Graphics for Statistics}},
year = {1988}
}
@article{Coffey2013,
abstract = {We present an interface for exploring large design spaces as encountered in simulation-based engineering, design of visual effects, and other tasks that require tuning parameters of computationally-intensive simulations and visually evaluating results. The goal is to enable a style of design with simulations that feels as-direct-as-possible so users can concentrate on creative design tasks. The approach integrates forward design via direct manipulation of simulation inputs (e.g., geometric properties, applied forces) in the same visual space with inverse design via 'tugging' and reshaping simulation outputs (e.g., scalar fields from finite element analysis (FEA) or computational fluid dynamics (CFD)). The interface includes algorithms for interpreting the intent of users' drag operations relative to parameterized models, morphing arbitrary scalar fields output from FEA and CFD simulations, and in-place interactive ensemble visualization. The inverse design strategy can be extended to use multi-touch input in combination with an as-rigid-as-possible shape manipulation to support rich visual queries. The potential of this new design approach is confirmed via two applications: medical device engineering of a vacuum-assisted biopsy device and visual effects design using a physically based flame simulation.},
annote = {The user changes a visual representation of a function's outcome and the function's parameters are learned to produce that outcome.},
author = {Coffey, Dane and Lin, Chi Lun and Erdman, Arthur G. and Keefe, Daniel F.},
doi = {10.1109/TVCG.2013.147},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Coffey-2013-Design-Drag.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Design,direct manipulation,multi-touch,simulation},
number = {12},
pages = {2783--2791},
pmid = {24051845},
title = {{Design by dragging: An interface for creative forward and inverse design with simulation ensembles}},
volume = {19},
year = {2013}
}
@inproceedings{Conti2011,
author = {Conti, M and Hasani, A and Crispo, Bruno},
booktitle = {Proceedings of the first ACM conference on Data and application security and privacy},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Conti, Hasani, Crispo - 2011 - Virtual Private Social Networks.pdf:pdf},
isbn = {9781450304658},
keywords = {all or part of,cial networking sites,facebook privacy,internet and privacy,or hard copies of,permission to make digital,so-,this work for,virtual private social networks},
pages = {39--50},
title = {{Virtual Private Social Networks}},
url = {http://dl.acm.org/citation.cfm?id=1943521},
year = {2011}
}
@article{Cook1997,
author = {Cook, D and Buja, A},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Manual Controls for High Dimensional Data Projection.pdf:pdf},
journal = {Journal of computational and Graphical {\ldots}},
title = {{Manual controls for high-dimensional data projections}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/10618600.1997.10474754},
year = {1997}
}
@article{Cook1995,
author = {Cook, D and Buja, A and Cabrera, J and Hurley, C},
file = {:Users/smcgregor/Desktop/Grand Tour and Projection Pursuit.pdf:pdf},
journal = {Journal of Computational {\ldots}},
title = {{Grand Tour and Projection Pursuit}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.1995.10474674},
year = {1995}
}
@inproceedings{correa2009framework,
abstract = {Visual analytics has become an important tool for gaining insight on large and complex collections of data. Numerous statistical tools and data transformations, such as projections, binning and clustering, have been coupled with visualization to help analysts understand data better and faster. However, data is inherently uncertain, due to error, noise or unreliable sources. When making decisions based on uncertain data, it is important to quantify and present to the analyst both the aggregated uncertainty of the results and the impact of the sources of that uncertainty. In this paper, we present a new framework to support uncertainty in the visual analytics process, through statistic methods such as uncertainty modeling, propagation and aggregation. We show that data transformations, such as regression, principal component analysis and k-means clustering, can be adapted to account for uncertainty. This framework leads to better visualizations that improve the decision-making process and help analysts gain insight on the analytic process itself.},
author = {Correa, C D and Chan, Y H and Ma, K L},
booktitle = {Visual Analytics Science and Technology, 2009. VAST 2009. IEEE Symposium on},
file = {:Users/smcgregor/Documents/Mendeley Desktop/05332611.pdf:pdf},
organization = {IEEE},
pages = {51--58},
title = {{A framework for uncertainty-aware visual analytics}},
year = {2009}
}
@article{correa2011towards,
abstract = {Sparse, irregular sampling is becoming a necessity for reconstructing large and high-dimensional signals. However, the analysis of this type of data remains a challenge. One issue is the robust selection of neighborhoods - a crucial part of analytic tools such as topological decomposition, clustering and gradient estimation. When extracting the topology of sparsely sampled data, common neighborhood strategies such as k-nearest neighbors may lead to inaccurate results, either due to missing neighborhood connections, which introduce false extrema, or due to spurious connections, which conceal true extrema. Other neighborhoods, such as the Delaunay triangulation, are costly to compute and store even in relatively low dimensions. In this paper, we address these issues. We present two new types of neighborhood graphs: a variation on and a generalization of empty region graphs, which considerably improve the robustness of neighborhood-based analysis tools, such as topological decomposition. Our findings suggest that these neighborhood graphs lead to more accurate topological representations of low- and high- dimensional data sets at relatively low cost, both in terms of storage and computation time. We describe the implications of our work in the analysis and visualization of scalar functions, and provide general strategies for computing and applying our neighborhood graphs towards robust data analysis.},
author = {Correa, C D and Lindstrom, P},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Correa, Lindstrom - 2011 - Towards Robust Topology of Sparsely Sampled Data.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {1852--1861},
publisher = {IEEE},
title = {{Towards Robust Topology of Sparsely Sampled Data}},
volume = {17},
year = {2011}
}
@article{correa2011topological,
abstract = {We present topological spines-a new visual representation that preserves the topological and geometric structure of a scalar field. This representation encodes the spatial relationships of the extrema of a scalar field together with the local volume and nesting structure of the surrounding contours. Unlike other topological representations, such as contour trees, our approach preserves the local geometric structure of the scalar field, including structural cycles that are useful for exposing symmetries in the data. To obtain this representation, we describe a novel mechanism based on the extraction of extremum graphs-sparse subsets of the Morse-Smale complex that retain the important structural information without the clutter and occlusion problems that arise from visualizing the entire complex directly. Extremum graphs form a natural multiresolution structure that allows the user to suppress noise and enhance topological features via the specification of a persistence range. Applications of our approach include the visualization of 3D scalar fields without occlusion artifacts, and the exploratory analysis of high-dimensional functions.},
author = {Correa, C and Lindstrom, P and Bremer, P T},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06064947.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {1842--1851},
publisher = {IEEE},
title = {{Topological Spines: A Structure-preserving Visual Representation of Scalar Fields}},
volume = {17},
year = {2011}
}
@article{Cristofaro2012,
author = {Cristofaro, Emiliano De and Soriente, Claudio and Tsudik, Gene and Williams, Andrew},
doi = {10.1109/SP.2012.26},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06234419.pdf:pdf},
isbn = {978-1-4673-1244-8},
journal = {2012 IEEE Symposium on Security and Privacy},
keywords = {privacy twitter multi-party computation},
month = {may},
pages = {285--299},
publisher = {Ieee},
title = {{Hummingbird: Privacy at the Time of Twitter}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6234419},
year = {2012}
}
@article{Cutler2014,
annote = {1. Introduction

Existing approaches
* Many robotics approaches have combined datasets from real scenarios and a simulator to fill in the gaps. However this is typically regarded as static.
* Another approach that has used multiple simulators is to use a fast simulator as a way of anchoring itself to a high heuristic value or potential field.
* Transfer learning doesn't have rules for selecting different simulators even if it has rules for combining knowledge.

This approach
* The main contribution seems to be the fixing of the heuristic from samples taken in the higher fidelity and moving between the two simulators as learning proceeds.

Makes the claims
* Does not run actions at high levels that have been proven suboptimal below
* Minimizes the number of samples used in the real world
* Limits the total number of samples used in all simulators
* Uses no more samples in the highest fidelity than other approaches (suggests that lower bound is tight)

2. Related Work

* In policy search the fast simulator has been used to compute the gradient.
* This work can be viewed as transfer learning, in which values or model parameters are used to bootstrap learning in the next task. Typically in transfer learning you would have a solved domain and want to use the expertise in a new domain. In this setting you would never return to the solved domain because you have all the information it can provide, but in MFRL we start with zero knowledge across all the domains. The distinction is forced only in domains where we have a set of simulators that are:

a) Have different costs of execution
b) The simulators still have sufficient complexity that they cannot be &quot;solved&quot; to generate a model that is accessible in constant time.

* MFO similarly combines samples from multiple simulators (airfoil) but it does not have the sequential decision making of RL.

3. Background and Assumptions

* Here we have the usual MDP formulation
* In the KWIK framework the learner is asked to predict the reward or transition with high probability. If it doesn't know, it reports bottom.
* Something is KWIK learnable only if it (with high probability) guarantees that it will report bottom a polynomial number of times.
* Example, in the KWIK-Rmax RL algorithm &quot;bottom&quot; is replaced with Rmax

b) Simulator assumptions and objectives

* Sigma is a simulator whose subscript indicates the complexity of the simulator and nearness to the target simulator.
* Higher subscripts indicate higher complexity/fidelity.
* They assume that lower fidelity simulator state space is a subset of the higher fidelity simulator.
* States in lower fidelity simulators are mapable to higher ones, but the reverse typically requires assumptions about missing variables. In the case of this paper they choose not to model elements like rotation, whose default setting can be zero.
* Fidelity is defined as the maximum across all (s,a) of the difference in optimal q values. If fidelity fails to meet an assumption (subscripted beta), then the fidelity is defined to be negative infinity.
* The fidelity assumption here is thus that all the Q values will be within a tolerance -- this is a STRONG assumption.
* IMHO, satisfying this assumption requires careful consideration and construction. However, with a proper hierarchy it is possible to ramp up from one domain to the next such that only adjacent simulators meaningfully satisfy the assumptions.
* The paper argues that the assumption is met in many domains where optimism (not accounting for noise, or low probability events) permits a faster simulator to be employed.
* Sigma D is the real world simulator (robot).
* Every simulator down the hierarchy is permitted a polynomial number of samples in its state space for each sample in the higher fidelity simulator. This would seem to indicate that a deep hierarchy would allow for the lowest level to be solved exactly.
* They assume access to trajectories from a current state, rather than generative access.
* There are three objectives: Minimizing suboptimal steps in the true world, guarantee polynomial number of samples and switches before a near-optimal policy is learned.

4. Multi-Fidelity Bandit Optimization

* MFRL is presented in the context of optimizing for bandits.
* The policy is learned with KWIK-Rmax derived learner.
* The upper bounds of a simulator+action is U_{da} and the informed upper bound includes the minimum of the heuristic derived from the lower level.
* con_{da} value of the action has converged
* closed_d optimal action has been identified
* change_d learned model has changed at simulator level d.

c. Theoretical analysis

* Polynomial complexity is easily shown. In the worst case it will sample all the models m times, so it would be NUMBER_MODELS*KWIK_COMPLEXITY.
* Theorem 2 shows that they (with high probability) don't eliminate a near action.

5. Multi-Fidelity Reinforcement Learning

* MFRL is similar to MF-Bandit but now the heuristic passed to higher fidelity simulators is the Q-function, and both the reward and transition functions are passed down to lower fidelity simulators.
* The lowest level simulator is initialized to the Rmax, so it needs to be completely explored before moving to the next higher level.
* (show puddle world results). I don't know how this could perform well when it would seem like the betas would be Rmax, perhaps it is because the betas are on a per-state basis so they will only be locally extreme and the learning tasks at the higher levels is to eliminate those states' optimism.

C. Theoretical analysis:
* Actions proven suboptimal at a lower level will not be executed at a higher level.


6. RC Car Results

7. Extensions and Conclusions},
author = {Cutler, Mark and Walsh, Thomas J. and How, Jonathan P.},
doi = {10.1109/ICRA.2014.6907423},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Cutler14_ICRA.pdf:pdf},
isbn = {978-1-4799-3685-4},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
number = {1},
pages = {3888--3895},
title = {{Reinforcement learning with multi-fidelity simulators}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6907423},
year = {2014}
}
@inproceedings{D'Angelo:2010:CCP:1774088.1774259,
abstract = {Web office suites such as Google Docs offer unparalleled collaboration experiences in terms of low software requirements, ease of use, data ubiquity, and availability. When the data holder (Google, Microsoft, etc.) is not perceived as trusted though, those benefits are considered at stake with important privacy requirements. Content cloaking is a lightweight, cryptographic, client-side solution to protect content from data holders while using web office suites and other "Web 2.0", AJAX-based, collaborative applications.},
address = {New York, NY, USA},
author = {D'Angelo, Gabriele and Vitali, Fabio and Zacchiroli, Stefano},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
doi = {10.1145/1774088.1774259},
isbn = {978-1-60558-639-7},
keywords = {AJAX,Google Docs,Web 2.0,privacy},
pages = {826--830},
publisher = {ACM},
series = {SAC '10},
title = {{Content cloaking: preserving privacy with Google Docs and other web applications}},
url = {http://doi.acm.org/10.1145/1774088.1774259},
year = {2010}
}
@article{dasgupta2010pargnostics,
abstract = {Interactive visualization requires the translation of data into a screen space of limited resolution. While currently ignored by most visualization models, this translation entails a loss of information and the introduction of a number of artifacts that can be useful, (e.g., aggregation, structures) or distracting (e.g., over-plotting, clutter) for the analysis. This phenomenon is observed in parallel coordinates, where overlapping lines between adjacent axes form distinct patterns, representing the relation between variables they connect. However, even for a small number of dimensions, the challenge is to effectively convey the relationships for all combinations of dimensions. The size of the dataset and a large number of dimensions only add to the complexity of this problem. To address these issues, we propose Pargnostics, parallel coordinates diagnostics, a model based on screen-space metrics that quantify the different visual structures. Pargnostics metrics are calculated for pairs of axes and take into account the resolution of the display as well as potential axis inversions. Metrics include the number of line crossings, crossing angles, convergence, overplotting, etc. To construct a visualization view, the user can pick from a ranked display showing pairs of coordinate axes and the structures between them, or examine all possible combinations of axes at once in a matrix display. Picking the best axes layout is an NP-complete problem in general, but we provide a way of automatically optimizing the display according to the user's preferences based on our metrics and model.},
author = {Dasgupta, A and Kosara, R},
file = {:Users/smcgregor/Documents/Mendeley Desktop/05613439.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1017--1026},
publisher = {IEEE},
title = {{Pargnostics: Screen-space metrics for parallel coordinates}},
volume = {16},
year = {2010}
}
@inproceedings{Davidson2014,
address = {Portland, Oregon},
author = {Davidson, Jennifer and McGregor, Sean},
booktitle = {Open Source Bridge},
publisher = {Stumptown Syndicate},
title = {{Making Your Privacy Software Usable}},
url = {http://opensourcebridge.org/sessions/1294},
year = {2014}
}
@misc{DebbySwayneDiCook2012,
author = {{Debby Swayne, Di Cook}, Duncan Temple Lang and Andreas Buja},
title = {{GGobi Website}},
url = {http://www.ggobi.org/rggobi/},
year = {2012}
}
@article{Deisenroth2011,
abstract = {Policy search is a subfield in reinforcement learning which focuses on finding good parameters for a given policy parametrization. It is well suited for robotics as it can cope with high-dimensional state and action spaces, one of the main challenges in robot learning. We review recent successes of both model-free and model-based policy search in robot learning. Model-free policy search is a general approach to learn policies based on sampled trajectories. We classify model-free methods based on their policy evaluation strategy, policy update strategy, and exploration strategy and present a unified view on existing algorithms. Learning a policy is often easier than learning an accurate forward model, and, hence, model-free methods are more frequently used in practice. However, for each sampled trajectory, it is necessary to interact with the robot, which can be time consuming and challenging in practice. Model-based policy search addresses this problem by first learning a simulator of the robot's dynamics from data. Subsequently, the simulator generates trajectories that are used for policy learning. For both model-free and model-based policy search methods, we review their respective properties and their applicability to robotic systems.},
author = {Deisenroth, Marc Peter},
doi = {10.1561/2300000021},
file = {:Users/smcgregor/Documents/Mendeley Desktop/PolicySearchReview.pdf:pdf},
issn = {1935-8253},
journal = {Foundations and Trends in Robotics},
keywords = {Artificial Intelligence in Robotics,Markov Decision Processes,Planning and Control,Policy Search},
number = {2011},
pages = {1--142},
title = {{A Survey on Policy Search for Robotics}},
url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-robotics/ROB-021},
volume = {2},
year = {2011}
}
@article{Demsar2006,
abstract = {Abstract While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams. Keywords: comparative studies, statistical methods, Wilcoxon signed ranks test, Friedman test, multiple comparisons tests},
annote = {Which statistical tests are appropriate for comparing classifier performance.

It wants us to use the Wilcoxon Signed Rank test for comparing two classifiers.

Use Friedman Test to compare many classifiers on many datasets.},
author = {Dem{\v{s}}ar, J},
doi = {10.1016/j.jecp.2010.03.005},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Demsar06.pdf:pdf},
isbn = {9781424450404},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {comparative studies,friedman test,multiple comparisons tests,statistical methods,wilcoxon signed ranks test},
pages = {1--30},
pmid = {20451214},
title = {{Statistical Comparisons of Classifiers over Multiple Data Sets}},
volume = {7},
year = {2006}
}
@misc{Diasp.eu2013,
abstract = {369760 users},
author = {Diasp.eu},
howpublished = {https://diasp.eu/stats.html},
title = {{How Many Users are in the DIASPORA* Network?}},
url = {https://diasp.eu/stats.html},
year = {2013}
}
@article{Dietterich2013,
author = {Dietterich, TG and Taleghan, MA and Crowley, Mark},
file = {:Users/smcgregor/Documents/Mendeley Desktop/dietterich-taleghan-crowley-pac-optimal-planning-for-invasive-species-management-etc-aaai2013.pdf:pdf},
journal = {Twenty-Seventh AAAI Conference on Artificial Intelligence},
title = {{PAC Optimal Planning for Invasive Species Management: Improved Exploration for Reinforcement Learning from Simulator-Defined MDPs}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/download/6478/6850},
year = {2013}
}
@article{Dingledine2004,
author = {Dingledine, Roger and Mathewson, Nick and Syverson, Paul},
file = {:Users/smcgregor/Documents/Mendeley Desktop/a465464.pdf:pdf},
journal = {Naval Research Lab Washington DC},
title = {{Tor : The Second-Generation Onion Router}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA465464},
year = {2004}
}
@inproceedings{Djeric2009,
author = {Djeric, Vladan and Goel, Ashvin},
booktitle = {Proceedings of the 19th USENIX Conference on Security},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Djeric.pdf:pdf},
title = {{Securing script-based extensibility in web browsers}},
url = {http://static.usenix.org/event/sec10/tech/full_papers/Djeric.pdf},
year = {2009}
}
@article{Doya2002,
author = {Doya, Kenji and Samejima, Kazuyuki},
journal = {Neural Computation},
keywords = {modeling},
mendeley-tags = {modeling},
pages = {1347----1369},
title = {{Multiple Model-based Reinforcement Learning}},
volume = {14},
year = {2002}
}
@article{Edwards2012,
abstract = {Previous research has shown that people err when making decisions aided by probability information. Surprisingly, there has been little exploration into the accuracy of decisions made based on many commonly used probabilistic display methods. Two experiments examined the ability of a comprehensive set of such methods to effectively communicate critical information to a decision maker and influence confidence in decision making. The second experiment investigated the performance of these methods under time pressure, a situational factor known to exacerbate judgmental errors. Ten commonly used graphical display methods were randomly assigned to participants. Across eight scenarios in which a probabilistic outcome was described, participants were asked questions regarding graph interpretation (e.g., mean) and made behavioral choices (i.e., act; do not act) based on the provided information indicated that decision-maker accuracy differed by graphical method; error bars and boxplots led to greatest mean estimation and behavioral choice accuracy whereas complementary cumulative probability distribution functions were associated with the highest probability estimation accuracy. Under time pressure, participant performance decreased when making behavioral choices.},
author = {Edwards, John a and Snyder, Frank J and Allen, Pamela M and Makinson, Kevin a and Hamby, David M},
doi = {10.1111/j.1539-6924.2012.01839.x},
file = {:Users/smcgregor/Documents/Mendeley Desktop/PresentingQuantitativeUncertainty.pdf:pdf},
issn = {1539-6924},
journal = {Risk analysis : an official publication of the Society for Risk Analysis},
keywords = {decision making,graphical communication,probability,risk management,uncertainty},
month = {dec},
number = {12},
pages = {2055--2070},
pmid = {22616656},
title = {{Decision Making for Risk Management: A Comparison of Graphical Methods for Presenting Quantitative Uncertainty.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22616656},
volume = {32},
year = {2012}
}
@inproceedings{Elkan2001,
abstract = {This paper revisits the problem of optimal learn- ing and decision-making when different misclassi- fication errors incur different penalties. We char- acterize precisely but intuitively when a cost ma- trix is reasonable, and we show how to avoid the mistake of defining a cost matrix that is economi- cally incoherent. For the two-class case, we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost-sensitive classification decisions using a classifier learned by a standard non-cost- sensitive learning method. However, we then argue that changing the balance of negative and positive training examples has little effect on the classifiers produced by standard Bayesian and decision tree learning methods. Accordingly, the recommended way of applying one of these methods in a domain with differing misclassification costs is to learn a classifier from the training set as given, and then to compute optimal decisions explicitly using the probability estimates given by the classifier},
author = {Elkan, Charles},
booktitle = {International Joint Conference on Artificial Intelligence},
file = {:Users/smcgregor/Documents/Mendeley Desktop/elkan-the-foundations-of-cost-sensitive-learning-ijcai2001.pdf:pdf},
number = {1},
pages = {973--978},
publisher = {Citeseer},
title = {{The foundations of cost-sensitive learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.8670&amp;rep=rep1&amp;type=pdf},
volume = {17},
year = {2001}
}
@article{elmqvist2008rolling,
abstract = {Scatterplots remain one of the most popular and widely-used visual representations for multidimensional data due to their simplicity, familiarity and visual clarity, even if they lack some of the flexibility and visual expressiveness of newer multidimensional visualization techniques. This paper presents new interactive methods to explore multidimensional data using scatterplots. This exploration is performed using a matrix of scatterplots that gives an overview of the possible configurations, thumbnails of the scatterplots, and support for interactive navigation in the multidimensional space. Transitions between scatterplots are performed as animated rotations in 3D space, somewhat akin to rolling dice. Users can iteratively build queries using bounding volumes in the dataset, sculpting the query from different viewpoints to become more and more refined. Furthermore, the dimensions in the navigation space can be reordered, manually or automatically, to highlight salient correlations and differences among them. An example scenario presents the interaction techniques supporting smooth and effortless visual exploration of multidimensional datasets.},
annote = {Won best paper},
author = {Elmqvist, N and Dragicevic, P and Fekete, J D},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Elmqvist, Dragicevic, Fekete - 2008 - Rolling the dice Multidimensional visual exploration using scatterplot matrix navigation.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1148--1539},
publisher = {IEEE},
title = {{Rolling the dice: Multidimensional visual exploration using scatterplot matrix navigation}},
volume = {14},
year = {2008}
}
@article{Epshteyn2008,
author = {Epshteyn, Arkady and Vogel, Adam and Dejong, Gerald},
keywords = {modeling},
mendeley-tags = {modeling},
pages = {296--303},
title = {{Active Reinforcement Learning}},
year = {2008}
}
@misc{Erwin2012,
abstract = {I have been very vocal, from the very beginning, about my gratitude and debt to The Quiet Earth (hi, Gordon!), and my appreciation for the Redditors who offered expertise and suggestions. That said: the story that went to Warner Brothers is not the same as the story posted on Reddit. It has a lot of the same DNA, mind you, and it feels like RSR because it's my writing. This is totally okay with me for two reasons. One, the story I was setting up was not super cinematic - hard to get through a lot of what I was gestating in two hours. Two, someday when it's not going to conflict with what goes up in the movie, what I was setting up can be completed as fan fiction. What I'm saying is you're not going to see Day Nine on the silver screen. But you will see it on Reddit someday.},
author = {Erwin, James},
booktitle = {Reddit.com},
howpublished = {Shortened Reddit Link: "http://bit.ly/Zcq5dm"},
title = {{IAMA Prufrock451}},
url = {http://www.reddit.com/r/IAmA/comments/11kt8o/iama_prufrock451_whose_reddit_story_rome_sweet/c6nc6h9},
year = {2012}
}
@article{Eyal2015,
abstract = {Cryptocurrencies, based on and led by Bitcoin, have shown promise as infrastructure for pseudonymous online payments, cheap remittance, trustless digital asset exchange, and smart contracts. However, Bitcoin-derived blockchain protocols have inherent scalability limits that trade-off between throughput and latency and withhold the realization of this potential. This paper presents Bitcoin-NG, a new blockchain protocol designed to scale. Based on Bitcoin's blockchain protocol, Bitcoin-NG is Byzantine fault tolerant, is robust to extreme churn, and shares the same trust model obviating qualitative changes to the ecosystem. In addition to Bitcoin-NG, we introduce several novel metrics of interest in quantifying the security and efficiency of Bitcoin-like blockchain protocols. We implement Bitcoin-NG and perform large-scale experiments at 15% the size of the operational Bitcoin system, using unchanged clients of both protocols. These experiments demonstrate that Bitcoin-NG scales optimally, with bandwidth limited only by the capacity of the individual nodes and latency limited only by the propagation time of the network.},
archivePrefix = {arXiv},
arxivId = {1510.02037},
author = {Eyal, Ittay and Gencer, Adem Efe and Sirer, Emin Gun and van Renesse, Robbert},
eprint = {1510.02037},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Bitcoin-NG (1).pdf:pdf},
title = {{Bitcoin-NG: A Scalable Blockchain Protocol}},
url = {http://arxiv.org/abs/1510.02037},
year = {2015}
}
@inproceedings{F.BeatoI.IonS.CapkunM.Langheinrich2013,
author = {{F. Beato, I. Ion, S. Capkun, M. Langheinrich}, and B. Preneel},
booktitle = {ACM Conference on Data and Application Security and Privacy},
file = {:Users/smcgregor/Documents/Mendeley Desktop/eth-5961-01.pdf:pdf},
title = {{For Some Eyes Only: Protecting Online Information Sharing}},
year = {2013}
}
@misc{Facebook2013,
abstract = {More than 1 billion users on Facebook},
author = {Facebook},
howpublished = {http://newsroom.fb.com/Key-Facts},
title = {{Key Facts - Facebook Newsroom}},
url = {http://newsroom.fb.com/Key-Facts},
year = {2013}
}
@article{Fahl2012,
author = {Fahl, Sascha and Harbach, Marian and Muders, Thomas},
file = {:Users/smcgregor/Documents/Mendeley Desktop/a11-fahl.pdf:pdf},
isbn = {9781450315326},
journal = {Proceedings of the Eighth Symposium on Usable Privacy and Security},
keywords = {message encryption,social networks,usable security},
title = {{Helping Johnny 2.0 to encrypt his Facebook conversations}},
url = {http://dl.acm.org/citation.cfm?id=2335371},
year = {2012}
}
@article{Fahl2012b,
author = {Fahl, Sascha and Harbach, Marian and Muders, Thomas and Smith, Matthew},
doi = {10.1109/TrustCom.2012.112},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06295970.pdf:pdf},
isbn = {978-1-4673-2172-3},
journal = {2012 IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications},
month = {jun},
pages = {153--162},
publisher = {Ieee},
title = {{Confidentiality as a Service -- Usable Security for the Cloud}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6295970},
year = {2012}
}
@article{Fahl2012a,
author = {Fahl, Sascha and Harbach, Marian and Muders, Thomas and Smith, Matthew},
file = {:Users/smcgregor/Documents/Mendeley Desktop/p145-fahl.pdf:pdf},
isbn = {9781450313353},
journal = {Proceedings of the 23rd ACM conference on Hypertext and social media},
keywords = {confidentiality,privacy,social networks,sym-,usability},
pages = {145--154},
title = {{TrustSplit : Usable Confidentiality for Social Network Messaging}},
url = {http://dl.acm.org/citation.cfm?id=2310022},
year = {2012}
}
@article{Feldman2012,
author = {Feldman, AJ and Blankstein, Aaron},
file = {:Users/smcgregor/Documents/Mendeley Desktop/sec12-final67.pdf:pdf},
journal = {Proceedings of the 21st USENIX Conference on Security Symposium},
title = {{Social Networking with Frientegrity: Privacy and Integrity with an Untrusted Provider}},
url = {https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final67.pdf},
year = {2012}
}
@article{Fiechter1994,
author = {Fiechter, Claude-Nicolas},
file = {:Users/smcgregor/Documents/Mendeley Desktop/p88-fiechter.pdf:pdf},
journal = {Proceedings of the seventh annual conference on Computational learning theory},
title = {{Efficient reinforcement learning}},
url = {http://dl.acm.org/citation.cfm?id=181019},
year = {1994}
}
@misc{Fielding1999,
author = {Fielding, R and Gettys, J and Frystyk, H and Masinter, L and Leach, P and Berners-Lee, T},
howpublished = {http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.2.1},
title = {{HTTP 1.1 Protocol Parameters}},
url = {http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.2.1},
year = {1999}
}
@article{FletcherIII1988,
author = {{Fletcher III}, C. Edward},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Sophisticated Investors Under the Federal Securities Laws.pdf:pdf},
journal = {Duke Law Journal},
number = {6},
pages = {1081--1154},
title = {{Sophisticated Investors Under the Federal Securities Laws}},
url = {http://heinonlinebackup.com/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/duklr1988&section=55},
volume = {1988},
year = {1988}
}
@article{Fonteneau2013a,
author = {Fonteneau, R and Ernst, D and Boigelot, B and Louveaux, Q},
file = {:Users/smcgregor/Documents/Mendeley Desktop/fonteneau-MIN-MAX-BMRL.pdf:pdf},
journal = {SIAM Journal on Control and {\ldots}},
keywords = {compu-,min max generalization,nonconvex optimization,reinforcement learning},
number = {5},
pages = {3355--3385},
title = {{Min max generalization for deterministic batch mode reinforcement learning: relaxation schemes}},
url = {http://epubs.siam.org/doi/abs/10.1137/120867263},
volume = {51},
year = {2013}
}
@article{Fonteneau2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1403.4514v2},
author = {Fonteneau, Raphael and Prashanth, {L A}},
eprint = {arXiv:1403.4514v2},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1403.4514.pdf:pdf},
title = {{Simultaneous Perturbation Algorithms for Batch Off-Policy Search}},
year = {2010}
}
@article{Fonteneau2013,
abstract = {In this paper, we consider the batch mode reinforcement learning setting, where the central problem is to learn from a sample of trajectories a policy that satisfies or optimizes a performance criterion. We focus on the continuous state space case for which usual resolution schemes rely on function approximators either to represent the underlying control problem or to represent its value function. As an alternative to the use of function approximators, we rely on the synthesis of "artificial trajectories" from the given sample of trajectories, and show that this idea opens new avenues for designing and analyzing algorithms for batch mode reinforcement learning.},
author = {Fonteneau, Raphael and Murphy, Susan a and Wehenkel, Louis and Ernst, Damien},
doi = {10.1007/s10479-012-1248-5},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Fonteneau2013ANOR.pdf:pdf},
issn = {0254-5330},
journal = {Annals of Operations Research},
keywords = {artificial trajectories,function,optimal control,reinforcement learning},
month = {Sep},
number = {1},
pages = {383--416},
pmid = {24049244},
title = {{Batch Mode Reinforcement Learning based on the Synthesis of Artificial Trajectories.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3773886&tool=pmcentrez&rendertype=abstract},
volume = {208},
year = {2013}
}
@inproceedings{Fonteneau2014,
annote = {They use trajectory synthesis/Model Free Monte Carlo (MFMC) as the basis for policy gradient. Their policy gradient proceeds by peturbing parameters (reffered to as &quot;simultaneous peturbation&quot;) and using MFMC to generate trajectories for two sets of parameters which are then combined in a policy update step.

They do not specify how they determine the distance metric, but since the illustrative problem is a sine wave and they assume Lipschitz, it is likely that they are using something as simple as Euclidean distance.},
author = {Fonteneau, Raphael and Prashanth, {L A}},
booktitle = {53rd IEEE Conference on Conference on Decision and Control},
journal = {53rd IEEE Conference on Conference on Decision and Control},
file = {:Users/smcgregor/Documents/Mendeley Desktop/2014MFMCSPSA_CDC.pdf:pdf},
title = {{Simultaneous Perturbation Algorithms for Batch Off-Policy Search}},
year = {2014}
}
@article{fbk-ousmpccfds-2000,
author = {Forrester, A I J and Bresslof, N W and Keane, A J},
journal = {Proceedings of the Royal Society A},
pages = {2177--2204},
title = {{Optimization using surrogate models and partially converged computational fluid dynamics simulations}},
volume = {462}
}
@book{fsk-edvsm:apg-2008,
author = {Forrester, A and Sobester, A and Keane, A},
publisher = {Wiley},
title = {{Engineering Design Via Surrogate Modelling: A Practical Guide}},
year = {2008}
}
@article{Forrester2007,
author = {Forrester, Alexander I.J. and S{\'{o}}bester, Andr{\'{a}}s and Keane, Andy J.},
doi = {10.1098/rspa.2007.1900},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Proc. R. Soc. A-2007-Forrester-3251-69.pdf:pdf},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {co-kriging,kriging,noise,subset selection,wing design},
month = {dec},
number = {2088},
pages = {3251--3269},
title = {{Multi-fidelity optimization via surrogate modelling}},
url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.2007.1900},
volume = {463},
year = {2007}
}
@book{Fowler1999,
abstract = {As the application of object technology-particularly the Java programming language-has become commonplace, a new problem has emerged to confront the software development community. Significant numbers of poorly designed programs have been created by less-experienced developers, resulting in applications that are inefficient and hard to maintain and extend. Increasingly, software system professionals are discovering just how difficult it is to work with these inherited, "non-optimal" applications. For several years, expert-level object programmers have employed a growing collection of techniques to improve the structural integrity and performance of such existing software programs. Referred to as "refactoring," these practices have remained in the domain of experts because no attempt has been made to transcribe the lore into a form that all developers could use. . .until now. In Refactoring: Improving the Design of Existing Software, renowned object technology mentor Martin Fowler breaks new ground, demystifying these master practices and demonstrating how software practitioners can realize the significant benefits of this new process. With proper training a skilled system designer can take a bad design and rework it into well-designed, robust code. In this book, Martin Fowler shows you where opportunities for refactoring typically can be found, and how to go about reworking a bad design into a good one. Each refactoring step is simple-seemingly too simple to be worth doing. Refactoring may involve moving a field from one class to another, or pulling some code out of a method to turn it into its own method, or even pushing some code up or down a hierarchy. While these individual steps may seem elementary, the cumulative effect of such small changes can radically improve the design. Refactoring is a proven way to prevent software decay. In addition to discussing the various techniques of refactoring, Refactoring: Improving the Design of Existing Software provides a detailed catalog of more than seventy proven refactorings with helpful pointers that teach you when to apply them; step-by-step instructions for applying each refactoring; and an example illustrating how the refactoring works. The illustrative examples are written in Java, but the ideas are applicable to any object-oriented programming language.},
author = {Fowler, Martin and Beck, Kent and Brant, John and Opdyke, William and Roberts, Don},
booktitle = {Xtemp01},
doi = {10.1007/s10071-009-0219-y},
file = {:Users/smcgregor/Documents/Mendeley Desktop/M.Fowler et al - Refactoring - Improving the Design of Existing.pdf:pdf},
isbn = {9780201485677},
issn = {14359456},
pages = {1--337},
pmid = {19263100},
title = {{Refactoring: Improving the Design of Existing Code}},
year = {1999}
}
@article{friedman1974projection,
abstract = {An algorithm for the analysis of multivariate data is presented and is discussed in terms of specific examples. The algorithm seeks to find one-and two-dimensional linear projections of multivariate data that are relatively highly revealing.},
author = {Friedman, J H and Tukey, J W},
journal = {Computers, IEEE Transactions on},
number = {9},
pages = {881--890},
publisher = {IEEE},
title = {{A projection pursuit algorithm for exploratory data analysis}},
volume = {100},
year = {1974}
}
@article{Furnas1994,
author = {Furnas, GW and Buja, A},
file = {:Users/smcgregor/Desktop/Prosection Views- Dimensional Inference through Sections and Projections.pdf:pdf},
journal = {Journal of Computational and Graphical {\ldots}},
title = {{Prosection views: Dimensional inference through sections and projections}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.1994.10474649},
year = {1994}
}
@article{Gabillon2011,
author = {Gabillon, Victor and Scherrer, Bruno and Nancy, Inria and Est, Grand and Maia, Team},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Gabillon et al. - 2011 - Classification-based Policy Iteration with a Critic.pdf:pdf},
journal = {Policy},
keywords = {boring formatting information, machine learning, I},
number = {x},
title = {{Classification-based Policy Iteration with a Critic}},
volume = {1},
year = {2011}
}
@incollection{Gabriel2005,
annote = {Interesting method of assigning classes to clusters and checking different levels of aggregation.},
author = {Gabriel, Thomas and Pintilie, A and Berthold, Michael},
doi = {10.1007/11552253_10},
editor = {Famili, A and Kok, Joost and Pe{\~{n}}a, Jos{\'{e}} and Siebes, Arno and Feelders, Ad},
file = {:Users/smcgregor/Documents/Mendeley Desktop/ehrs.pdf:pdf},
isbn = {978-3-540-28795-7},
keywords = {Computer Science},
pages = {741},
publisher = {Springer Berlin / Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Exploring Hierarchical Rule Systems in Parallel Coordinates}},
url = {http://dx.doi.org/10.1007/11552253_10},
volume = {3646},
year = {2005}
}
@misc{Galindo2013,
author = {Galindo, Virginie},
howpublished = {http://www.w3.org/2012/webcrypto/},
title = {{W3C Web Cryptography Working Group}},
url = {http://www.w3.org/2012/webcrypto/},
urldate = {2013-12-11},
year = {2013}
}
@misc{Gardner2011,
abstract = {Fledging author James Erwin sold Warner Bros. on movie rights to his short story posted on Reddit. But the user agreement for the news community website could raise some thorny intellectual property questions.},
address = {Hollywood},
author = {Gardner, Eriq},
booktitle = {The Hollywood Reporter},
howpublished = {http://www.hollywoodreporter.com/thr-esq/does-warner-bros-have-movie-250726},
month = {oct},
title = {{Does Warner Bros. Really Have Exclusive Movie Rights to a Story Posted on Reddit? (Analysis)}},
url = {http://www.hollywoodreporter.com/thr-esq/does-warner-bros-have-movie-250726},
year = {2011}
}
@book{garfinkel1994pgp,
author = {Garfinkel, Simson},
publisher = {O'Reilly Media, Incorporated},
title = {{PGP: pretty good privacy}},
year = {1994}
}
@inproceedings{garg2008model,
abstract = {We describe a visual analytics (VA) infrastructure, rooted on techniques in machine learning and logic-based deductive reasoning that will assist analysts to make sense of large, complex data sets by facilitating the generation and validation of models representing relationships in the data. We use logic programming (LP) as the underlying computing machinery to encode the relations as rules and facts and compute with them. A unique aspect of our approach is that the LP rules are automatically learned, using Inductive Logic Programming, from examples of data that the analyst deems interesting when viewing the data in the high-dimensional visualization interface. Using this system, analysts will be able to construct models of arbitrary relationships in the data, explore the data for scenarios that fit the model, refine the model if necessary, and query the model to automatically analyze incoming (future) data exhibiting the encoded relationships. In other words it will support both model-driven data exploration, as well as data-driven model evolution. More importantly, by basing the construction of models on techniques from machine learning and logic-based deduction, the VA process will be both flexible in terms of modeling arbitrary, user-driven relationships in the data as well as readily scale across different data domains.},
author = {Garg, S and Nam, J E and Ramakrishnan, I V and Mueller, K},
booktitle = {Visual Analytics Science and Technology, 2008. VAST'08. IEEE Symposium on},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Garg et al. - 2008 - Model-driven visual analytics.pdf:pdf},
organization = {IEEE},
pages = {19--26},
title = {{Model-driven visual analytics}},
year = {2008}
}
@article{Geambasu2009,
author = {Geambasu, R and Kohno, T and Levy, A and Levy, HM},
file = {:Users/smcgregor/Documents/Mendeley Desktop/geambasu.pdf:pdf},
journal = {Proceedings of the 18th USENIX Conference on Security Symposium},
title = {{Vanish: Increasing Data Privacy with Self-Destructing Data}},
url = {http://static.usenix.org/events/sec09/tech/full_papers/sec09_javascript.pdf http://www.usenix.org/event/sec09/tech/full_papers/sec09_crypto.pdf},
year = {2009}
}
@article{geng2011angular,
abstract = {Parallel coordinates is a popular and well-known multivariate data visualization technique. However, one of their inherent limitations has to do with the rendering of very large data sets. This often causes an overplotting problem and the goal of the visual information seeking mantra is hampered because of a cluttered overview and non-interactive update rates. In this paper, we propose two novel solutions, namely, angular histograms and attribute curves. These techniques are frequency-based approaches to large, high-dimensional data visualization. They are able to convey both the density of underlying polylines and their slopes. Angular histogram and attribute curves offer an intuitive way for the user to explore the clustering, linear correlations and outliers in large data sets without the over-plotting and clutter problems associated with traditional parallel coordinates. We demonstrate the results on a wide variety of data sets including real-world, high-dimensional biological data. Finally, we compare our methods with the other popular frequency-based algorithms.},
author = {Geng, Z and Peng, Z M and Laramee, R S and Walker, R and Roberts, J C},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Geng et al. - 2011 - Angular histograms Frequency-based visualizations for large, high dimensional data.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2572--2580},
publisher = {IEEE},
title = {{Angular histograms: Frequency-based visualizations for large, high dimensional data}},
volume = {17},
year = {2011}
}
@article{Gleicher2013,
abstract = {This paper introduces an approach to exploration and discovery in high-dimensional data that incorporates a user's knowledge and questions to craft sets of projection functions meaningful to them. Unlike most prior work that defines projections based on their statistical properties, our approach creates projection functions that align with user-specified annotations. Therefore, the resulting derived dimensions represent concepts defined by the user's examples. These especially crafted projection functions, or explainers, can help find and explain relationships between the data variables and user-designated concepts. They can organize the data according to these concepts. Sets of explainers can provide multiple perspectives on the data. Our approach considers tradeoffs in choosing these projection functions, including their simplicity, expressive power, alignment with prior knowledge, and diversity. We provide techniques for creating collections of explainers. The methods, based on machine learning optimization frameworks, allow exploring the tradeoffs. We demonstrate our approach on model problems and applications in text analysis.},
author = {Gleicher, Michael},
doi = {10.1109/TVCG.2013.157},
file = {:Users/smcgregor/Documents/Mendeley Desktop/explainers-preprint.pdf:pdf},
isbn = {1077-2626 VO - 19},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {High-dimensional spaces,exploration,support vector machines},
number = {12},
pages = {2042--2051},
pmid = {24051770},
title = {{Explainers: Expert explorations with crafted projections}},
volume = {19},
year = {2013}
}
@misc{Goldberg,
author = {Goldberg, Ian},
howpublished = {http://www.cypherpunks.ca/otr/},
title = {{Off-the-Record Messaging}},
url = {http://www.cypherpunks.ca/otr/}
}
@article{Goldberger2004,
abstract = {In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classification algorithm. The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and fast classification. Unlike other methods, our classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. The performance of the method is demonstrated on several data sets, both for metric learning and linear dimensionality reduction.},
author = {Goldberger, Jacob and Roweis, Sam and Hinton, Geoff and Salakhutdinov, Ruslan},
file = {:Users/smcgregor/Documents/Mendeley Desktop/2566-neighbourhood-components-analysis.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
keywords = {Learning/Statistics & Optimisation,Theory & Algorithms},
title = {{Neighbourhood Components Analysis}},
url = {http://eprints.pascal-network.org/archive/00001570/},
year = {2004}
}
@misc{Goodin2013,
author = {Goodin, Dan},
booktitle = {Ars Technica},
howpublished = {http://arstechnica.com/security/2013/08/gone-in-30-seconds-new-attack-plucks-secrets-from-https-protected-pages/},
title = {{Gone in 30 seconds: New attack plucks secrets from HTTPS-protected pages}},
url = {http://arstechnica.com/security/2013/08/gone-in-30-seconds-new-attack-plucks-secrets-from-https-protected-pages/},
urldate = {2013-12-11},
year = {2013}
}
@misc{Goodin2013a,
author = {Goodin, Dan},
booktitle = {Ars Technica},
howpublished = {http://arstechnica.com/security/2013/05/critical-ruby-on-rails-bug-exploited-in-wild-hacked-servers-join-botnet/},
title = {{Critical Ruby on Rails bug exploited in wild, hacked servers join botnet}},
url = {http://arstechnica.com/security/2013/05/critical-ruby-on-rails-bug-exploited-in-wild-hacked-servers-join-botnet/},
urldate = {2013-12-11},
year = {2013}
}
@misc{Google2013a,
abstract = {Compromised Gmail account Your account may have been compromised if you’ve experienced any of the following issues: Your contacts have received suspicious messages from your address Contacts and/or mail have gone missing, and you’ve already tried the tips on our missing messages troubleshooter, such as searching your mail and checking your settings You’ve received a warning about suspicious activity from your Last account activity I can still sign in to my account I can’t sign in to my account Please note that we aren't able to provide you with information about attempted logins to your account including, but not limited to, the IP address1 from which the attempted login was made, and the time and date attempted logins occurred.},
author = {Google},
booktitle = {Gmail Help},
howpublished = {http://support.google.com/mail/bin/answer.py?hl=en&answer=50270},
title = {{Compromised Gmail Account - Gmail Help}},
url = {http://support.google.com/mail/bin/answer.py?hl=en&answer=50270},
year = {2013}
}
@misc{Google2013,
abstract = {475,664 users},
author = {Google},
howpublished = {Shortened Google URL: "http://bit.ly/QYxFaZ"},
title = {{Tampermonkey}},
url = {https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=en},
year = {2013}
}
@article{gcddc-asmastcbd-2010,
author = {Gorissen, D and Couckuyt, I and Demeester, P and Dhaene, T and Crombecq, K},
journal = {Journal of Machine Learning Research},
pages = {2051--2055},
title = {{A Surrogate Modeling and Adaptive Sampling Toolbox for Computer Based Design}},
volume = {11},
year = {2010}
}
@article{gosink2007variable,
abstract = {Our ability to generate ever-larger, increasingly-complex data, has established the need for scalable methods that identify, and provide insight into, important variable trends and interactions. Query-driven methods are among the small subset of techniques that are able to address both large and highly complex datasets. This paper presents a new method that increases the utility of query-driven techniques by visually conveying statistical information about the trends that exist between variables in a query. In this method, correlation fields, created between pairs of variables, are used with the cumulative distribution functions of variables expressed in a users query. This integrated use of cumulative distribution functions and correlation fields visually reveals, with respect to the solution space of the query, statistically important interactions between any three variables, and allows for trends between these variables to be readily identified. We demonstrate our method by analyzing interactions between variables in two flame-front simulations.},
author = {Gosink, L J and Anderson, J C and {Wes Bethel}, E and Joy, K I},
file = {:Users/smcgregor/Documents/Mendeley Desktop/04376167.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1400--1407},
publisher = {IEEE},
title = {{Variable interactions in query-driven visualization}},
volume = {13},
year = {2007}
}
@article{green_2006,
author = {Green, T R G and Blandford, A E and Church, L and Roast, C R and Clarke, S},
doi = {10.1016/j.jvlc.2006.04.004},
issn = {1045-{926X}},
journal = {Journal of Visual Languages & Computing},
month = {aug},
number = {4},
pages = {328--365},
shorttitle = {Ten Years of Cognitive Dimensions Ten Years of Cog},
title = {{Cognitive dimensions: Achievements, new directions, and open questions}},
url = {http://www.sciencedirect.com/science/article/pii/S1045926X06000280},
volume = {17},
year = {2006}
}
@article{green_1996,
author = {Green, T R G and Petre, M and Others},
journal = {Journal of visual languages and computing},
number = {2},
pages = {131--174},
title = {{Usability Analysis of Visual Programming Environments: A 'Cognitive Dimensions' Framework}},
volume = {7},
year = {1996}
}
@misc{Greenwald2013,
author = {Greenwald, Glenn and MacAskill, Ewen},
booktitle = {The Guardian},
howpublished = {http://www.guardian.co.uk/world/2013/jun/06/us-tech-giants-nsa-data},
month = {jun},
title = {{NSA Prism program taps in to user data of Apple, Google and others}},
url = {http://www.guardian.co.uk/world/2013/jun/06/us-tech-giants-nsa-data},
year = {2013}
}
@article{Gregorio2013,
abstract = {In the field of wildfire risk management the so-called burn probability maps (BPMs) are increasingly used with the aim of estimating the probability of each point of a landscape to be burned under certain environmental conditions. Such BPMs are usually computed through the explicit simulation of thousands of fires using fast and accurate models. However, even adopting the most optimized algorithms, the building of simulation-based BPMs for large areas results in a highly intensive computational process that makes mandatory the use of high performance computing. In this paper, General-Purpose Computation with Graphics Processing Units (GPGPU) is applied, in conjunction with a wildfire simulation model based on the Cellular Automata approach, to the process of BPM building. Using three different GPGPU devices, the paper illustrates several implementation strategies to speedup the overall mapping process and discusses some numerical results obtained on a real landscape. ?? 2013 Elsevier Inc. All rights reserved.},
author = {Gregorio, Salvatore Di and Filippone, Giuseppe and Spataro, William and Trunfio, Giuseppe a.},
doi = {10.1016/j.jpdc.2013.03.014},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Accelerating-wildfire-susceptibility-mapping-through-GPGPU_2013_Journal-of-Parallel-and-Distributed-Computing.pdf:pdf},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Cellular automata,GPGPU,Hazard maps,Wildfire simulation,Wildfire susceptibility},
number = {8},
pages = {1183--1194},
publisher = {Elsevier Inc.},
title = {{Accelerating wildfire susceptibility mapping through GPGPU}},
url = {http://dx.doi.org/10.1016/j.jpdc.2013.03.014\nhttp://ac.els-cdn.com/S0743731513000580/1-s2.0-S0743731513000580-main.pdf?_tid=48af94a0-3089-11e5-9673-00000aacb35f&acdnat=1437580369_8a157251ec1529f8783ff5f504d52ecc},
volume = {73},
year = {2013}
}
@inproceedings{Grinberg2014,
annote = {Policy search is used in combination with predctive modeling to produce a strong policy for reservior management.

Tell Tom: I don't think the comparison between policies in the chart is exactly fair because it is DP versus policy search with weather prediction. If DP were to benefit from the same weather prediction it would likely be able to produce smoother policies.},
author = {Grinberg, Yuri and Precup, Doina and Gendreau, Michel},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS 2014)},
file = {:Users/smcgregor/Documents/Mendeley Desktop/5446-optimizing-energy-production-using-policy-search-and-predictive-state-representations.pdf:pdf},
pages = {1--9},
title = {{Optimizing Energy Production Using Policy Search and Predictive State Representations}},
year = {2014}
}
@article{Groce2014,
abstract = {How do you test a program when only a single user, with no expertise in software testing, is able to determine if the program is performing correctly? Such programs are common today in the form of machine-learned classifiers. We consider the problem of testing this common kind of machine-generated program when the only oracle is an end user: e.g., only you can determine if your email is properly filed. We present test selection methods that provide very good failure rates even for small test suites, and show that these methods work in both large-scale random experiments using a “gold standard” and in studies with real users. Our methods are inexpensive and largely algorithm-independent. Key to our methods is an exploitation of properties of classifiers that is not possible in traditional software testing. Our results suggest that it is plausible for time-pressured end users to interactively detect failures-even very hard-to-find failures-without wading through a large number of successful (and thus less useful) tests. We additionally show that some methods are able to find the arguably most difficult-to-detect faults of classifiers: cases where machine learning algorithms have high confidence in an incorrect result.},
annote = {Gives recommendations for which classified instances the user should review. This is based on one of several possible metrics that are compared theoretically and experimentally.},
author = {Groce, Alex and Kulesza, Todd and Zhang, Chaoqiang and Shamasunder, Shalini and Burnett, Margaret and Wong, Weng-Keen and Stumpf, Simone and Das, Shubhomoy and Shinsel, Amber and Bice, Forrest and McIntosh, Kevin},
doi = {10.1109/TSE.2013.59},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06682887.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {3},
pages = {307--323},
title = {{You Are the Only Possible Oracle: Effective Test Selection for End Users of Interactive Machine Learning Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6682887},
volume = {40},
year = {2014}
}
@inproceedings{Guarnieri2009,
author = {Guarnieri, Salvatore and Livshits, Benjamin},
booktitle = {Proceedings of the 19th USENIX Conference on Security Symposium},
file = {:Users/smcgregor/Documents/Mendeley Desktop/guarnieri.pdf:pdf},
title = {{GATEKEEPER : Mostly Static Enforcement of Security and Reliability Policies for JavaScript Code}},
url = {http://static.usenix.org/events/sec09/tech/full_papers/sec09_javascript.pdf},
year = {2009}
}
@article{Guestrin2003,
abstract = {This paper addresses the problem of planning under uncertainty in large Markov Decision Processes (MDPs). Factored MDPs represent a complex state space using state variables and the transition model using a dynamic Bayesian network. This representation often allows an exponential reduction in the representation size of structured MDPs, but the complexity of exact solution algorithms for such MDPs can grow exponentially in the representation size. In this paper, we present two approximate solution algorithms that exploit structure in factored MDPs. Both use an approximate value function represented as a linear combination of basis functions, where each basis function involves only a small subset of the domain variables. A key contribution of this paper is that it shows how the basic operations of both algorithms can be performed efficiently in closed form, by exploiting both additive and context-specific structure in a factored MDP. A central element of our algorithms is a novel linear program decomposition technique, analogous to variable elimination in Bayesian networks, which reduces an exponentially large LP to a provably equivalent, polynomial-sized one. One algorithm uses approximate linear programming, and the second approximate dynamic programming. Our dynamic programming algorithm is novel in that it uses an approximation based on max-norm, a technique that more directly minimizes the terms that appear in error bounds for approximate MDP algorithms. We provide experimental results on problems with over 10^40 states, demonstrating a promising indication of the scalability of our approach, and compare our algorithm to an existing state-of-the-art approach, showing, in some problems, exponential gains in computation time.},
archivePrefix = {arXiv},
arxivId = {1106.1822},
author = {Guestrin, Carlos and Koller, Daphne and Parr, Ronald and Venkataraman, Shobha},
doi = {10.1613/jair.1000},
eprint = {1106.1822},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Guestrin+al-JAIR03.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
number = {c},
pages = {399--468},
title = {{Efficient solution algorithms for factored MDPs}},
volume = {19},
year = {2003}
}
@inproceedings{Guha2008,
author = {Guha, Saikat and Tang, Kevin and Francis, Paul},
booktitle = {Proceedings of the first workshop on Online social networks},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Guha, Tang, Francis - 2008 - NOYB Privacy in online social networks.pdf:pdf},
isbn = {9781605581828},
keywords = {cloud computing,noyb,privacy},
pages = {49--54},
title = {{NOYB: Privacy in online social networks}},
url = {http://dl.acm.org/citation.cfm?id=1397747},
year = {2008}
}
@inproceedings{guo2009model,
abstract = {Discovering and extracting linear trends and correlations in datasets is very important for analysts to understand multivariate phenomena. However, current widely used multivariate visualization techniques, such as parallel coordinates and scatterplot matrices, fail to reveal and illustrate such linear relationships intuitively, especially when more than 3 variables are involved or multiple trends coexist in the dataset. We present a novel multivariate model parameter space visualization system that helps analysts discover single and multiple linear patterns and extract subsets of data that fit a model well. Using this system, analysts are able to explore and navigate in model parameter space, interactively select and tune patterns, and refine the model for accuracy using computational techniques. We build connections between model space and data space visually, allowing analysts to employ their domain knowledge during exploration to better interpret the patterns they discover and their validity. Case studies with real datasets are used to investigate the effectiveness of the visualizations.},
author = {Guo, Z and Ward, M O and Rundensteiner, E A},
booktitle = {Visual Analytics Science and Technology, 2009. VAST 2009. IEEE Symposium on},
file = {:Users/smcgregor/Documents/Mendeley Desktop/05333431.pdf:pdf},
organization = {IEEE},
pages = {75--82},
title = {{Model space visualization for multivariate linear trend discovery}},
year = {2009}
}
@article{Gurbat2012,
abstract = {In a variety of application areas, the use of simulation steering in decision making is limited at best. Research focusing on this problem suggests that most user interfaces are too complex for the end user. Our goal is to let users create and investigate multiple, alternative scenarios without the need for special simulation expertise. To simplify the specification of parameters, we move from a traditional manipulation of numbers to a sketch-based input approach. Users steer both numeric parameters and parameters with a spatial correspondence by sketching a change onto the rendering. Special visualizations provide immediate visual feedback on how the sketches are transformed into boundary conditions of the simulation models. Since uncertainty with respect to many intertwined parameters plays an important role in planning, we also allow the user to intuitively setup complete value ranges, which are then automatically transformed into ensemble simulations. The interface and the underlying system were developed in collaboration with experts in the field of flood management. The real-world data they have provided has allowed us to construct scenarios used to evaluate the system. These were presented to a variety of flood response personnel, and their feedback is discussed in detail in the paper. The interface was found to be intuitive and relevant, although a certain amount of training might be necessary.},
author = {Gurbat, Roman and Sadransky, Bernhard and Gr, M Eduard},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Gurbat, Sadransky, Gr - 2012 - Sketching Uncertainty into Simulations.pdf:pdf},
number = {12},
pages = {2255--2264},
title = {{Sketching Uncertainty into Simulations}},
volume = {18},
year = {2012}
}
@article{haber1990visualization,
annote = {Gives visualization pipeline.},
author = {Haber, R B and McNabb, D A},
journal = {Visualization in scientific computing},
pages = {93},
title = {{Visualization idioms: A conceptual model for scientific visualization systems}},
volume = {74},
year = {1990}
}
@article{Habrard2015,
author = {Habrard, Amaury and Curien, Laboratoire Hubert},
file = {:Users/smcgregor/Documents/Mendeley Desktop/5687-regressive-virtual-metric-learning.pdf:pdf},
journal = {Advances in neural information processing systems},
pages = {1--9},
title = {{Regressive Virtual Metric Learning}},
year = {2015}
}
@article{Hallak2015,
author = {Hallak, Assaf and Mann, Timothy},
file = {:Users/smcgregor/Documents/Mendeley Desktop/icml2015_hallak15.pdf:pdf},
journal = {Proceedings of the 32nd International Conference on Machine Learning},
title = {{Off-policy Model-based Learning under Unknown Factored Dynamics}},
volume = {37},
year = {2015}
}
@unpublished{Hanrahan2012,
author = {Hanrahan, Pat and Scientist, Chief and Software, Tableau},
file = {:Users/smcgregor/Documents/Mendeley Desktop/selecting-visual-analytics-application_2.pdf:pdf},
institution = {Tableau Software},
title = {{Selecting a Visual Analytics Application}},
year = {2012}
}
@misc{Harik2013,
author = {Harik, Georges},
howpublished = {https://imo.im/},
title = {{imo Messenger}},
url = {https://imo.im/},
urldate = {2013-12-11},
year = {2013}
}
@article{Hefny2015,
annote = {&quot;Predicting future from past and future+1 from past allows you to form an estimate of system dynamics.&quot;},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.05310v1},
author = {Hefny, Ahmed and Downey, Carlton and Gordon, Geoffrey J},
eprint = {arXiv:1505.05310v1},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1505.05310v1.pdf:pdf},
pages = {1--32},
title = {{A New View of Predictive State Methods for Dynamical System Learning}},
year = {2015}
}
@article{heinrich2009continuous,
abstract = {Typical scientific data is represented on a grid with appropriate interpolation or approximation schemes,defined on a continuous domain. The visualization of such data in parallel coordinates may reveal patterns latently contained in the data and thus can improve the understanding of multidimensional relations. In this paper, we adopt the concept of continuous scatterplots for the visualization of spatially continuous input data to derive a density model for parallel coordinates. Based on the point-line duality between scatterplots and parallel coordinates, we propose a mathematical model that maps density from a continuous scatterplot to parallel coordinates and present different algorithms for both numerical and analytical computation of the resulting density field. In addition, we show how the 2-D model can be used to successively construct continuous parallel coordinates with an arbitrary number of dimensions. Since continuous parallel coordinates interpolate data values within grid cells, a scalable and dense visualization is achieved, which will be demonstrated for typical multi-variate scientific data.},
author = {Heinrich, J and Weiskopf, D},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Heinrich, Weiskopf - 2009 - Continuous parallel coordinates.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1531--1538},
publisher = {IEEE},
title = {{Continuous parallel coordinates}},
volume = {15},
year = {2009}
}
@article{Heninger2012,
author = {Heninger, Nadia and Durumeric, Z},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Heninger, Durumeric - 2012 - Mining your Ps and Qs Detection of widespread weak keys in network devices.pdf:pdf},
journal = {Proceedings of the 21st USENIX Conference on Security Symposium {\ldots}},
title = {{Mining your Ps and Qs: Detection of widespread weak keys in network devices}},
url = {https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final228.pdf},
year = {2012}
}
@article{Hertz2004,
abstract = {The performance of graph based clusteringmeth- ods critically depends on the quality of the dis- tance function, used to compute similarities be- tween pairs of neighboring nodes. In this pa- per we learn distance functions by training bi- nary classifiers with margins. The classifiers are defined over the product space of pairs of points and are trained to distinguish whether two points come from the same class or not. The signed margin is used as the distance value. Our main contribution is a distance learning method (DistBoost), which combines boosting hypothe- ses over the product space with a weak learner based on partitioning the original feature space. Each weak hypothesis is a Gaussian mixture model computed using a semi-supervised con- strained EM algorithm, which is trained using both unlabeled and labeled data. We also con- sider SVM and decision trees boosting as mar- gin based classifiers in the product space. We experimentally compare the margin based dis- tance functions with other existing metric learn- ingmethods, andwith existing techniques for the direct incorporation of constraints into various clustering algorithms. Clustering performance is measured on some benchmark databases from the UCI repository, a sample from the MNIST database, and a data set of color images of ani- mals. In most cases the DistBoost algorithm sig- nificantly and robustly outperformed its competi- tors.},
annote = {todo: start reading from 2.2

This algorithm has significant advantages over other learning a transformation matrix as in learning a Mahalonobis distance metric. Specifically, it is able to capture non-linearities by partitioning the input space. I still need to read and learn more...

todo: read about C4.5},
author = {Hertz, Tomer and Bar-Hillel, Aharon and Weinshall, Daphna},
doi = {10.1145/1015330.1015389},
file = {:Users/smcgregor/Documents/Mendeley Desktop/distboost-icml.pdf:pdf},
isbn = {1581138285},
journal = {In Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {393--400},
title = {{Boosting Margin Based Distance Functions for Clustering}},
year = {2004}
}
@article{Hoang2010,
abstract = {The destruction caused by wildfires has led to the development of various models that try to predict the effects of this phenomenon. However, as the computational complexity of these models increases, their utility for real-time applications diminishes. Fortunately, the burgeoning processing power of the graphics processing unit can not only mitigate these concerns but also allow for high-fidelity visualization. We present VFire, an immersive wildfire simulation and visualization system. Users are placed in a virtual environment generated from real-world data regarding topology and vegetation. There they can simulate wildfires in real-time under various conditions. They can then experiment with various suppression techniques, such as fire breaks and water drops. The simulation is performed on the graphics card, which then provides visualization of the results. The system is intended to train fire chiefs in planning containment efforts and to educate firefighters, policymakers, and the general public about wildfire behavior and the effects of preventative measures. ?? 2010 Elsevier Ltd. All rights reserved.},
author = {Hoang, Roger V. and Sgambati, Matthew R. and Brown, Timothy J. and Coming, Daniel S. and Harris, Frederick C.},
doi = {10.1016/j.cag.2010.09.014},
file = {:Users/smcgregor/Documents/Mendeley Desktop/VFire-Immersive-wildfire-simulation-and-visualization_2010_Computers-Graphics.pdf:pdf},
isbn = {0097-8493},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {Collaborative virtual environment,Computer-based training,Earth and atmospheric sciences,Virtual reality},
number = {6},
pages = {655--664},
publisher = {Elsevier},
title = {{VFire: Immersive wildfire simulation and visualization}},
url = {http://dx.doi.org/10.1016/j.cag.2010.09.014},
volume = {34},
year = {2010}
}
@article{Houtman2013,
author = {Houtman, Rachel M. and Montgomery, Claire A. and Gagnon, Aaron R. and Calkin, David E. and Dietterich, Thomas G. and McGregor, Sean and Crowley, Mark},
file = {:Users/smcgregor/Documents/Mendeley Desktop/WF12157.pdf:pdf},
journal = {International Journal of Wildland Fire},
number = {7},
pages = {871--882},
title = {{Allowing a Wildfire to Burn: Estimating the Effect on Future Fire Suppression Costs}},
volume = {22},
year = {2013}
}
@article{Howard2011,
author = {Howard, Philip N. and Hussain, Muzammil M.},
doi = {10.1353/jod.2011.0041},
file = {:Users/smcgregor/Documents/Mendeley Desktop/22.3.howard.pdf:pdf},
issn = {1086-3214},
journal = {Journal of Democracy},
number = {3},
pages = {35--48},
title = {{The Role of Digital Media}},
url = {http://muse.jhu.edu/content/crossref/journals/journal_of_democracy/v022/22.3.howard.html},
volume = {22},
year = {2011}
}
@article{Huang2012,
author = {Huang, LS and Moshchuk, Alex},
file = {:Users/smcgregor/Documents/Mendeley Desktop/sec12-final39.pdf:pdf},
journal = {Proceedings of the 21st USENIX Conference on Security Symposium},
title = {{Clickjacking: Attacks and Defenses}},
url = {https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final39.pdf},
year = {2012}
}
@article{huber1985projection,
abstract = {Projection pursuit is concerned with "Interesting" projections of high dimensional data sets, with finding such projections by machine, and with using them for nonparametric fitting and other data-analytics purposes. This survey attempts to put the fascinating problems and ramifications of projection pursuit-which range from principal components, multidimensional scaling, factor analysis, nonparametric regression, density estimation and deconvolution of time series to computer tomography and problems in pure mathematics-into a coherent perspective.},
author = {Huber, P J},
journal = {The annals of Statistics},
pages = {435--475},
publisher = {JSTOR},
title = {{Projection Pursuit}},
year = {1985}
}
@misc{Incorporated2013,
author = {Incorporated, Reddit},
howpublished = {http://www.reddit.com/wiki/useragreement},
title = {{Reddit User Agreement}},
url = {http://www.reddit.com/wiki/useragreement},
year = {2013}
}
@article{Inselberg1997,
abstract = {The display of multivariate datasets in parallel coordinates, transforms the search for relations among the variables into a 2-D pattern recognition problem. This is the basis for the application to visual data mining. The knowledge discovery process together with some general guidelines are illustrated on a dataset from the production of a VLSI chip. The special strength of parallel coordinates is in modeling relations. As an example, a simplified economic model is constructed with data from various economic sectors of a real country. The visual model shows the interelationship and dependencies between the sectors, circumstances where there is competition for the same resource, and feasible economic policies. Interactively, the model can be used to do trade-off analyses, discover sensitivities, do approximate optimization, monitor (as in a process) and provide decision support.},
author = {Inselberg, A},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Inselberg - 1997 - Multidimensional detective.pdf:pdf},
journal = {Information Visualization, 1997. Proceedings., {\ldots}},
keywords = {2D pattern recognition problem,VLSI chip,approximate optimization,competition,data visualisation,decision support,economic model,economic policies,economic sectors,knowledge discovery,modeling relations,monitoring,multidimensional detective,multivariate dataset display,parallel coordinates,trade-off analyses,visual data mining},
title = {{Multidimensional detective}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=636793},
year = {1997}
}
@book{Inselberg2009,
author = {Inselberg, A},
isbn = {9780387215075},
publisher = {Springer},
title = {{Parallel Coordinates: Visual Multidimensional Geometry and Its Applications}},
year = {2009}
}
@article{Inselberg1990,
abstract = {A methodology for visualizing analytic and synthetic geometry in RN is presented. It is based on a system of parallel coordinates which induces a non-projective mapping between N-Dimensional and 2-Dimensional sets. Hypersurfaces are represented by their planar images which have some geometrical properties analogous to the properties of the hypersurface that they represent. A point ← → line duality when N = 2 generalizes to lines and hyperplanes enabling the representation of polyhedra in RN. The representation of a class of convex and non-convex hypersurfaces is discussed together with an algorithm for constructing and displaying any interior point. The display shows some local properties of the hypersurface and provides information on the point's proximity to the boundary. Applications to Air Traffic Control, Robotics, Computer Vision, Computational Geometry, Statistics, Instrumentation and other areas are discussed.},
author = {Inselberg, A and Dimsdale, B},
file = {:Users/smcgregor/Documents/Mendeley Desktop/p361-inselbert.pdf:pdf},
journal = {Proceedings of the 1st conference on {\ldots}},
title = {{Parallel coordinates: a tool for visualizing multi-dimensional geometry}},
url = {http://dl.acm.org/citation.cfm?id=949588},
year = {1990}
}
@article{Inselberg1985,
annote = {The original parallel coordinates paper},
author = {Inselberg, Alfred},
doi = {10.1007/BF01898350},
issn = {0178-2789},
journal = {The Visual Computer},
keywords = {Convexity; Duality; Parallel coordinates; Intellig},
number = {2},
pages = {69--91},
publisher = {Springer-Verlag},
title = {{The plane with parallel coordinates}},
url = {http://dx.doi.org/10.1007/BF01898350},
volume = {1},
year = {1985}
}
@article{Itani2009,
author = {Itani, Wassim and Kayssi, Ayman and Chehab, Ali},
doi = {10.1109/DASC.2009.139},
file = {:Users/smcgregor/Documents/Mendeley Desktop/PrivacyAsAService.pdf:pdf},
isbn = {978-1-4244-5420-4},
journal = {2009 Eighth IEEE International Conference on Dependable, Autonomic and Secure Computing},
keywords = {-privacy,cloud computing,cryptographic coprocessors,privacy, cloud computing, cryptographic coprocesso},
month = {dec},
pages = {711--716},
publisher = {Ieee},
title = {{Privacy as a Service: Privacy-Aware Data Storage and Processing in Cloud Computing Architectures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5380584},
year = {2009}
}
@article{Jackson2007,
author = {Jackson, Collin and Wang, HJ},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Subspace.pdf:pdf},
isbn = {9781595936547},
journal = {16th international conference on World Wide Web},
keywords = {access control,same origin policy,trust,web services},
title = {{Subspace: secure cross-domain communication for web mashups}},
url = {http://dl.acm.org/citation.cfm?id=1242572.1242655},
year = {2007}
}
@article{Jacob2012,
author = {Jacob, Gregoire and Kruegel, Christopher},
file = {:Users/smcgregor/Documents/Mendeley Desktop/sec12-final30.pdf:pdf},
journal = {Proceedings of the 21st USENIX Conference on Security Symposium},
title = {{PUB CRAWL: Protecting Users and Businesses from CRAWLers}},
year = {2012}
}
@inproceedings{Jahid2011,
author = {Jahid, Sonia and Mittal, P and Borisov, Nikita},
booktitle = {Proceedings of the 6th ACM Symposium on Information, Computer and Communications Security},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Jahid, Mittal, Borisov - 2011 - EASiER Encryption-based access control in social networks with efficient revocation.pdf:pdf},
isbn = {9781450305648},
keywords = {access control,proxy,revocation,social network},
pages = {411--415},
title = {{EASiER: Encryption-based access control in social networks with efficient revocation}},
url = {http://dl.acm.org/citation.cfm?id=1966970},
year = {2011}
}
@article{Jang2012,
author = {Jang, Dongseok and Tatlock, Zachary and Lerner, S},
file = {:Users/smcgregor/Documents/Mendeley Desktop/sec12-final217.pdf:pdf},
journal = {Proceedings of the 21st USENIX Conference on Security Symposium},
title = {{Establishing browser security guarantees through formal shim verification}},
url = {http://dl.acm.org/citation.cfm?id=2362793.2362801},
year = {2012}
}
@article{Johansson,
author = {Johansson, J. and Treloar, R. and Jern, M.},
doi = {10.1109/IV.2004.1320124},
file = {:Users/smcgregor/Documents/Mendeley Desktop/01320124.pdf:pdf},
isbn = {0-7695-2177-0},
journal = {Proceedings. Eighth International Conference on Information Visualisation, 2004. IV 2004.},
keywords = {interactive visualization,linked views,parallel coordinates,unsupervised clustering},
pages = {52--57},
publisher = {Ieee},
title = {{Integration of unsupervised clustering, interaction and parallel coordinates for the exploration of large multivariate data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1320124}
}
@article{Johnston2008,
author = {Johnston, Paul and Kelso, Joel and Milne, George J.},
doi = {10.1071/WF06147},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Johnston et al. - 2008 - Efficient simulation of wildfire spread on an irre.pdf:pdf},
issn = {1049-8001},
journal = {International Journal of Wildland Fire},
number = {5},
pages = {614},
title = {{Efficient simulation of wildfire spread on an irregular grid}},
url = {http://www.publish.csiro.au/?paper=WF06147},
volume = {17},
year = {2008}
}
@article{joia2011local,
abstract = {Multidimensional projection techniques have experienced many improvements lately, mainly regarding computational times and accuracy. However, existing methods do not yet provide flexible enough mechanisms for visualization-oriented fully interactive applications. This work presents a new multidimensional projection technique designed to be more flexible and versatile than other methods. This novel approach, called Local Affine Multidimensional Projection (LAMP), relies on orthogonal mapping theory to build accurate local transformations that can be dynamically modified according to user knowledge. The accuracy, flexibility and computational efficiency of LAMP is confirmed by a comprehensive set of comparisons. LAMP’s versatility is exploited in an application which seeks to correlate data that, in principle, has no connection as well as in visual exploration of textual documents.},
author = {Joia, P and Paulovich, F V and Coimbra, D and Cuminato, J A and Nonato, L G},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06065024 17-44-47.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2563--2571},
publisher = {IEEE},
title = {{Local affine multidimensional projection}},
volume = {17},
year = {2011}
}
@article{Jordan2013,
annote = {YouTube video by Michael Jordan.




https://www.youtube.com/watch?v=ZGhW2XyIB8s


* Article comes in three parts* Unifying theme is addressing the data/algorithm tradeoff
* In classical statistics, where one considers the increase in inferential accuracy as the number of data points grows, there is little or no consideration of computational complexity.
* Classical theory provides no guidance as to how to design an inferential strategy for limited time budgets.


PART 3


* In particular, we define a class TD(t(p), n(p), $\epsilon$(p)) of parameter estimation problems in which a p-dimensional parameter underlying an unknown population can be estimated with a risk of $\epsilon$(p), given n(p) independent and identically distributed (i.i.d.) samples using an inference procedure with runtime t(p).
* In this formalization, classical results in estimation theory can be viewed as emphasizing the tradeoffs between the second and third parameters (amount of data and risk). Our focus in this paper is to fix $\epsilon$(p) to some desired level of accuracy and to investigate the tradeoffs between the first two parameters, namely, runtime and dataset size.


Research Notes


* We are concerned with minimizing the number of samples so in some ways it is the opposite inspiration from Big Data and this paper: we would be willing to take the most expensive algorithms possible if they don't require additional samples.
* However, if we view the simulator as a component of the algorithm then we can weaken the simulator to obtain more samples. Current results concentrate on $\sim$simple parameter estimation tasks like denoising, but these tasks make the simplifying assumption that the data are generated i.i.d., switching to non-i.i.d. data remains an open problem.
* Need to figure out the minimal set of elements to fix, then vary over the sampling allocation.
* Todo: look at the field of sequential analysis, it provides tools to assess risk after a certain number of datapoints have arrived.






&quot;Computational and statistical tradeoffs via convex relaxation&quot;&quot;
http://www.pnas.org/content/110/13/E1181.full


In computer science, the growth of the number of data points is a source of “complexity” that must be tamed via algorithms or hardware, whereas in statistics, the growth of the number of data points is a source of “simplicity” in that inferences are generally stronger and asymptotic results can be invoked.


In classical statistics, where one considers the increase in inferential accuracy as the number of data points grows, there is little or no consideration of computational complexity.


Indeed, if one imposes the additional constraint, as is prevalent in real-world applications, that a certain level of inferential accuracy must be achieved within a limited time budget, classical theory provides no guidance as to how to design an inferential strategy.


Thus, there is little or no consideration of the idea that computation can be simplified in large datasets because of the enhanced inferential power in the data. In general, in computer science, datasets are not viewed formally as a resource on a par with time and space (such that the more of the resource, the better).


More generally, one would like to consider some notion of “algorithm weakening,” where as data accumulate, one can back off to simpler algorithmic strategies that nonetheless achieve a desired risk. The challenge is to do this in a theoretically sound manner.


We base our approach to this problem on the notion of a “time-data complexity class.” In particular, we define a class GraphicGraphic(t(p), n(p), $\epsilon$(p)) of parameter estimation problems in which a p-dimensional parameter underlying an unknown population can be estimated with a risk of $\epsilon$(p), given n(p) independent and identically distributed (i.i.d.) samples using an inference procedure with runtime t(p). Our definition parallels the definition of the time-space (TISP) complexity class in computational complexity theory for describing algorithmic tradeoffs between time and space resources (2).


Reflecting the fact that the space of all algorithms is poorly understood, we retain the focus on convex optimization from high-dimensional statistics, but we consider parameterized hierarchies of optimization procedures in which a form of algorithm weakening is obtained by using successively weaker outer approximations to convex sets.


This research landscape informs the qualitative nature of the statements on time-data tradeoffs we make in this paper. First, we will not attempt to prove combined lower bounds, as is traditionally done in the characterization of tradeoffs between physical quantities, involving n(p) and t(p) jointly; this is because obtaining a lower bound just on t(p) remains a substantial challenge. Hence, our time-data tradeoff results on the use of more efficient algorithms for larger datasets refer to a reduction in the upper bounds on runtimes of estimation procedures with increases in dataset size.},
author = {Jordan, Michael I.},
doi = {10.3150/12-BEJSP17},
file = {:Users/smcgregor/Documents/Mendeley Desktop/BEJSP17.pdf:pdf},
issn = {1350-7265},
journal = {Bernoulli},
month = {sep},
number = {4},
pages = {1378--1390},
title = {{On statistics, computation and scalability}},
url = {http://projecteuclid.org/euclid.bj/1377612856},
volume = {19},
year = {2013}
}
@phdthesis{Kakade2003,
author = {Kakade, SM},
file = {:Users/smcgregor/Documents/Mendeley Desktop/SK.pdf:pdf},
number = {March},
school = {University of London},
title = {{On the sample complexity of reinforcement learning}},
url = {http://www.ias.tu-darmstadt.de/uploads/Research/NIPS2006/SK.pdf},
year = {2003}
}
@inproceedings{Kandogan2001,
address = {New York, New York, USA},
author = {Kandogan, Eser},
booktitle = {Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '01},
doi = {10.1145/502512.502530},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Kandogan - 2001 - Visualizing multi-dimensional clusters, trends, and outliers using star coordinates.pdf:pdf},
isbn = {158113391X},
keywords = {Multi-dimensional visualization,knowledge discovery},
month = {aug},
pages = {107--116},
publisher = {ACM Press},
title = {{Visualizing multi-dimensional clusters, trends, and outliers using star coordinates}},
url = {http://dl.acm.org/citation.cfm?id=502512.502530},
year = {2001}
}
@article{Karpathy,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02078v1},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
eprint = {arXiv:1506.02078v1},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1506.02078v1.pdf:pdf},
pages = {1--13},
title = {{Visualizing and Understanding Recurrent Networks}}
}
@article{keim2002pixel,
abstract = {Simple presentation graphics are intuitive and easy-to-use, but show only highly aggregated data presenting only a very small number of data values (as in the case of bar charts) and may have a high degree of overlap occluding a significant portion of the data values (as in the case of the x-y plots). In this article, the authors therefore propose a generalization of traditional bar charts and x-y plots, which allows the visualization of large amounts of data. The basic idea is to use the pixels within the bars to present detailed information of the data records. The so-called pixel bar charts retain the intuitiveness of traditional bar charts while allowing very large data sets to be visualized in an effective way. It is shown that, for an effective pixel placement, a complex optimization problem has to be solved. The authors then present an algorithm which efficiently solves the problem. The application to a number of real-world e-commerce data sets shows the wide applicability and usefulness of this new idea, and a comparison to other well-known visualization techniques (parallel coordinates and spiral techniques) shows a number of clear advantages.},
author = {Keim, D A and Hao, M C and Dayal, U and Hsu, M},
journal = {Information Visualization},
number = {1},
pages = {20--34},
publisher = {SAGE Publications},
title = {{Pixel Bar Charts: A Visualization Technique for Very Large Multi-Attribute Data Sets†}},
volume = {1},
year = {2002}
}
@article{Keller2013,
author = {Keller, Thomas and Helmert, M},
journal = {Proceedings of the 23rd International Conference on Automated Planning and Scheduling (ICAPS13)},
title = {{Trial-based Heuristic Tree Search for Finite Horizon MDPs}},
url = {http://www.informatik.uni-freiburg.de/$\sim$ki/papers/keller-helmert-icaps2013.pdf},
year = {2013}
}
@article{Kennedy2000,
author = {Kennedy, MC and O'Hagan, A},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Biometrika-2000-Kennedy-1-13.pdf:pdf},
journal = {Biometrika},
keywords = {modeling},
mendeley-tags = {modeling},
pages = {1--13},
title = {{Predicting the output from a complex computer code when fast approximations are available}},
url = {http://biomet.oxfordjournals.org/content/87/1/1.short},
year = {2000}
}
@book{Kohonen2001,
author = {Kohonen, Teuvo},
booktitle = {Proceedings of the IEEE},
file = {:Users/smcgregor/Documents/Mendeley Desktop/cis-biennial-report-2002-2003-8.pdf:pdf},
publisher = {Springer-Verlag},
title = {{The self-organizing map}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=58325},
year = {2001}
}
@article{Kontaxis2012,
author = {Kontaxis, Georgios and Polychronakis, Michalis},
file = {:Users/smcgregor/Documents/Mendeley Desktop/sec12-final150.pdf:pdf},
journal = {Proceedings of the 21st USENIX Conference on Security Symposium},
title = {{Privacy-preserving social plugins}},
url = {https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final150.pdf},
year = {2012}
}
@article{koop2008viscomplete,
abstract = {Building visualization and analysis pipelines is a large hurdle in the adoption of visualization and workflow systems by domain scientists. In this paper, we propose techniques to help users construct pipelines by consensus-automatically suggesting completions based on a database of previously created pipelines. In particular, we compute correspondences between existing pipeline subgraphs from the database, and use these to predict sets of likely pipeline additions to a given partial pipeline. By presenting these predictions in a carefully designed interface, users can create visualizations and other data products more efficiently because they can augment their normal work patterns with the suggested completions. We present an implementation of our technique in a publicly-available, open-source scientific workflow system and demonstrate efficiency gains in real-world situations.},
author = {Koop, D and Scheidegger, C E and Callahan, S P and Freire, J and Silva, C T},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Koop et al. - 2008 - Viscomplete Automating suggestions for visualization pipelines.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1691--1698},
publisher = {IEEE},
title = {{Viscomplete: Automating suggestions for visualization pipelines}},
volume = {14},
year = {2008}
}
@inproceedings{Kulesza2015,
author = {Kulesza, Todd and Burnett, Margaret and Wong, Weng-keen and Stumpf, Simone},
booktitle = {ACM Conference on Intelligent User Interfaces},
file = {:Users/smcgregor/Documents/Mendeley Desktop/iui15-elucidebug.pdf:pdf},
isbn = {9781450333061},
keywords = {Interactive machine learning,end user programming.},
title = {{Principles of Explanatory Debugging to Personalize Interactive Machine Learning}},
year = {2015}
}
@article{Kumar2007,
abstract = {The k-nearest neighbour (kNN) rule is a simple and effective method for multi-way classification that is much used in Computer Vision. However, its performance depends heavily on the distance metric being employed. The recently proposed large margin nearest neighbour (LMNN) classifier [21] learns a distance metric for kNN classification and thereby improves its accuracy. Learning involves optimizing a convex problem using semidefinite programming (SDP). We extend the LMNN framework to incorporate knowledge about invariance of the data. The main contributions of our work are three fold: (i) Invariances to multivariate polynomial transformations are incorporated without explicitly adding more training data during learning - these can approximate common transformations such as rotations and affinities; (ii) the incorporation of different regularizes on the parameters being learnt; and (Hi) for all these variations, we show that the distance metric can still be obtained by solving a convex SDP problem. We call the resulting formulation invariant LMNN (lLMNN) classifier. We test our approach to learn a metric for matching (i) feature vectors from the standard Iris dataset; and (ii) faces obtained from TV video (an episode of 'Buffy the Vampire Slayer'). We compare our method with the state of the art classifiers and demonstrate improvements.},
annote = {In the wildfire problem, we could potentially use invariance of the landscape to increase the sampel size.},
author = {Kumar, M. Pawan and Torr, P. H S and Zisserman, a.},
doi = {10.1109/ICCV.2007.4409041},
file = {:Users/smcgregor/Documents/Mendeley Desktop/kumar07.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {8},
title = {{An invariant large margin nearest neighbour classifier}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4409041},
year = {2007}
}
@article{Lalas2012,
author = {Lalas, Efthymios and Papathanasiou, Anastasios and Lambrinoudakis, Costas},
doi = {10.1109/PCi.2012.57},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06377379.pdf:pdf},
isbn = {978-1-4673-2720-6},
journal = {2012 16th Panhellenic Conference on Informatics},
keywords = {-privacy,social networking sites,traceability},
month = {oct},
pages = {127--132},
publisher = {Ieee},
title = {{Privacy and Traceability in Social Networking Sites}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6377379},
year = {2012}
}
@article{Langford2005,
address = {New York, New York, USA},
author = {Langford, John and Zadrozny, Bianca},
doi = {10.1145/1102351.1102411},
file = {:Users/smcgregor/Documents/Mendeley Desktop/langford-zadrozny-relating-reinforcement-learning-performance-to-classification-performance-icml2005.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning},
pages = {473--480},
publisher = {ACM Press},
title = {{Relating reinforcement learning performance to classification performance}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102411},
year = {2005}
}
@inproceedings{Lanning2000,
address = {New York, New York, USA},
author = {Lanning, Tom and Wittenburg, Kent and Heinrichs, Michael and Fyock, Christina and Li, Glenn},
booktitle = {Proceedings of the working conference on Advanced visual interfaces - AVI '00},
doi = {10.1145/345513.345306},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Lanning et al. - 2000 - Multidimensional information visualization through sliding rods.pdf:pdf},
isbn = {1581132522},
keywords = {focus+context,multidimensional information visualization,shopping interfaces,visual interface design},
month = {may},
pages = {173--180},
publisher = {ACM Press},
title = {{Multidimensional information visualization through sliding rods}},
url = {http://dl.acm.org/citation.cfm?id=345513.345306},
year = {2000}
}
@misc{Lawrence,
author = {Lawrence, Michael},
booktitle = {ggobi.org},
file = {:Users/smcgregor/Documents/Mendeley Desktop/pipeline-design.pdf:pdf},
pages = {1--5},
title = {{The GGobi Data Pipeline}},
url = {http://www.ggobi.org/docs/pipeline-design.pdf},
urldate = {2012-12-09}
}
@article{LeBras2014,
abstract = {Newly-discovered materials have been central to recent technological advances. They have contributed significantly to breakthroughs in electronics, renewable energy and green buildings, and overall, have promoted the advancement of global human welfare. Yet, only a fraction of all possible materials have been explored. Accelerating the pace of discovery of materials would foster technological innovations, and would potentially address pressing issues in sustainability, such as energy production or consumption. The bottleneck of this discovery cycle lies, however, in the analysis of the materials data. As materials scientists have recently devised techniques to efficiently create thousands of materials and experimentalists have developed new methods and tools to characterize these materials, the limiting factor has become the data analysis itself. Hence, the goal of this paper is to stimulate the development of new computational techniques for the analysis of materials data, by bringing together the complimentary expertise of materials scientists and computer scientists. In collaboration with two major research laboratories in materials science, we provide the first publicly available dataset for the phase map identification problem. In addition, we provide a parameterized synthetic data generator to assess the quality of proposed approaches, as well as tools for data visualization and solution evaluation.},
author = {{Le Bras}, Ronan and Bernstein, Richard and Gregoire, John M. and Suram, Santosh K. and Gomes, Carla P. and Selman, Bart and van Dover, Robert B.},
file = {:Users/smcgregor/Documents/Mendeley Desktop/LeBras2014.pdf:pdf},
isbn = {9781577356776},
journal = {AAAI},
keywords = {Computational Sustainability and AI},
pages = {438--443},
title = {{A Computational Challenge Problem in Materials Discovery: Synthetic Problem Generator and Real-World Datasets}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8647},
year = {2014}
}
@misc{LeapEncryptionAccessProject2013,
author = {{Leap Encryption Access Project}},
howpublished = {https://leap.se/en/docs/design/nicknym},
title = {{LEAP Encryption Access Project: Nicknym}},
url = {https://leap.se/en/docs/design/nicknym},
urldate = {2013-12-11},
year = {2013}
}
@inproceedings{leblanc1990exploring,
abstract = {The authors present a tool for the display and analysis of N -dimensional data based on a technique called dimensional stacking. This technique is described. The primary goal is to create a tool that enables the user to project data of arbitrary dimensions onto a two-dimensional image. Of equal importance is the ability to control the viewing parameters, so that one can interactively adjust what ranges of values each dimension takes and the form in which the dimensions are displayed. This will allow an intuitive feel for the data to be developed as the database is explored. The system uses dimensional stacking, to collapse and N-dimension space down into a 2-D space and then render the values contained therein. Each value can then be represented as a pixel or rectangular region on a 2-D screen whose intensity corresponds to the data value at that point},
author = {LeBlanc, J and Ward, M O and Wittels, N},
booktitle = {Visualization, 1990. Visualization'90., Proceedings of the First IEEE Conference on},
organization = {IEEE},
pages = {230--237},
title = {{Exploring< e1> N</e1>-dimensional databases}},
year = {1990}
}
@article{Lee2007,
abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons. 1},
author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y.},
doi = {10.1.1.69.2112},
file = {:Users/smcgregor/Documents/Mendeley Desktop/NIPS2006_878.pdf:pdf},
isbn = {0262195682},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {2},
pages = {801},
pmid = {17051527},
title = {{Efficient sparse coding algorithms}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.2112&amp;rep=rep1&amp;type=pdf\nhttp://books.nips.cc/papers/txt/nips19/NIPS2006_0878.txt},
volume = {19},
year = {2007}
}
@article{Lee2006,
author = {Lee, Kanghoon and Kim, Kee-eung},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Tighter Value Function Bounds for Bayesian Reinforcement Learning.pdf:pdf},
title = {{Tighter Value Function Bounds for Bayesian Reinforcement Learning}},
year = {2006}
}
@article{lehmann2011features,
abstract = {Continuous Parallel Coordinates (CPC) are a contemporary visualization technique in order to combine several scalar fields, given over a common domain. They facilitate a continuous view for parallel coordinates by considering a smooth scalar field instead of a finite number of straight lines. We show that there are feature curves in CPC which appear to be the dominant structures of a CPC. We present methods to extract and classify them and demonstrate their usefulness to enhance the visualization of CPCs. In particular, we show that these feature curves are related to discontinuities in Continuous Scatterplots (CSP). We show this by exploiting a curve-curve duality between parallel and Cartesian coordinates, which is a generalization of the well-known point-line duality. Furthermore, we illustrate the theoretical considerations. Concluding, we discuss relations and aspects of the CPC's/CSP's features concerning the data analysis.},
author = {Lehmann, D J and Theisel, H},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06064954.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {1912--1921},
publisher = {IEEE},
title = {{Features in Continuous Parallel Coordinates}},
volume = {17},
year = {2011}
}
@article{Lekies,
author = {Lekies, Sebastian and Nikiforakis, Nick and Tighzert, Walter},
file = {:Users/smcgregor/Documents/Mendeley Desktop/demacro_raid2012.pdf:pdf},
journal = {securitee.org},
title = {{DEMACRO: Defense against Malicious Cross-domain Requests}},
url = {http://securitee.org/files/demacro_raid2012.pdf}
}
@article{lex2010comparative,
abstract = {When analyzing multidimensional, quantitative data, the comparison of two or more groups of dimensions is a common task. Typical sources of such data are experiments in biology, physics or engineering, which are conducted in different configurations and use replicates to ensure statistically significant results. One common way to analyze this data is to filter it using statistical methods and then run clustering algorithms to group similar values. The clustering results can be visualized using heat maps, which show differences between groups as changes in color. However, in cases where groups of dimensions have an a priori meaning, it is not desirable to cluster all dimensions combined, since a clustering algorithm can fragment continuous blocks of records. Furthermore, identifying relevant elements in heat maps becomes more difficult as the number of dimensions increases. To aid in such situations, we have developed Matchmaker, a visualization technique that allows researchers to arbitrarily arrange and compare multiple groups of dimensions at the same time. We create separate groups of dimensions which can be clustered individually, and place them in an arrangement of heat maps reminiscent of parallel coordinates. To identify relations, we render bundled curves and ribbons between related records in different groups. We then allow interactive drill-downs using enlarged detail views of the data, which enable in-depth comparisons of clusters between groups. To reduce visual clutter, we minimize crossings between the views. This paper concludes with two case studies. The first demonstrates the value of our technique for the comparison of clustering algorithms. In the second, biologists use our system to investigate why certain strains of mice develop liver disease while others remain healthy, informally showing the efficacy of our system when analyzing multidimensional data containing distinct groups of dimensions.},
author = {Lex, A and Streit, M and Partl, C and Kashofer, K and Schmalstieg, D},
file = {:Users/smcgregor/Documents/Mendeley Desktop/05613440.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1027--1035},
publisher = {IEEE},
title = {{Comparative analysis of multidimensional, quantitative data}},
volume = {16},
year = {2010}
}
@msthesis{liem-smlsbbs-2007,
author = {Liem, Rhea Patricia},
institution = {School of Engineering, Massachusetts Institute of Technology},
pages = {110},
title = {{Surrogate Modeling for Large-Scale Black-Box Systems}},
year = {2007}
}
@inproceedings{Lim2009,
abstract = {Context-aware intelligent systems employ implicit inputs, and make decisions based on complex rules and machine learning models that are rarely clear to users. Such lack of system intelligibility can lead to loss of user trust, satisfaction and acceptance of these systems. However, automatically providing explanations about a system's decision process can help mitigate this problem. In this paper we present results from a controlled study with over 200 participants in which the effectiveness of different types of explanations was examined. Participants were shown examples of a system's operation along with various automatically generated explanations, and then tested on their understanding of the system. We show, for example, that explanations describing why the system behaved a certain way resulted in better understanding and stronger feelings of trust. Explanations describing why the system did not behave a certain way, resulted in lower understanding yet adequate performance. We discuss implications for the use of our findings in real-world context-aware applications.},
annote = {This paper examines the effectiveness of different types of explanations. In particular, they looked at explanations of why a system behaved a certain way and why they did not behave a certain way. They found both explanations improved understanding.},
author = {Lim, Brian Y. and Dey, Anind K. and Avrahami, Daniel},
booktitle = {Proceedings of the 27th international conference on Human factors in computing systems - CHI 09},
doi = {10.1145/1518701.1519023},
file = {:Users/smcgregor/Documents/Mendeley Desktop/p2119-lim.pdf:pdf},
isbn = {9781605582467},
keywords = {Intelligibility,context-aware,explanations},
pages = {2119--2129},
title = {{Why and why not explanations improve the intelligibility of context-aware intelligent systems}},
url = {http://dl.acm.org.prox.lib.ncsu.edu/citation.cfm?id=1518701.1519023},
year = {2009}
}
@article{livingston2011evaluation,
abstract = {Multi-valued data sets are increasingly common, with the number of dimensions growing. A number of multi-variate visualization techniques have been presented to display such data. However, evaluating the utility of such techniques for general data sets remains difficult. Thus most techniques are studied on only one data set. Another criticism that could be levied against previous evaluations of multi-variate visualizations is that the task doesn't require the presence of multiple variables. At the same time, the taxonomy of tasks that users may perform visually is extensive. We designed a task, trend localization, that required comparison of multiple data values in a multi-variate visualization. We then conducted a user study with this task, evaluating five multivariate visualization techniques from the literature (Brush Strokes, Data-Driven Spots, Oriented Slivers, Color Blending, Dimensional Stacking) and juxtaposed grayscale maps. We report the results and discuss the implications for both the techniques and the task.},
author = {Livingston, M A and Decker, J W},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Livingston, Decker - 2011 - Evaluation of Trend Localization with Multi-Variate Visualizations.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2053--2062},
publisher = {IEEE},
title = {{Evaluation of Trend Localization with Multi-Variate Visualizations}},
volume = {17},
year = {2011}
}
@article{livingston2012evaluation,
abstract = {Multivariate visualization techniques have attracted great interest as the dimensionality of data sets grows. One premise of such techniques is that simultaneous visual representation of multiple variables will enable the data analyst to detect patterns amongst multiple variables. Such insights could lead to development of new techniques for rigorous (numerical) analysis of complex relationships hidden within the data. Two natural questions arise from this premise: Which multivariate visualization techniques are the most effective for high-dimensional data sets? How does the analysis task change this utility ranking? We present a user study with a new task to answer the first question. We provide some insights to the second question based on the results of our study and results available in the literature. Our task led to significant differences in error, response time, and subjective workload ratings amongst four visualization techniques. We implemented three integrated techniques (Data-driven Spots, Oriented Slivers, and Attribute Blocks), as well as a baseline case of separate grayscale images. The baseline case fared poorly on all three measures, whereas Datadriven Spots yielded the best accuracy and was among the best in response time. These results differ from comparisons of similar techniques with other tasks, and we review all the techniques, tasks, and results (from our work and previous work) to understand the reasons for this discrepancy.},
author = {Livingston, M A and Decker, J W and Ai, Z},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Livingston, Decker, Ai - 2012 - Evaluation of Multivariate Visualization on a Multivariate Task.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2114--2121},
publisher = {IEEE},
title = {{Evaluation of Multivariate Visualization on a Multivariate Task}},
volume = {18},
year = {2012}
}
@article{Lowe2004,
author = {Lowe, David G},
file = {:Users/smcgregor/Documents/Mendeley Desktop/ijcv04.pdf:pdf},
journal = {International Journal of Computer Vision},
number = {2},
pages = {91--110},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
volume = {60},
year = {2004}
}
@article{Lowe1995,
abstract = {Nearest-neighbor interpolation algorithms have many useful properties for applications to learning, but they often exhibit poor generalization. In this paper, it is shown that much better generalization can be obtained by using a variable interpolation kernel in combination with conjugate gradient optimization of the similarity metric and kernel size. The resulting method is called variable-kernel similarity metric (VSM) learning. It has been tested on several standard classification data sets, and on these problems it shows better generalization than backpropagation and most other learning methods. The number of parameters that must be determined through optimization are orders of magnitude less than for backpropagation or radial basis function (RBF) networks, which may indicate that the method better captures the essential degrees of variation in learning. Other features of VSM learning are discussed that make it relevant to models for biological learning in the brain.},
annote = {Takes a weighted vote of points nearby. The weights are scaled differently depending on the proximity of points by changing the variance of a Gaussian Kernel.

They use conjugate gradient ascent on the weights by minimizing a loss function that is the square of the distance between true and predicted probability labels on the actual class of the labeled point. This is a fully supervised method of learning.

We could potentially use this method for trajectory synthesis by expressing the cost as the normalized distance across resultant states of a trajectory.},
author = {Lowe, David G.},
doi = {10.1162/neco.1995.7.1.72},
file = {:Users/smcgregor/Documents/Mendeley Desktop/TR-93-43.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
number = {1},
pages = {72--85},
title = {{Similarity Metric Learning for a Variable-Kernel Classifier}},
volume = {7},
year = {1995}
}
@inproceedings{Lucas2008,
author = {Lucas, Matthew M and Borisov, Nikita},
booktitle = {Proceedings of the 7th ACM workshop on Privacy in the electronic society},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Lucas, Borisov - 2008 - flyByNight Mitigating the Privacy Risks of Social Networking.pdf:pdf},
isbn = {9781605582894},
pages = {1--8},
title = {{flyByNight : Mitigating the Privacy Risks of Social Networking}},
year = {2008}
}
@inproceedings{luo2009facecloak,
author = {Luo, W and Xie, Q and Hengartner, U},
booktitle = {Computational Science and Engineering, 2009. CSE'09. International Conference on},
file = {:Users/smcgregor/Documents/Mendeley Desktop/05283227.pdf:pdf},
organization = {IEEE},
pages = {26--33},
title = {{Facecloak: An architecture for user privacy on social networking sites}},
volume = {3},
year = {2009}
}
@article{Maaten2008,
abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
annote = {When the crowding problem doesn't apply, it is more appropriate to use ISOMAP or LLE.},
author = {Maaten, Laurens Van Der and Hinton, Geoffrey},
doi = {10.1007/s10479-011-0841-3},
editor = {Lawrence, N},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Maaten, Hinton - 2008 - Visualizing Data using t-SNE.pdf:pdf},
issn = {02545330},
journal = {Journal of Machine Learning Research},
keywords = {Machine Learning,Specific Visualization,dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,n variable,visualization},
mendeley-tags = {Machine Learning,Specific Visualization,n variable},
number = {2579-2605},
pages = {2579--2605},
publisher = {Microtome Publishing},
title = {{Visualizing Data using t-SNE}},
url = {http://www.cs.toronto.edu/$\sim$hinton/absps/tsnefinal.pdf},
volume = {9},
year = {2008}
}
@article{MacKinnon2009,
abstract = {This study explores an under-studied layer of Chinese Internet censorship: how Chinese Internet companies censor user–generated content, usually by deleting it or preventing its publication. Systematic testing of Chinese blog service providers reveals that domestic censorship is very decentralized with wide variation from company to company. Test results also showed that a great deal of politically sensitive material survives in the Chinese blogosphere, and that chances for its survival can likely be improved with knowledge and strategy. The study concludes that choices and actions by private individuals and companies can have a significant impact on the overall balance of freedom and control in the Chinese blogosphere.},
author = {MacKinnon, Rebecca},
journal = {First Monday},
number = {2},
pages = {[online]},
title = {{China's Censorship 2.0: How companies censor bloggers}},
url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/2378/2089},
volume = {14},
year = {2009}
}
@misc{MatasanoSecurity2013,
author = {{Matasano Security}},
howpublished = {http://www.matasano.com/articles/javascript-cryptography/},
title = {{Javascript Cryptography Considered Harmful}},
url = {http://www.matasano.com/articles/javascript-cryptography/},
urldate = {2013-12-11},
year = {2013}
}
@article{Matkovic2008,
abstract = {Interactive steering with visualization has been a common goal of the visualization research community for twenty years, but it is rarely ever realized in practice. In this paper we describe a successful realization of a tightly coupled steering loop, integrating new simulation technology and interactive visual analysis in a prototyping environment for automotive industry system design. Due to increasing pressure on car manufacturers to meet new emission regulations, to improve efficiency, and to reduce noise, both simulation and visualization are pushed to their limits. Automotive system components, such as the powertrain system or the injection system have an increasing number of parameters, and new design approaches are required. It is no longer possible to optimize such a system solely based on experience or forward optimization. By coupling interactive visualization with the simulation back-end (computational steering), it is now possible to quickly prototype a new system, starting from a non-optimized initial prototype and the corresponding simulation model. The prototyping continues through the refinement of the simulation model, of the simulation parameters and through trial-and-error attempts to an optimized solution. The ability to early see the first results from a multidimensional simulation space--thousands of simulations are run for a multidimensional variety of input parameters--and to quickly go back into the simulation and request more runs in particular parameter regions of interest significantly improves the prototyping process and provides a deeper understanding of the system behavior. The excellent results which we achieved for the common rail injection system strongly suggest that our approach has a great potential of being generalized to other, similar scenarios.},
annote = {Talks about visual steering. This could be important.},
author = {Matkovi{\'{c}}, Kresimir and Gracanin, Denis and Jelovi{\'{c}}, Mario and Hauser, Helwig},
doi = {10.1109/TVCG.2008.145},
file = {:Users/smcgregor/Documents/Mendeley Desktop/04658193.pdf:pdf},
issn = {1077-2626},
journal = {IEEE transactions on visualization and computer graphics},
number = {6},
pages = {1699--1706},
pmid = {18989028},
title = {{Interactive visual steering--rapid visual prototyping of a common rail injection system.}},
volume = {14},
year = {2008}
}
@inproceedings{may2011guiding,
abstract = {We propose a method for the semi-automated refinement of the results of feature subset selection algorithms. Feature subset selection is a preliminary step in data analysis which identifies the most useful subset of features (columns) in a data table. So-called filter techniques use statistical ranking measures for the correlation of features. Usually a measure is applied to all entities (rows) of a data table. However, the differing contributions of subsets of data entities are masked by statistical aggregation. Feature and entity subset selection are, thus, highly interdependent. Due to the difficulty in visualizing a high-dimensional data table, most feature subset selection algorithms are applied as a black box at the outset of an analysis. Our visualization technique, SmartStripes, allows users to step into the feature subset selection process. It enables the investigation of dependencies and interdependencies between different feature and entity subsets. A user may even choose to control the iterations manually, taking into account the ranking measures, the contributions of different entity subsets, as well as the semantics of the features.},
author = {May, T and Bannach, A and Davey, J and Ruppert, T and Kohlhammer, J},
booktitle = {Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on},
organization = {IEEE},
pages = {111--120},
title = {{Guiding feature subset selection with an interactive visualization}},
year = {2011}
}
@article{McGarigal2014,
author = {McGarigal, K},
file = {:Users/smcgregor/Documents/Mendeley Desktop/fragstats.help.4.2.pdf:pdf},
title = {{FRAGSTATS HELP}},
url = {http://www.umass.edu/landeco/research/fragstats/documents/fragstats.help.4.2.pdf},
year = {2014}
}
@misc{McGregor2015,
author = {McGregor, Sean},
howpublished = {https://goo.gl/qgC1pF},
title = {{MDP State Transition Caching Cost Model}},
url = {https://docs.google.com/spreadsheets/d/1bUUATq6drv-cnq3NHTaYpRUbfTBvYs_waEGDa0Vmzx4/edit?usp=sharing},
urldate = {2015-01-07},
year = {2015}
}
@inproceedings{McGregor2015a,
address = {Atlanta},
author = {McGregor, Sean and Buckingham, Hailey and Dietterich, Thomas G. and Houtman, Rachel and Montgomery, Claire and Metoyer, Ron},
booktitle = {IEEE Symposium on Visual Languages and Human-Centric Computing},
file = {:Users/smcgregor/Documents/Mendeley Desktop/VL-HCC (1).pdf:pdf},
title = {{Facilitating Testing and Debugging of Markov Decision Processes with Interactive Visualization}},
year = {2015}
}
@article{McGregor2015b,
author = {McGregor, Sean and Buckingham, Hailey and Dietterich, Thomas G. and Houtman, Rachel and Montgomery, Claire and Metoyer, Ron},
file = {:Users/smcgregor/Documents/Mendeley Desktop/GraduateConsortium.pdf:pdf},
journal = {IEEE Symposium on Visual Languages and Human-Centric Computing (Graduate Consortium)},
title = {{Facilitating Testing and Debugging of Markov Decision Processes with Interactive Visualization}},
year = {2015}
}
@inproceedings{McGregor2015c,
author = {McGregor, Sean and Buckingham, Hailey and Houtman, Rachel and Montgomery, Claire and Metoyer, Ronald and Dietterich, Thomas G},
booktitle = {AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents},
file = {:Users/smcgregor/Documents/Mendeley Desktop/SDMIA (1).pdf:pdf},
title = {{MDPvis: An Interactive Visualization for Testing Markov Decision Processes}},
year = {2015}
}
@inproceedings{McGregor2013,
address = {Portland, Oregon},
author = {McGregor, Sean and Davidson, Jennifer},
booktitle = {OSCON: Open Source Convention},
publisher = {O'Reilly Media, Incorporated},
title = {{The Open Privacy Stack: Privly}},
url = {http://www.oscon.com/oscon2013/public/schedule/detail/29222},
year = {2013}
}
@inproceedings{McGregor2014,
address = {Montreal, Canada},
author = {McGregor, Sean and Dietterich, Thomas G. and Metoyer, Ronald},
booktitle = {Neural Information Processing Systems, Workshop on Large-scale reinforcement learning and Markov decision problems},
title = {{Toward Visualization Methods for Interactive Improvement of MDP Specifications}},
year = {2014}
}
@inproceedings{McGregor2012,
address = {Portland, Oregon},
author = {McGregor, Sean and Karve, Sanchit and Davidson, Jennifer},
booktitle = {Open Source Bridge},
publisher = {Stumptown Syndicate},
title = {{How to Encrypt Your Content on Any Website: Privly}},
url = {http://opensourcebridge.org/sessions/788},
year = {2012}
}
@inproceedings{McKenna2014,
author = {McKenna, S. and Mazur, D. and Agutter, J. and Meyer, M.},
booktitle = {Visualization and Computer Graphics, IEEE Transactions on},
doi = {10.1109/TVCG.2014.2346331},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06876000.pdf:pdf},
isbn = {1077-2626},
issn = {10772626},
number = {12},
pages = {2191--2200},
title = {{Design activity framework for visualization design}},
volume = {20},
year = {2014}
}
@article{Mehta2008,
abstract = {Transfer learning seeks to leverage previously learned tasks to achieve faster learning in a new task. In this paper, we consider transfer learning in the context of related but distinct Reinforcement Learning (RL) problems. In particular, our RL problems are derived from Semi-Markov Decision Processes (SMDPs) that share the same transition dynamics but have different reward functions that are linear in a set of reward features. We formally define the transfer learning problem in the context of RL as learning an efficient algorithm to solve any SMDP drawn from a fixed distribution after experiencing a finite number of them. Furthermore, we introduce an online algorithm to solve this problem, Variable-Reward Reinforcement Learning (VRRL), that compactly stores the optimal value functions for several SMDPs, and uses them to optimally initialize the value function for a new SMDP. We generalize our method to a hierarchical RL setting where the different SMDPs share the same task hierarchy. Our experimental results in a simplified real-time strategy domain show that significant transfer learning occurs in both flat and hierarchical settings. Transfer is especially effective in the hierarchical setting where the overall value functions are decomposed into subtask value functions which are more widely amenable to transfer across different SMDPs.},
author = {Mehta, Neville and Natarajan, Sriraam and Tadepalli, Prasad and Fern, Alan},
doi = {10.1007/s10994-008-5061-y},
file = {:Users/smcgregor/Documents/Mendeley Desktop/var-reward.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Average-reward learning,Hierarchical reinforcement learning,Multi-criteria learning,Transfer learning},
pages = {289--312},
title = {{Transfer in variable-reward hierarchical reinforcement learning}},
volume = {73},
year = {2008}
}
@article{Meyer2013,
abstract = {We propose the nested blocks and guidelines model for the design and validation of visualization systems. The nested blocks and guidelines model extends the previously proposed four-level nested model by adding finer grained structure within each level, providing explicit mechanisms to capture and discuss design decision rationale. Blocks are the outcomes of the design process at a specific level, and guidelines discuss relationships between these blocks. Blocks at the algorithm and technique levels describe design choices, as do data blocks at the abstraction level, whereas task abstraction blocks and domain situation blocks are identified as the outcome of the designer’s understanding of the requirements. In the nested blocks and guidelines model, there are two types of guidelines: within-level guidelines provide comparisons for blocks within the same level, while between-level guidelines provide mappings between adjacent levels of design. We analyze several recent articles using the nested blocks and guidelines model to provide concrete examples of how a researcher can use blocks and guidelines to describe and evaluate visualization research. We also discuss the nested blocks and guidelines model with respect to other design models to clarify its role in visualization design. Using the nested blocks and guidelines model, we pinpoint two implications for visualization evaluation. First, comparison of blocks at the domain level must occur implicitly downstream at the abstraction level; second, comparison between blocks must take into account both upstream assumptions and downstream requirements. Finally, we use the model to analyze two open problems: the need for mid-level task taxonomies to fill in the task blocks at the abstraction level and the need for more guidelines mapping between the algorithm and technique levels.},
author = {Meyer, M. and Sedlmair, M. and Quinan, P. S. and Munzner, T.},
doi = {10.1177/1473871613510429},
file = {:Users/smcgregor/Documents/Mendeley Desktop/nbgm.pdf:pdf},
issn = {1473-8716},
journal = {Information Visualization},
title = {{The nested blocks and guidelines model}},
url = {http://ivi.sagepub.com/lookup/doi/10.1177/1473871613510429},
year = {2013}
}
@article{Meyer2009,
abstract = {In the field of comparative genomics, scientists seek to answer questions about evolution and genomic function by comparing the genomes of species to find regions of shared sequences. Conserved syntenic blocks are an important biological data abstraction for indicating regions of shared sequences. The goal of this work is to show multiple types of relationships at multiple scales in a way that is visually comprehensible in accordance with known perceptual principles. We present a task analysis for this domain where the fundamental questions asked by biologists can be understood by a characterization of relationships into the four types of proximity/location, size, orientation, and similarity/strength, and the four scales of genome, chromosome, block, and genomic feature. We also propose a new taxonomy of the design space for visually encoding conservation data. We present MizBee, a multiscale synteny browser with the unique property of providing interactive side-by-side views of the data across the range of scales supporting exploration of all of these relationship types. We conclude with case studies from two biologists who used MizBee to augment their previous automatic analysis work flow, providing anecdotal evidence about the efficacy oft he system for the visualization of syntenic data, the analysis of conservation relationships, and the communication of scientific insights.},
author = {Meyer, Miriah and Munzner, Tamara and Pfister, Hanspeter},
doi = {10.1109/TVCG.2009.167},
file = {:Users/smcgregor/Documents/Mendeley Desktop/mizbee.pdf:pdf},
isbn = {1077-2626 VO - 15},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Information visualization,bioinformatics,design study,synteny},
number = {October},
pages = {897--904},
pmid = {19834152},
title = {{MizBee: A multiscale synteny browsers}},
volume = {15},
year = {2009}
}
@article{Meyer2012,
abstract = {We propose an extension to the four-level nested model for design and validation of visualization systems that defines the term “guide- lines” in terms of blocks at each level. Blocks are the outcomes of the design process at a specific level, and guidelines discuss re- lationships between these blocks. Within-level guidelines provide comparisons for blocks within the same level, while between-level guidelines provide mappings between adjacent levels of design. These guidelines help a designer choose which abstractions, tech- niques, and algorithms are reasonable to combine when building a visualization system. This definition of guideline allows analysis of how the validation efforts in different kinds of papers typically lead to different kinds of guidelines. Analysis through the lens of blocks and guidelines also led us to identify four major needs: a definition of the meaning of block at the problem level; mid-level task taxonomies to fill in the blocks at the abstraction level; re- finement of the model itself at the abstraction level; and a more complete set of guidelines that map up from the algorithm level to the technique level. These gaps in visualization knowledge present rich opportunities for future work.},
author = {Meyer, Miriah and Sedlmair, Michael and Munzner, Tamara},
doi = {10.1145/2442576.2442587},
file = {:Users/smcgregor/Documents/Mendeley Desktop/meyer2012beliv.pdf:pdf},
isbn = {9781450317917},
journal = {Ieee Beliv},
keywords = {design studies,nested model,validation,visualization},
pages = {1--6},
title = {{The Four-Level Nested Model Revisited : Blocks and Guidelines}},
year = {2012}
}
@inproceedings{Migut2010,
author = {Migut, Malgorzata and Worring, Marcel},
booktitle = {Visual Analytics Science and Technology (VAST), 2010 IEEE Symposium on},
file = {:Users/smcgregor/Documents/Mendeley Desktop/05652398.pdf:pdf},
isbn = {9781424494873},
keywords = {cision boundary visualization,classifi-,de-,interactive visual exploration,multi-dimensional space,visual analytics},
pages = {11--18},
title = {{Visual Exploration of Classification Models for Risk Assessment}},
year = {2010}
}
@misc{Miller2014,
author = {Miller, Joe},
booktitle = {BBC News},
title = {{Google's driverless cars designed to exceed speed limit}},
url = {http://www.bbc.com/news/technology-28851996},
urldate = {2014-02-10},
year = {2014}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:Users/smcgregor/Documents/Mendeley Desktop/nature14236 (1).pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@misc{Mordvintsev2015,
author = {Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike},
booktitle = {Google Research Blog},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Research Blog- Inception...er into Neural Networks.pdf:pdf},
title = {{Inceptionism: Going Deeper into Neural Networks}},
url = {http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-into-neural.html},
urldate = {2015-01-01},
year = {2015}
}
@misc{Mozilla2013a,
author = {Mozilla},
howpublished = {http://www.mozilla.org/projects/security/pki/nss/},
title = {{Network Security Services (NSS)}},
url = {http://www.mozilla.org/projects/security/pki/nss/},
year = {2013}
}
@misc{Mozilla2013,
abstract = {2,402,743 users},
author = {Mozilla},
howpublished = {https://addons.mozilla.org/en-US/firefox/addon/greasemonkey/},
title = {{Greasemonkey}},
url = {https://addons.mozilla.org/en-US/firefox/addon/greasemonkey/},
year = {2013}
}
@misc{Mozilla.org2013,
author = {Mozilla.org},
howpublished = {http://www.mozilla.org/en-US/persona/},
title = {{Introducing Mozilla Persona}},
url = {http://www.mozilla.org/en-US/persona/},
urldate = {2013-12-11},
year = {2013}
}
@article{Mulder1999,
abstract = {Computational steering is a powerful concept that allows scientists to interactively control a computational process during its execution. In this paper, a survey of computational steering environments for the on-line steering of ongoing scientific and engineering simulations is presented. These environments can be used to create steerable applications for model exploration, algorithm experimentation, or performance optimization. For each environment the scope is identified, the architecture is summarized, and the concepts of the user interface are described. The environments are compared and conclusions and future research issues are given.},
author = {Mulder, Jurriaan D. and van Wijk, Jarke J. and van Liere, Robert},
doi = {10.1016/S0167-739X(98)00047-8},
file = {:Users/smcgregor/Documents/Mendeley Desktop/10.1.1.79.6373.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {computational steering},
number = {1},
pages = {119--129},
title = {{A survey of computational steering environments}},
volume = {15},
year = {1999}
}
@article{Muller2003,
author = {M{\"{u}}ller, Wolfgang and Schumann, Heidrun},
file = {:Users/smcgregor/Documents/Mendeley Desktop/M{\"{u}}ller, Schumann - 2003 - Visualization for modeling and simulation visualization methods for time-dependent data - an overview.pdf:pdf},
isbn = {0-7803-8132-7},
month = {dec},
pages = {737--745},
title = {{Visualization for modeling and simulation: visualization methods for time-dependent data - an overview}},
url = {http://dl.acm.org/citation.cfm?id=1030818.1030916},
year = {2003}
}
@inproceedings{mm-ivmcatdoc-1999,
author = {Munos, R and Moore, A},
booktitle = {Proceedings of the 38th IEEE Conference on Decision and Control},
pages = {6},
title = {{Influence and Variance of a Markov Chain: Application to Adaptive Discretization in Optimal Control}},
year = {1999}
}
@article{Munzner,
author = {Munzner, Tamara},
file = {:Users/smcgregor/Documents/Mendeley Desktop/NestedModel.pdf:pdf},
title = {{A Nested Model for Visualization Design and Validation}}
}
@article{Narayanan2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1202.4503v1},
author = {Narayanan, Arvind and Toubiana, Vincent},
eprint = {arXiv:1202.4503v1},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1202.4503v1.pdf:pdf},
journal = {arXiv preprint},
title = {{A Critical Look at Decentralized Personal Data Architectures}},
url = {http://arxiv.org/abs/1202.4503},
year = {2012}
}
@misc{Money2014,
author = {{National Public Radio}},
booktitle = {Planet Money},
title = {{Episode 509: Will A Computer Decide Whether You Get Your Next Job?}},
url = {http://www.npr.org/blogs/money/2014/01/15/262789258/episode-509-will-a-computer-decide-whether-you-get-your-next-job},
urldate = {2014-02-10},
year = {2014}
}
@phdthesis{Ng2003,
author = {Ng, Andrew Y.},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Ng - 2003 - Shaping and policy search in reinforcement learning.pdf:pdf},
pages = {167},
school = {University of California, Berkeley},
title = {{Shaping and policy search in reinforcement learning}},
type = {Doctor of Philosophy},
url = {http://www.cs.ubc.ca/$\sim$nando/550-2006/handouts/andrew-ng.pdf},
year = {2003}
}
@article{Ng2006,
author = {Ng, Andrew Y. and Coates, Adam and Diel, Mark and Ganapathi, Varun},
file = {:Users/smcgregor/Documents/Mendeley Desktop/iser04-invertedflight.pdf:pdf},
journal = {Experimental Robotics IX},
pages = {1--10},
title = {{Autonomous inverted helicopter flight via reinforcement learning}},
url = {http://link.springer.com/chapter/10.1007/11552246_35},
year = {2006}
}
@article{Ng2000,
author = {Ng, Andrew Y. and Jordan, M},
file = {:Users/smcgregor/Documents/Mendeley Desktop/ng-jordan-pegasus-a-policy-search-method-for-large-mdps-and-pomdps-uai2000.pdf:pdf},
journal = {Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence},
title = {{PEGASUS : A policy search method for large MDPs and POMDPs}},
url = {http://dl.acm.org/citation.cfm?id=2073994},
year = {2000}
}
@article{Nilim,
author = {Nilim, Arnab and Ghaoui, Laurent El},
keywords = {modeling},
mendeley-tags = {modeling},
title = {{Robustness in Markov Decision Problems with Uncertain Transition Matrices∗}}
}
@article{Nilizadeh2012,
author = {Nilizadeh, Shirin and Jahid, Sonia and Mittal, P},
file = {:Users/smcgregor/Documents/Mendeley Desktop/cachet-conext12.pdf:pdf},
isbn = {9781450317757},
journal = {Proceedings of the 8th international conference on Emerging networking experiments and technologies},
keywords = {caching,peer-to-peer systems,privacy,social networking},
title = {{Cachet: a Decentralized Architecture for Privacy Preserving Social Networking with Caching}},
url = {http://dl.acm.org/citation.cfm?id=2413215},
year = {2012}
}
@article{oeltze2011interactive,
abstract = {In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.},
author = {Oeltze, S and Freiler, W and Hillert, R and Doleisch, H and Preim, B and Schubert, W},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06064951.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {1882--1891},
publisher = {IEEE},
title = {{Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics}},
volume = {17},
year = {2011}
}
@article{Osband2013,
author = {Osband, I and Russo, D and Roy, B Van},
file = {:Users/smcgregor/Documents/Mendeley Desktop/5185-more-efficient-reinforcement-learning-via-posterior-sampling.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 26 (NIPS 2013)},
pages = {1--10},
title = {{(More) Efficient Reinforcement Learning via Posterior Sampling}},
url = {http://papers.nips.cc/paper/5185-more-efficient-reinforcement-learning-via-posterior-sampling},
year = {2013}
}
@article{Osband2014,
abstract = {We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as $\tilde{O}(\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is the Kolmogorov dimension and $d_E$ is the \emph{eluder dimension}. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm \emph{posterior sampling for reinforcement learning} (PSRL) that satisfies these bounds.},
archivePrefix = {arXiv},
arxivId = {1406.1853},
author = {Osband, Ian and {Van Roy}, Benjamin},
eprint = {1406.1853},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1406.1853v2.pdf:pdf},
month = {jun},
pages = {1--14},
title = {{Model-based Reinforcement Learning and the Eluder Dimension}},
url = {http://arxiv.org/abs/1406.1853},
year = {2014}
}
@phdthesis{Paduraru2013,
annote = {* Contains theoretical analysis of difference in RMSE for on-policy and off-policy as the policy moves away from uniform in bandits
* I can probably use the analysis of bandits because each state in the fire problem is essentially a bandit with two actions.},
author = {Paduraru, Cosmin},
file = {:Users/smcgregor/Documents/Mendeley Desktop/thesis.pdf:pdf},
school = {McGill University},
title = {{Off-policy evaluation in Markov Decision Processes}},
year = {2013}
}
@phdthesis{Paduraru2013,
annote = {* Contains theoretical analysis of difference in RMSE for on-policy and off-policy as the policy moves away from uniform in bandits
* I can probably use the analysis of bandits because each state in the fire problem is essentially a bandit with two actions.},
author = {Paduraru, Cosmin},
file = {:Users/smcgregor/Documents/Mendeley Desktop/thesis.pdf:pdf},
title = {{Off-policy evaluation in Markov Decision Processes}},
year = {2013},
number = {February},
school = {McGill University}
}
@article{Parker1996,
abstract = {With today's large and complex applications, scientists have increasing difficulty analyzing and visualizing vast amounts of data. Computational steering is an emerging technology that addresses this problem, providing a mechanism for integrating simulation, data analysis, visualization, and postprocessing},
author = {Parker, Steven G and Johnson, Christopher R and Beazley, David},
doi = {10.1109/99.641609},
file = {:Users/smcgregor/Documents/Mendeley Desktop/10.1.1.40.702.pdf:pdf},
issn = {10709924},
journal = {IEEE computational science & engineering},
number = {4},
pages = {50--59},
title = {{Computational steering software systems and strategies}},
volume = {4},
year = {1997}
}
@article{Paulovich2012,
author = {Paulovich, By Fernando V and Silva, Cl{\'{a}}udio T and Nonato, Luis G},
file = {:Users/smcgregor/Downloads/06241366.pdf:pdf},
pages = {74--81},
title = {{U -C M P t}},
year = {2012}
}
@article{Pearson2009,
author = {Pearson, Siani},
file = {:Users/smcgregor/Documents/Mendeley Desktop/HP Lab.pdf:pdf},
journal = {{\ldots} Challenges of Cloud Computing, 2009. CLOUD' {\ldots}},
keywords = {cloud computing,design,privacy,s},
title = {{Taking Account of Privacy when Designing Cloud Computing Services}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5071532},
year = {2009}
}
@misc{Peng2004,
abstract = {Visual clutter denotes a disordered collection of graphical entities in information visualization. Clutter can obscure the structure present in the data. Even in a small dataset, clutter can make it hard for the viewer to find patterns, relationships and structure. In this paper, we define visual clutter as any aspect of the visualization that interferes with the viewer's understanding of the data, and present the concept of clutter-based dimension reordering. Dimension order is an attribute that can significantly affect a visualization's expressiveness. By varying the dimension order in a display, it is possible to reduce clutter without reducing information content or modifying the data in any way. Clutter reduction is a display-dependent task. In this paper, we follow a three-step procedure for four different visualization techniques. For each display technique, first, we determine what constitutes clutter in terms of display properties; then we design a metric to measure visual clutter in this display; finally we search for an order that minimizes the clutter in a display},
author = {Peng, W and Ward, M O and Rundensteiner, E A},
booktitle = {IEEE Symposium on Information Visualization},
doi = {10.1109/INFVIS.2004.15},
isbn = {0780387793},
issn = {1522404X},
keywords = {dimension order,multidimensional visualization,visual clutter,visual structure},
pages = {89--96},
publisher = {Ieee},
title = {{Clutter Reduction in Multi-Dimensional Data Visualization Using Dimension Reordering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1382895},
year = {2004}
}
@article{Perez-Cruz2008,
abstract = {We present a method for estimating the KL divergence between continuous densities and we prove it converges almost surely. Divergence estimation is typically solved estimating the densities first. Our main result shows this intermediate step is unnecessary and that the divergence can be either estimated using the empirical cdf or k-nearest-neighbour density estimation, which does not converge to the true measure for finite k. The convergence proof is based on describing the statistics of our estimator using waiting-times distributions, as the exponential or Erlang. We illustrate the proposed estimators and show how they compare to existing methods based on density estimation, and we also outline how our divergence estimators can be used for solving the two-sample problem.},
author = {P{\'{e}}rez-Cruz, Fernando},
doi = {10.1109/ISIT.2008.4595271},
file = {:Users/smcgregor/Documents/Mendeley Desktop/bare_conf3.pdf:pdf},
isbn = {9781424422579},
issn = {21578101},
journal = {IEEE International Symposium on Information Theory - Proceedings},
pages = {1666--1670},
title = {{Kullback-leibler divergence estimation of continuous distributions}},
year = {2008}
}
@article{pham2010visualization,
abstract = {Understanding the diversity of a set of multivariate objects is an important problem in many domains, including ecology, college admissions, investing, machine learning, and others. However, to date, very little work has been done to help users achieve this kind of understanding. Visual representation is especially appealing for this task because it offers the potential to allow users to efficiently observe the objects of interest in a direct and holistic way. Thus, in this paper, we attempt to formalize the problem of visualizing the diversity of a large (more than 1000 objects), multivariate (more than 5 attributes) data set as one worth deeper investigation by the information visualization community. In doing so, we contribute a precise definition of diversity, a set of requirements for diversity visualizations based on this definition, and a formal user study design intended to evaluate the capacity of a visual representation for communicating diversity information. Our primary contribution, however, is a visual representation, called the Diversity Map, for visualizing diversity. An evaluation of the Diversity Map using our study design shows that users can judge elements of diversity consistently and as or more accurately than when using the only other representation specifically designed to visualize diversity.},
author = {Pham, T and Hess, R and Ju, C and Zhang, E and Metoyer, R},
file = {:Users/smcgregor/Documents/Mendeley Desktop/05613443.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1053--1062},
publisher = {IEEE},
title = {{Visualization of diversity in large multivariate data sets}},
volume = {16},
year = {2010}
}
@book{pilgrim2005greasemonkey,
author = {Pilgrim, Mark},
publisher = {O'Reilly Media, Incorporated},
title = {{Greasemonkey Hacks: Tips & Tools for Remixing the Web with Firefox}},
year = {2005}
}
@article{Pinheiro1988,
abstract = {The estimation of variance-covariancematrices in situations that involve the optimization of an ob- jective function (e.g. a log-likelihood function) is usually a difficult numerical problem, since the resulting estimates should be positive semi-definite matrices. We can either use constrained opti- mization, or employ a parameterization that enforces this condition. We describe here five different parameterizations for variance-covariance matrices that ensure positive definiteness, while leaving the estimation problem unconstrained. We compare the parameterizations based on their computa- tional efficiency and statistical interpretability. The results described here are particularly useful in maximumlikelihood and restrictedmaximumlikelihood estimation inmixed effectsmodels, but are also applicable to other areas of statistics.},
author = {Pinheiro, Jos{\'{e}} and Bates, Douglas},
file = {:Users/smcgregor/Documents/Mendeley Desktop/pinheiro-bates-unconstrained-parameterizations-for-variance-covariance-matrices.pdf:pdf},
journal = {Statistics and Computing},
keywords = {cholesky,factorization,matrix logarithm,unconstrained estimation,variance-covariance components estimation},
pages = {289--296},
title = {{Unconstrained Parameterizations for Variance-Covariance Matrices}},
volume = {6},
year = {1988}
}
@inproceedings{Politz,
author = {Politz, Joe Gibbs},
booktitle = {USENIX Conference on Security Symposium},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Politz.pdf:pdf},
title = {{ADsafety: Type-Based Verification of JavaScript Sandboxing}},
year = {2011}
}
@article{Potter2009,
abstract = {Scientists increasingly use ensemble data sets to explore relationships present in dynamic systems. Ensemble data sets combine spatio-temporal simulation results generated using multiple numerical models, sampled input conditions and perturbed parameters. While ensemble data sets are a powerful tool for mitigating uncertainty, they pose significant visualization and analysis challenges due to their complexity. In this article, we present Ensemble-Vis, a framework consisting of a collection of overview and statistical displays linked through a high level of interactivity. Ensemble-Vis allows scientists to gain key scientific insight into the distribution of simulation results as well as the uncertainty associated with the scientific data. In contrast to methods that present large amounts of diverse information in a single display, we argue that combining multiple linked displays yields a clearer presentation of the data and facilitates a greater level of visual data analysis. We demonstrate our framework using driving problems from climate modeling and meteorology and discuss generalizations to other fields.},
author = {Potter, Kristin and Wilson, Andrew and Bremer, Peer Timo and Williams, Dean and Doutriaux, Charles and Pascucci, Valerio and Johnson, Chris R.},
doi = {10.1109/ICDMW.2009.55},
file = {:Users/smcgregor/Documents/Mendeley Desktop/09e41506066462c0ce000000.pdf:pdf},
isbn = {9780769539027},
journal = {ICDM Workshops 2009 - IEEE International Conference on Data Mining},
keywords = {Coordinated and linked views,Ensemble data,Statistical graphics,Uncertainty},
pages = {233--240},
title = {{Ensemble-vis: A framework for the statistical visualization of ensemble data}},
year = {2009}
}
@book{Puterman1994,
author = {Puterman, Martin},
edition = {1st},
publisher = {Wiley-Interscience},
title = {{Markov Decision Processes: Discrete Stochastic Dynamic Programming}},
year = {1994}
}
@article{Ramanan2011,
abstract = {We present a taxonomy for local distance functions where most existing algorithms can be regarded as approximations of the geodesic distance defined by a metric tensor. We categorize existing algorithms by how, where, and when they estimate the metric tensor. We also extend the taxonomy along each axis. How: We introduce hybrid algorithms that use a combination of techniques to ameliorate overfitting. Where: We present an exact polynomial-time algorithm to integrate the metric tensor along the lines between the test and training points under the assumption that the metric tensor is piecewise constant. When: We propose an interpolation algorithm where the metric tensor is sampled at a number of references points during the offline phase. The reference points are then interpolated during the online classification phase. We also present a comprehensive evaluation on tasks in face recognition, object recognition, and digit recognition.},
author = {Ramanan, Deva and Baker, Simon},
doi = {10.1109/TPAMI.2010.127},
file = {:Users/smcgregor/Documents/Mendeley Desktop/localdist_journal.pdf:pdf},
isbn = {978-1-4244-4420-5},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Nearest neighbor classification,database,evaluation.,local distance functions,metric learning,metric tensor,taxonomy},
number = {4},
pages = {794--806},
pmid = {20603519},
title = {{Local distance functions: A taxonomy, new algorithms, and an evaluation}},
volume = {33},
year = {2011}
}
@book{rs-fda-2005,
author = {Ramsay, J O and Silverman, B W},
publisher = {Springer},
title = {{Functional Data Analysis, Second Edition}},
year = {2005}
}
@inproceedings{Randlov1998,
author = {Randlov, J and Alstrom, P},
booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
file = {:Users/smcgregor/Documents/Mendeley Desktop/learning_to_drive_a_bicycle_using_reinforcement_learning_and_shaping.pdf:pdf},
title = {{Learning to Drive a Bicycle using Reinforcement Learning and Shaping}},
url = {http://pdf.aminer.org/000/335/589/learning_to_drive_a_bicycle_using_reinforcement_learning_and_shaping.pdf},
year = {1998}
}
@article{Reimann2012,
author = {Reimann, Sirke and D{\"{u}}rmuth, Markus},
isbn = {9781450316637},
pages = {65--73},
title = {{Timed Revocation of User Data : Long Expiration Times from Existing Infrastructure}},
year = {2012}
}
@article{reininghaus2011scale,
abstract = {This paper introduces a novel importance measure for critical points in 2D scalar fields. This measure is based on a combination of the deep structure of the scale space with the well-known concept of homological persistence. We enhance the noise robust persistence measure by implicitly taking the hill-, ridge- and outlier-like spatial extent of maxima and minima into account. This allows for the distinction between different types of extrema based on their persistence at multiple scales. Our importance measure can be computed efficiently in an out-of-core setting. To demonstrate the practical relevance of our method we apply it to a synthetic and a real-world data set and evaluate its performance and scalability.},
author = {Reininghaus, J and Kotava, N and Gunther, D and Kasten, J and Hagen, H and Hotz, I},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Reininghaus et al. - 2011 - A scale space based persistence measure for critical points in 2d scalar fields.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2045--2052},
publisher = {IEEE},
title = {{A scale space based persistence measure for critical points in 2d scalar fields}},
volume = {17},
year = {2011}
}
@inproceedings{rheingans2000visualizing,
abstract = {Using inductive learning techniques to construct classification models from large, high-dimensional data sets is a useful way to make predictions in complex domains. However, these models can be difficult for users to understand. We have developed a set of visualization methods that help users to understand and analyze the behavior of learned models, including techniques for high-dimensional data space projection, display of probabilistic predictions, variable/class correlation, and instance mapping. We show the results of applying these techniques to models constructed from a benchmark data set of census data, and draw conclusions about the utility of these methods for model understanding.},
author = {Rheingans, P and Desjardins, M},
booktitle = {Visualization 2000. Proceedings},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Rheingans, Desjardins - 2000 - Visualizing high-dimensional predictive model quality.pdf:pdf},
organization = {IEEE},
pages = {493--496},
title = {{Visualizing high-dimensional predictive model quality}},
year = {2000}
}
@article{Ribicic2013,
author = {Ribicic, Hrvoje and Waser, Jurgen and Fuchs, Raphael and Bloschl, Gunter and Groller, Eduard},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06280550.pdf:pdf},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {6},
pages = {1062--1075},
title = {{Visual Analysis and Steering of Flooding Simulations}},
volume = {19},
year = {2013}
}
@article{rieck2012multivariate,
abstract = {The extraction of significant structures in arbitrary high-dimensional data sets is a challenging task. Moreover, classifying data points as noise in order to reduce a data set bears special relevance for many application domains. Standard methods such as clustering serve to reduce problem complexity by providing the user with classes of similar entities. However, they usually do not highlight relations between different entities and require a stopping criterion, e.g. the number of clusters to be detected. In this paper, we present a visualization pipeline based on recent advancements in algebraic topology. More precisely, we employ methods from persistent homology that enable topological data analysis on high-dimensional data sets. Our pipeline inherently copes with noisy data and data sets of arbitrary dimensions. It extracts central structures of a data set in a hierarchical manner by using a persistence-based filtering algorithm that is theoretically well-founded. We furthermore introduce persistence rings, a novel visualization technique for a class of topological features-the persistence intervals-of large data sets. Persistence rings provide a unique topological signature of a data set, which helps in recognizing similarities. In addition, we provide interactive visualization techniques that assist the user in evaluating the parameter space of our method in order to extract relevant structures. We describe and evaluate our analysis pipeline by means of two very distinct classes of data sets: First, a class of synthetic data sets containing topological objects is employed to highlight the interaction capabilities of our method. Second, in order to affirm the utility of our technique, we analyse a class of high-dimensional real-world data sets arising from current research in cultural heritage.},
author = {Rieck, B and Mara, H and Leitte, H},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Rieck, Mara, Leitte - 2012 - Multivariate Data Analysis Using Persistence-Based Filtering and Topological Signatures.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2382--2391},
publisher = {IEEE},
title = {{Multivariate Data Analysis Using Persistence-Based Filtering and Topological Signatures}},
volume = {18},
year = {2012}
}
@article{Rieck2012,
abstract = {The extraction of significant structures in arbitrary high-dimensional data sets is a challenging task. Moreover, classifying data points as noise in order to reduce a data set bears special relevance for many application domains. Standard methods such as clustering serve to reduce problem complexity by providing the user with classes of similar entities. However, they usually do not highlight relations between different entities and require a stopping criterion, e.g. the number of clusters to be detected. In this paper, we present a visualization pipeline based on recent advancements in algebraic topology. More precisely, we employ methods from persistent homology that enable topological data analysis on high-dimensional data sets. Our pipeline inherently copes with noisy data and data sets of arbitrary dimensions. It extracts central structures of a data set in a hierarchical manner by using a persistence-based filtering algorithm that is theoretically well-founded. We furthermore introduce persistence rings, a novel visualization technique for a class of topological features-the persistence intervals-of large data sets. Persistence rings provide a unique topological signature of a data set, which helps in recognizing similarities. In addition, we provide interactive visualization techniques that assist the user in evaluating the parameter space of our method in order to extract relevant structures. We describe and evaluate our analysis pipeline by means of two very distinct classes of data sets: First, a class of synthetic data sets containing topological objects is employed to highlight the interaction capabilities of our method. Second, in order to affirm the utility of our technique, we analyse a class of high-dimensional real-world data sets arising from current research in cultural heritage.},
author = {Rieck, B and Mara, H and Leitte, H},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Rieck, Mara, Leitte - 2012 - Multivariate Data Analysis Using Persistence-Based Filtering and Topological Signatures.pdf:pdf},
journal = {Visualization and Computer {\ldots}},
title = {{Multivariate Data Analysis Using Persistence-Based Filtering and Topological Signatures}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6327243},
year = {2012}
}
@article{Rippel2016,
archivePrefix = {arXiv},
arxivId = {arXiv:submit/1408508},
author = {Rippel, Oren and Paluri, Manohar and Dollar, Piotr and Bourdev, Lubomir},
eprint = {1408508},
file = {:Users/smcgregor/Documents/Mendeley Desktop/rippel-paluri-dollar-bourdev-metric-learning-with-adapative-density-discrimination-iclr2006submision.pdf:pdf},
pages = {1--15},
primaryClass = {arXiv:submit},
title = {{Metric Learning with Adaptive Density Discrimination}},
year = {2016}
}
@article{robertson2008effectiveness,
abstract = {Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.},
author = {Robertson, G and Fernandez, R and Fisher, D and Lee, B and Stasko, J},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Robertson et al. - 2008 - Effectiveness of animation in trend visualization.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1325--1332},
publisher = {IEEE},
title = {{Effectiveness of animation in trend visualization}},
volume = {14},
year = {2008}
}
@article{Rosch1975,
abstract = {Six experiments explored the hypothesis that the members of categories which are considered most prototypical are those with most attributes in common with other members of the category and least attributes in common with other categories. In probabilistic terms, the hypothesis is that prototypicality is a function of the total cue validity of the attributes of items. In Experiments 1 and 3, subjects listed attributes for members of semantic categories which had been previously rated for degree of prototypicality. High positive correlations were obtained between those ratings and the extent of distribution of an item's attributes among the other items of the category. In Experiments 2 and 4, subjects listed superordinates of category members and listed attributes of members of contrasting categories. Negative correlations were obtained between prototypicality and superordinates other than the category in question and between prototypicality and an item's possession of attributes possessed by members of contrasting categories. Experiments 5 and 6 used artificial categories and showed that family resemblance within categories and lack of overlap of elements with contrasting categories were correlated with ease of learning, reaction time in identifying an item after learning, and rating of prototypicality of an item. It is argued that family resemblance offers an alternative to criterial features in defining categories.},
author = {Rosch, Eleanor and Mervis, Carolyn B},
doi = {10.1016/0010-0285(75)90024-9},
file = {:Users/smcgregor/Documents/Mendeley Desktop/rm75.pdf:pdf},
isbn = {0010-0285},
issn = {00100285},
journal = {Cognitive Psychology},
number = {4},
pages = {573--605},
pmid = {20401473},
title = {{Family resemblances: Studies in the internal structure of categories}},
volume = {7},
year = {1975}
}
@article{Rueda2006,
author = {Rueda, Luis and Zhang, Yuanquan},
doi = {10.1016/j.patcog.2006.02.006},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Cluster visualization,Expectation maximization,Fuzzy clustering,Fuzzy $\alpha$-means},
month = {aug},
number = {8},
pages = {1415--1429},
title = {{Geometric visualization of clusters obtained from fuzzy clustering algorithms}},
url = {http://dl.acm.org/citation.cfm?id=1220981.1221474},
volume = {39},
year = {2006}
}
@article{Russo2013,
annote = {This paper deals with the relationship between rewards on bandit arms, but the multiple simulator problem is concerned with the relationship between transition probabilities. It is possible that the eluder dimension could be applied to the multiple simulator problem if we think about the relationship between simulator value functions rather than reward or transition functions. This would seem to be similar to a transfer learning problem where we are solving one MDP using another MDP's value function.},
author = {Russo, Daniel and Roy, Benjamin Van},
file = {:Users/smcgregor/Documents/Mendeley Desktop/4909-eluder-dimension-and-the-sample-complexity-of-optimistic-exploration.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Eluder Dimension and the Sample Complexity of Optimistic Exploration}},
url = {http://papers.nips.cc/paper/4909-eluder-dimension-and-the-sample-complexity-of-optimistic-exploration},
year = {2013}
}
@article{Sanner,
author = {Sanner, Scott},
file = {:Users/smcgregor/Documents/Mendeley Desktop/RDDL.pdf:pdf},
title = {{Relational Dynamic Influence Diagram Language ( RDDL ): Language Description}}
}
@inproceedings{Ribic2013,
author = {Schindler, Benjamin and Ribicic, Hrvoje and Fuchs, Raphael and Peikert, Ronald},
booktitle = {IEEE Transactions on Visualization and Computer Graphics},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06329370.pdf:pdf},
number = {6},
pages = {1005--1019},
title = {{Multiverse data-flow control}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6329370},
volume = {19},
year = {2013}
}
@article{Schulman2006,
archivePrefix = {arXiv},
arxivId = {arXiv:submit/1186791},
author = {Schulman, John and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
eprint = {1186791},
file = {:Users/smcgregor/Documents/Mendeley Desktop/schulman-levine-moritz-jordan-abbeel-trust-region-policy-optimization-arxiv2015.pdf:pdf},
primaryClass = {arXiv:submit},
title = {{Trust Region Policy Optimization}},
year = {2006}
}
@article{Schultz2004,
abstract = {This paper presents a method for learning a distance metric from relative\ncomparison such as �A is closer to B than A is to C�. Taking a Support\nVector Machine (SVM) approach, we develop an algorithm that provides\na flexible way of describing qualitative training data as a set of\nconstraints. We show that such constraints lead to a convex quadratic\nprogramming problem that can be solved by adapting standard methods\nfor SVM training. We empirically evaluate the performance and the\nmodelling flexibility of the algorithm on a collection of text documents.},
author = {Schultz, Matthew and Joachims, Thorsten},
doi = {10.1016/j.ajodo.2009.11.015},
file = {:Users/smcgregor/Documents/Mendeley Desktop/schultz_joachims_03a.pdf:pdf},
isbn = {1049-5258},
issn = {1097-6752},
journal = {Advances in Neural Information Processing Systems 16},
pages = {41--48},
pmid = {21724089},
title = {{Learning a Distance Metric from Relative Comparisons}},
year = {2004}
}
@article{Seaman1997,
author = {Seaman, CB and Basili, VR},
isbn = {0897919149},
title = {{An empirical study of communication in code inspections}},
url = {http://dl.acm.org/citation.cfm?id=253248},
year = {1997}
}
@misc{Sebsauvage2013,
author = {Sebsauvage},
howpublished = {http://sebsauvage.net/wiki/doku.php?id=php:zerobin},
title = {{ZeroBin}},
url = {http://sebsauvage.net/wiki/doku.php?id=php:zerobin},
year = {2013}
}
@article{Sedlmair2014,
author = {Sedlmair, M. and Heinzl, C. and Bruckner, S. and Piringer, H. and M{\"{o}}ller, T.},
doi = {10.1109/TVCG.2014.2346321},
file = {:Users/smcgregor/Documents/Mendeley Desktop/visualParameterSpace.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {12},
title = {{Visual parameter space analysis: A conceptual framework}},
volume = {20},
year = {2014}
}
@article{Sedlmair2012,
author = {Sedlmair, Michael and Meyer, Miriah and Munzner, Tamara},
file = {:Users/smcgregor/Documents/Mendeley Desktop/dsm.pdf:pdf},
journal = {IEEE transactions on visualization and computer graphics Visualization and Computer Graphics},
number = {October},
title = {{Design Study Methodology: Reflections from the Trenches and the Stacks}},
year = {2012}
}
@book{seow2008designing,
author = {Seow, Steven C},
file = {:Users/smcgregor/Documents/Mendeley Desktop/UI Timing Cheatsheet.pdf:pdf},
publisher = {Addison-Wesley Professional},
title = {{Designing and engineering time: The psychology of time perception in software}},
year = {2008}
}
@article{Shakhnarovich2003,
abstract = {Example-based methods are effective for parameter estimation problems when the underlying system is simple or the dimensionality of the input is low. For complex and high-dimensional problems such as pose estimation, the number of required examples and the computational complexity rapidly become prohibitively high. We introduce a new algorithm that learns a set of hashing functions that efficiently index examples relevant to a particular estimation task. Our algorithm extends locality-sensitive hashing, a recently developed method to find approximate neighbors in time sublinear in the number of examples. This method depends critically on the choice of hash functions that are optimally relevant to a particular estimation problem. Experiments demonstrate that the resulting algorithm, which we call parameter-sensitive hashing, can rapidly and accurately estimate the articulated pose of human figures from a large database of example images.},
author = {Shakhnarovich, G. and Viola, P. and Darrell, T.},
doi = {10.1109/ICCV.2003.1238424},
file = {:Users/smcgregor/Documents/Mendeley Desktop/iccv2003.pdf:pdf},
isbn = {0-7695-1950-4},
journal = {Proceedings Ninth IEEE International Conference on Computer Vision},
title = {{Fast pose estimation with parameter-sensitive hashing}},
year = {2003}
}
@article{Shekhar2012,
author = {Shekhar, Shashi and Dietz, Michael and Wallach, DS},
file = {:Users/smcgregor/Documents/Mendeley Desktop/sec12-final101.pdf:pdf},
journal = {Proceedings of the 21st USENIX Conference on Security Symposium},
title = {{Adsplit: Separating smartphone advertising from applications}},
url = {https://www.usenix.org/system/files/conference/usenixsecurity12/sec12-final101.pdf},
year = {2012}
}
@article{Simpson2008,
author = {Simpson, TW and Toropov, V},
file = {:Users/smcgregor/Documents/Mendeley Desktop/2008_MAO.pdf:pdf},
journal = {{\ldots} /ISSMO multidisciplinary {\ldots}},
pages = {1--22},
title = {{Design and analysis of computer experiments in multidisciplinary design optimization: a review of how far we have come or not}},
url = {http://arc.aiaa.org/doi/pdf/10.2514/6.2008-5802},
year = {2008}
}
@article{Sims1994,
address = {New York, New York, USA},
author = {Sims, Karl},
doi = {10.1145/192161.192167},
file = {:Users/smcgregor/Documents/Mendeley Desktop/sims-virtual-creatures.pdf:pdf},
isbn = {0897916670},
journal = {Proceedings of the 21st annual conference on Computer graphics and interactive techniques - SIGGRAPH '94},
pages = {15--22},
publisher = {ACM Press},
title = {{Evolving virtual creatures}},
url = {http://portal.acm.org/citation.cfm?doid=192161.192167},
year = {1994}
}
@misc{Singel2007,
author = {Singel, Ryan},
booktitle = {Wired},
howpublished = {http://www.wired.com/threatlevel/2007/11/encrypted-e-mai/},
title = {{Encrypted E-Mail Company Hushmail Spills to Feds}},
url = {http://www.wired.com/threatlevel/2007/11/encrypted-e-mai/},
urldate = {2013-12-11},
year = {2007}
}
@misc{Sophos2013,
author = {Sophos},
howpublished = {http://www.sophos.com/en-us/security-news-trends/reports/security-threat-report/java-security.aspx},
title = {{Security Threat Report 2013}},
url = {http://www.sophos.com/en-us/security-news-trends/reports/security-threat-report/java-security.aspx},
urldate = {2013-12-11},
year = {2013}
}
@article{Spence1995,
abstract = {We present two novel visualisation tools: the Influence Explorer and the Prosection Matrix. These were specifically created to support engineering artifact design and similar tasks in which a set of parameter values must be chosen to lead to acceptable artifact performance. These tools combine two concepts. One is the interactive and virtually immediate responsive display of data in a manner conducive to the acquisition of insight. The other, involving the precalculation of samples of artifact performance, facilitates smooth exploration and optimisation leading to a design decision. The anticipated benefits of these visualisation tools are illustrated by an example taken from electronic circuit design, in which full account must be taken of the uncertainties in parameter values arising from inevitable variations in the manufacturing process.},
annote = {This seems to be a foundational paper on using visualization in the design process. Specifically, it advocates an iterative process.},
author = {Spence, B. and Tweedie, L. and Dawkes, H. and Su, Hua Su Hua},
doi = {10.1109/INFVIS.1995.528680},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Spence95.pdf:pdf},
isbn = {0-8186-7201-3},
journal = {Proceedings of Visualization 1995 Conference},
pages = {4--10},
title = {{Visualisation for functional design}},
year = {1995}
}
@book{Stanton2013,
address = {Burlington, VT},
author = {Stanton, Neville A. and Salmon, Paul M. and Rafferty, Laura A. and Walker, Guy H. and Baber, Chris and Jenkins, Daniel P.},
edition = {Second},
isbn = {9781409457534},
pages = {627},
publisher = {Ashgate Publishing Company},
title = {{Human Factors Methods: A Practical Guide for Engineering And Design}},
year = {2013}
}
@inproceedings{Starin2009,
abstract = {Online social networks (OSNs) are immensely popular, with some claiming over 200 million users. Users share private content, such as personal information or photographs, using OSN applications. Users must trust the OSN service to protect personal information even as the OSN provider benefits from examining and sharing that information. We present Persona, an OSN where users dictate who may access their information. Persona hides user data with attribute-based encryption (ABE), allowing users to apply fine-grained policies over who may view their data. Persona provides an effective means of creating applications in which users, not the OSN, define policy over access to private data. We demonstrate new cryptographic mechanisms that enhance the general applicability of ABE. We show how Persona provides the functionality of existing online social networks with additional privacy benefits. We describe an implementation of Persona that replicates Facebook applications and show that Persona provides acceptable performance when browsing privacy-enhanced web pages, even on mobile devices.},
author = {Starin, Daniel and Baden, Randy and Bender, Adam and Spring, Neil and Bhattacharjee, Bobby},
booktitle = {Proceedings of the ACM SIGCOMM 2009 conference on Data communication},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Starin et al. - 2009 - Persona An Online Social Network with User-Defined Privacy Categories and Subject Descriptors.pdf:pdf},
isbn = {9781605585949},
keywords = {abe,facebook,osn,persona,privacy,social networks},
pages = {135--146},
title = {{Persona : An Online Social Network with User-Defined Privacy Categories and Subject Descriptors}},
year = {2009}
}
@article{Stark2009,
author = {Stark, Emily and Hamburg, Michael and Boneh, Dan},
doi = {10.1109/ACSAC.2009.42},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Stark, Hamburg, Boneh - 2009 - Symmetric Cryptography in Javascript.pdf:pdf},
isbn = {978-1-4244-5327-6},
journal = {2009 Annual Computer Security Applications Conference},
keywords = {-javascript,cryptography,optimization},
month = {dec},
pages = {373--381},
publisher = {Ieee},
title = {{Symmetric Cryptography in Javascript}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5380691},
year = {2009}
}
@misc{Steadman2013,
author = {Steadman, Ian},
booktitle = {Wired.co.uk},
file = {:Users/smcgregor/Documents/Mendeley Desktop/This AI 'solves' Super M...ic NES games (Wired UK).pdf:pdf},
pages = {1--4},
title = {{This AI ' solves ' Super Mario Bros . and other classic NES games}},
year = {2013}
}
@article{Strehl2008b,
author = {Strehl, Alexander L. and Littman, Michael L.},
doi = {10.1016/j.jcss.2007.08.009},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1-s2.0-S0022000008000767-main.pdf:pdf},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
keywords = {reinforcement learning},
month = {dec},
number = {8},
pages = {1309--1331},
publisher = {Elsevier Inc.},
title = {{An analysis of model-based Interval Estimation for Markov Decision Processes}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022000008000767},
volume = {74},
year = {2008}
}
@article{Strens2000,
annote = {A &quot;hypothesis&quot; of the world is drawn from the prior belief of the worlds combined with the samples. The MDP is then explored with a policy greedy in the hypothesis MDP.},
author = {Strens, Malcolm},
file = {:Users/smcgregor/Documents/Mendeley Desktop/paper1c.pdf:pdf},
journal = {ICML},
title = {{A Bayesian framework for reinforcement learning}},
url = {http://web.eecs.utk.edu/$\sim$itamar/courses/ECE-692/paper1c.pdf},
year = {2000}
}
@article{Sun2011,
author = {Sun, Fangqi and Xu, L and Su, Z},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Sun.pdf:pdf},
journal = {Proceedings of the 20th USENIX Conference on Security Symposium},
title = {{Static detection of access control vulnerabilities in web applications}},
url = {http://static.usenix.org/event/sec11/tech/full_papers/Sun.pdf},
year = {2011}
}
@article{Sun2010,
author = {Sun, Guangyong and Li, Guangyao and Zhou, Shiwei and Xu, Wei and Yang, Xujing and Li, Qing},
doi = {10.1007/s00158-010-0596-5},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Multi-fidelity_optimization_for_sheet_metal_forming_process_(1).pdf:pdf},
issn = {1615-147X},
journal = {Structural and Multidisciplinary Optimization},
keywords = {drawbead design,method,mls,moving least squares,multi-fidelity optimization,multiobjective optimization,response surface,rsm,sheet-metal forming},
month = {dec},
number = {1},
pages = {111--124},
title = {{Multi-fidelity optimization for sheet metal forming process}},
url = {http://link.springer.com/10.1007/s00158-010-0596-5},
volume = {44},
year = {2010}
}
@article{Sutherland2000,
author = {Sutherland, Peter and Rossini, A},
file = {:Users/smcgregor/Documents/Mendeley Desktop/trs46_orca (1).pdf:pdf},
journal = {{\ldots} of Computational and {\ldots}},
number = {046},
title = {{Orca: A visualization toolkit for high-dimensional data}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474896},
year = {2000}
}
@inproceedings{sutton1996model,
author = {Sutton, Leonid Kuvayev Rich},
booktitle = {Proceedings of the ninth Yale workshop on adaptive and learning systems},
keywords = {modeling},
mendeley-tags = {modeling},
pages = {101--105},
title = {{Model-based reinforcement learning with an approximate, learned model}},
year = {1996}
}
@article{Sutton2000,
abstract = {Function approximation is essential to reinforcement learning, but\nthe standard approach of approximating a value function and determining\na policy from it has so far proven theoretically intractable. In\nthis paper we explore an alternative approach in which the policy\nis explicitly represented by its own function approximator, independent\nof the value function, and is updated according to the gradient of\nexpected reward with respect to the policy parameters. Williams's\nREINFORCE method and actor-critic methods are examples of this approach.\nOur main new result is to show that the gradient can be written in\na form suitable for estimation from experience aided by an approximate\naction-value or advantage function. Using this result, we prove for\nthe first time that a version of policy iteration with arbitrary\ndifferentiable function approximation is convergent to a locally\noptimal policy.},
author = {Sutton, R S and Mcallester, D and Singh, S and Mansour, Y},
file = {:Users/smcgregor/Documents/Mendeley Desktop/sutton-mcallester-singh-mansour-policy-gradient-methods-for-reinforcement-learning-with-function-approximation-nips2000.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1057--1063},
title = {{Policy gradient methods for reinforcement learning with function approximation}},
volume = {12},
year = {2000}
}
@article{Syed2008,
address = {New York, New York, USA},
author = {Syed, Umar and Bowling, Michael and Schapire, Robert E.},
doi = {10.1145/1390156.1390286},
file = {:Users/smcgregor/Documents/Mendeley Desktop/645.pdf:pdf},
isbn = {9781605582054},
journal = {Proceedings of the 25th international conference on Machine learning},
pages = {1032--1039},
publisher = {ACM Press},
title = {{Apprenticeship learning using linear programming}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390286},
year = {2008}
}
@misc{Symantec2011,
abstract = {Third parties, in particular advertisers, have accidentally had access to Facebook users’ accounts including profiles, photographs, chat, and also had the ability to post messages and mine personal information. Fortunately, these third-parties may not have realized their ability to access this information. We have reported this issue to Facebook, who has taken corrective action to help eliminate this issue. Facebook applications are Web applications that are integrated onto the Facebook platform. According to Facebook, 20 million Facebook applications are installed every day. Symantec has discovered that in certain cases, Facebook IFRAME applications inadvertently leaked access tokens to third parties like advertisers or analytic platforms. We estimate that as of April 2011, close to 100,000 applications were enabling this leakage. We estimate that over the years, hundreds of thousands of applications may have inadvertently leaked millions of access tokens to third parties. Access tokens are like ‘spare keys’ granted by you to the Facebook application. Applications can use these tokens or keys to perform certain actions on behalf of the user or to access the user’s profile. Each token or ‘spare key’ is associated with a select set of permissions, like reading your wall, accessing your friend’s profile, posting to your wall, etc.},
author = {Symantec},
booktitle = {Official Blog},
howpublished = {http://www.symantec.com/connect/blogs/facebook-applications-accidentally-leaking-access-third-parties-updated},
title = {{Facebook Applications Accidentally Leaking Access to Third Parties - Updated}},
url = {http://www.symantec.com/connect/blogs/facebook-applications-accidentally-leaking-access-third-parties-updated},
year = {2011}
}
@misc{TableauSoftware2012,
author = {{Tableau Software}},
title = {{Who We Are}},
url = {http://www.tableausoftware.com/about/who-we-are},
year = {2012}
}
@inproceedings{tatu2009combining,
abstract = {Visual exploration of multivariate data typically requires projection onto lower-dimensional representations. The number of possible representations grows rapidly with the number of dimensions, and manual exploration quickly becomes ineffective or even unfeasible. This paper proposes automatic analysis methods to extract potentially relevant visual structures from a set of candidate visualizations. Based on features, the visualizations are ranked in accordance with a specified user task. The user is provided with a manageable number of potentially useful candidate visualizations, which can be used as a starting point for interactive data analysis. This can effectively ease the task of finding truly useful visualizations and potentially speed up the data exploration task. In this paper, we present ranking measures for class-based as well as non class-based Scatterplots and Parallel Coordinates visualizations. The proposed analysis methods are evaluated on different datasets.},
author = {Tatu, A and Albuquerque, G and Eisemann, M and Schneidewind, J and Theisel, H and Magnork, M and Keim, D},
booktitle = {Visual Analytics Science and Technology, 2009. VAST 2009. IEEE Symposium on},
file = {:Users/smcgregor/Documents/Mendeley Desktop/05332628.pdf:pdf},
organization = {IEEE},
pages = {59--66},
title = {{Combining automated analysis and visualization techniques for effective exploration of high-dimensional data}},
year = {2009}
}
@article{TenCaat2007,
abstract = {The field of visualization assists data interpretation in many areas, but does not manage all types of data equally well. This holds, in particular, for time-varying multichannel EEG data. No existing method can successfully visualize simultaneous information from all channels in use at all time steps. To address this problem, a new visualization method is presented based on the parallel coordinate method and making use of a tiled organization. This tiled organization employs a two-dimensional row-column representation, rather than a one-dimensional arrangement in columns as used for classical parallel coordinates. The usefulness of the new method, referred to as tiled parallel coordinates (TPC), is demonstrated by a particular type of EEG data. It can be applied to an arbitrary number of time steps, handling the maximum number of channels currently in use. An extensive user evaluation shows that, for a typical EEG assessment task, data evaluation by the TPC method is faster than by an existing clinical EEG visualization method, without loss of information. The generality of the TPC method makes it widely applicable to other time-varying multivariate data types.},
author = {ten Caat, Michael and Maurits, Natasha M and Roerdink, Jos B T M},
doi = {10.1109/TVCG.2007.9},
file = {:Users/smcgregor/Documents/Mendeley Desktop/04015399.pdf:pdf},
issn = {1077-2626},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Computer Graphics,Data Display,Diagnosis, Computer-Assisted,Diagnosis, Computer-Assisted: methods,Electroencephalography,Electroencephalography: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Software,Software Design,Software Validation,User-Computer Interface},
number = {1},
pages = {70--9},
pmid = {17093337},
title = {{Design and evaluation of tiled parallel coordinate visualization of multichannel EEG data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17093337},
volume = {13},
year = {2007}
}
@misc{Project2013,
author = {{The Gnu Project}},
howpublished = {http://www.gnupg.org/},
title = {{The Gnu Privacy Guard}},
url = {http://www.gnupg.org/},
urldate = {2013-12-11},
year = {2013}
}
@misc{TheMendeleySupportTeam2011a,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:Users/smcgregor/Documents/Mendeley Desktop/The Mendeley Support Team - 2011 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@misc{TheMendeleySupportTeam2011b,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:Users/smcgregor/Documents/Mendeley Desktop/The Mendeley Support Team - 2011 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:Users/smcgregor/Documents/Mendeley Desktop/The Mendeley Support Team - 2011 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{thomas2011symmetry,
abstract = {Study of symmetric or repeating patterns in scalar fields is important in scientific data analysis because it gives deep insights into the properties of the underlying phenomenon. Though geometric symmetry has been well studied within areas like shape processing, identifying symmetry in scalar fields has remained largely unexplored due to the high computational cost of the associated algorithms. We propose a computationally efficient algorithm for detecting symmetric patterns in a scalar field distribution by analysing the topology of level sets of the scalar field. Our algorithm computes the contour tree of a given scalar field and identifies subtrees that are similar. We define a robust similarity measure for comparing subtrees of the contour tree and use it to group similar subtrees together. Regions of the domain corresponding to subtrees that belong to a common group are extracted and reported to be symmetric. Identifying symmetry in scalar fields finds applications in visualization, data exploration, and feature detection. We describe two applications in detail: symmetry-aware transfer function design and symmetry-aware isosurface extraction.},
author = {Thomas, D M and Natarajan, V},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06064967.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2035--2044},
publisher = {IEEE},
title = {{Symmetry in Scalar Field Topology}},
volume = {17},
year = {2011}
}
@article{Thomas,
annote = {1. The type of this paper is a Model paper: This studies a set of popular R libraries for a formal characterization of the types of user involvement that are possible.

2. Interesting because it describes the types of interactions that may be possible across many different algorithms.

3. Main contributions (see highlight)

4. It outlines several means of interogating the algorithm, including methods of decreasing the complexity so as to get intermediate results.

5. Not a design

6. Good and needed.},
author = {Thomas, M and Piringer, Harald and Gratzl, Samuel and Sedlmair, Michael and Streit, Marc},
file = {:Users/smcgregor/Documents/Mendeley Desktop/2014_vast_openingblackbox.pdf:pdf},
title = {{Opening the Black Box : Strategies for Increased User Involvement in Existing Algorithm Implementations}}
}
@article{Thompson2012,
author = {Thompson, Matthew P and Vaillant, Nicole M and Haas, Jessica R and Gebert, Krista M and Stockmann, Keith D},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Thompson_FuelTreatment_SuppressionCosts2012.pdf:pdf},
keywords = {2006,avoided suppression costs,benefits from fuel treatments,hazardous fuels,however,in terms of,rigor-,risk assessment,snider et al,studies have yet to,suppression cost,wildfire management},
pages = {1--10},
title = {{Quantifying the Potential Impacts of Fuel Treatments on Wildfire Suppression Costs}},
year = {2012}
}
@article{Thompson1933,
author = {Thompson, WR},
file = {:Users/smcgregor/Documents/Mendeley Desktop/2332286.pdf:pdf},
journal = {Biometrika},
number = {3},
pages = {285--294},
title = {{On the likelihood that one unknown probability exceeds another in view of the evidence of two samples}},
url = {http://www.jstor.org/stable/2332286},
volume = {25},
year = {1933}
}
@article{tierny2012generalized,
abstract = {We present a combinatorial algorithm for the general topological simplification of scalar fields on surfaces. Given a scalar field f, our algorithm generates a simplified field g that provably admits only critical points from a constrained subset of the singularities of f, while guaranteeing a small distance ||f - g||∞ for data-fitting purpose. In contrast to previous algorithms, our approach is oblivious to the strategy used for selecting features of interest and allows critical points to be removed arbitrarily. When topological persistence is used to select the features of interest, our algorithm produces a standard ϵ-simplification. Our approach is based on a new iterative algorithm for the constrained reconstruction of sub- and sur-level sets. Extensive experiments show that the number of iterations required for our algorithm to converge is rarely greater than 2 and never greater than 5, yielding O(n log(n)) practical time performances. The algorithm handles triangulated surfaces with or without boundary and is robust to the presence of multi-saddles in the input. It is simple to implement, fast in practice and more general than previous techniques. Practically, our approach allows a user to arbitrarily simplify the topology of an input function and robustly generate the corresponding simplified function. An appealing application area of our algorithm is in scalar field design since it enables, without any threshold parameter, the robust pruning of topological noise as selected by the user. This is needed for example to get rid of inaccuracies introduced by numerical solvers, thereby providing topological guarantees needed for certified geometry processing. Experiments show this ability to eliminate numerical noise as well as validate the time efficiency and accuracy of our algorithm. We provide a lightweight C++ implementation as supplemental material that can be used for topological cleaning on surface meshes.},
author = {Tierny, J and Pascucci, V},
file = {:Users/smcgregor/Documents/Mendeley Desktop/06327204.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2005--2013},
publisher = {IEEE},
title = {{Generalized Topological Simplification of Scalar Fields on Surfaces}},
volume = {18},
year = {2012}
}
@article{TinKamHoinM.GrahamM.Fitzpatrick2008,
author = {{Tin Kam Ho, in M. Graham, M. Fitzpatrick}, T. McGlynn},
file = {:Users/smcgregor/Documents/Mendeley Desktop/mirage_vobook.pdf:pdf},
journal = {The National Virtual Observatory: Tools And Techniques For Astronomical Research},
number = {2008, 29-36},
pages = {1--9},
title = {{Chapter 1 : Mirage : A Visualization Tool for VO Data}},
volume = {CS-382},
year = {2008}
}
@article{Treisman1985,
abstract = {Visual analysis appears to be functionally divided between an early preattentive level of processing at which simple features are coded spatially in parallel and a later stage at which focused attention is required to conjoin the separate features into coherent objects. Evidence supporting this dichotomy comes from behavioral studies of visual search, from differences in the ease of texture segregation, from reports of illusory conjunctions when attention is overloaded, from subjects' ability to identify simple features correctly even when they mislocate them, and from the substantial benefit of pre-cuing the location of a relevant item when the task requires that features be conjoined but not when simple features are sufficient. Some further studies of search have revealed a striking asymmetry between several pairs of stimuli which differ in the presence or absence of a single part or property. The asymmetry depends solely on which of the pair is allocated the role of target and which is replicated to form the background items. It suggests that search for the presence of a visual primitive is automatic and parallel, whereas search for the absence of the same feature is serial and requires focused attention. The search asymmetry can be used as an additional diagnostic to help define the functional features extracted by the visual system.},
author = {Treisman, Anne},
journal = {Computer Vision, Graphics, and Image Processing},
number = {2},
pages = {156--177},
title = {{Preattentive Processing in Vision}},
volume = {31},
year = {1985}
}
@book{tufte1983visual,
address = {Cheshire, CT},
author = {Tufte, Edward R},
pages = {197},
publisher = {Graphics press},
title = {{The Visual Display of Quantitative Information}},
volume = {31},
year = {1983}
}
@article{turkay2011brushing,
abstract = {In many application fields, data analysts have to deal with datasets that contain many expressions per item. The effective analysis of such multivariate datasets is dependent on the user's ability to understand both the intrinsic dimensionality of the dataset as well as the distribution of the dependent values with respect to the dimensions. In this paper, we propose a visualization model that enables the joint interactive visual analysis of multivariate datasets with respect to their dimensions as well as with respect to the actual data values. We describe a dual setting of visualization and interaction in items space and in dimensions space. The visualization of items is linked to the visualization of dimensions with brushing and focus+context visualization. With this approach, the user is able to jointly study the structure of the dimensions space as well as the distribution of data items with respect to the dimensions. Even though the proposed visualization model is general, we demonstrate its application in the context of a DNA microarray data analysis.},
author = {Turkay, C and Filzmoser, P and Hauser, H},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Turkay, Filzmoser, Hauser - 2011 - Brushing Dimensions-A Dual Visual Analysis Model for High-Dimensional Data.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {2591--2599},
publisher = {IEEE},
title = {{Brushing Dimensions-A Dual Visual Analysis Model for High-Dimensional Data}},
volume = {17},
year = {2011}
}
@inproceedings{tzeng2005opening,
abstract = {Artificial neural networks are computer software or hardware models inspired by the structure and behavior of neurons in the human nervous system. As a powerful learning tool, increasingly neural networks have been adopted by many large-scale information processing applications but there is no a set of well defined criteria for choosing a neural network. The user mostly treats a neural network as a black box and cannot explain how learning from input data was done nor how performance can be consistently ensured. We have experimented with several information visualization designs aiming to open the black box to possibly uncover underlying dependencies between the input data and the output data of a neural network. In this paper, we present our designs and show that the visualizations not only help us design more efficient neural networks, but also assist us in the process of using neural networks for problem solving such as performing a classification task.},
author = {Tzeng, F Y and Ma, K L},
booktitle = {Visualization, 2005. VIS 05. IEEE},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Tzeng, Ma - 2005 - Opening the black box-data driven visualization of neural networks.pdf:pdf},
organization = {IEEE},
pages = {383--390},
title = {{Opening the black box-data driven visualization of neural networks}},
year = {2005}
}
@misc{Unhosted2013,
author = {Unhosted},
howpublished = {http://tos-dr.info/},
title = {{Terms of Service; Didn't Read}},
url = {http://tos-dr.info/},
year = {2013}
}
@inproceedings{urness2003effectively,
abstract = {In this paper we offer several new insights and techniques for effectively using color and texture to simultaneously convey information about multiple 2D scalar and vector distributions, in a way that facilitates allowing each distribution to be understood both individually and in the context of one or more of the other distributions. Specifically, we introduce the concepts of: color weaving for simultaneously representing information about multiple co-located color encoded distributions; and texture stitching for achieving more spatially accurate multi-frequency line integral convolution representations of combined scalar and vector distributions. The target application for our research is the definition, detection and visualization of regions of interest in a turbulent boundary layer flow at moderate Reynolds number. In this work, we examine and analyze streamwise-spanwise planes of three-component velocity vectors with the goal of identifying and characterizing spatially organized packets of hairpin vortices.},
author = {Urness, T and Interrante, V and Marusic, I and Longmire, E and Ganapathisubramani, B},
booktitle = {Visualization, 2003. VIS 2003. IEEE},
organization = {IEEE},
pages = {115--121},
title = {{Effectively visualizing multi-valued flow data using color and texture}},
year = {2003}
}
@article{VanRoy,
annote = {In this paper the value function is randomized instead of an epsilon greedy approach. It seems like the same effect could be achieved if epsilon greedy were simply modified to go off-policy with probability scaled to the expected regret. This is not a novel observation since Thompson sampling is essentially this, but it appears that this work is a generalization on Thompson sampling. Worth thinking about, but I don't see how it would apply to our problem.},
archivePrefix = {arXiv},
arxivId = {arXiv:submit/0904806},
author = {{Van Roy}, Benjamin and Wen, Zheng},
eprint = {0904806},
file = {:Users/smcgregor/Documents/Mendeley Desktop/vanroy-wen-generalization-and-exploration-via-randomized-value-functions-arxiv2014.pdf:pdf},
pages = {1--18},
primaryClass = {arXiv:submit},
title = {{Generalization and Exploration via Randomized Value Functions}}
}
@inproceedings{van2005value,
abstract = {The field of visualization is getting mature. Many problems have been solved, and new directions are sought for. In order to make good choices, an understanding of the purpose and meaning of visualization is needed. Especially, it would be nice if we could assess what a good visualization is. In this paper an attempt is made to determine the value of visualization. A technological viewpoint is adopted, where the value of visualization is measured based on effectiveness and efficiency. An economic model of visualization is presented, and benefits and costs are established. Next, consequences (brand limitations of visualization are discussed (including the use of alternative methods, high initial costs, subjective/less, and the role of interaction), as well as examples of the use of the model for the judgement of existing classes of methods and understanding why they are or are not used in practice. Furthermore, two alternative views on visualization are presented and discussed: viewing visualization as an art or as a scientific discipline. Implications and future directions are identified.},
author = {{Van Wijk}, J J},
booktitle = {Visualization, 2005. VIS 05. IEEE},
file = {:Users/smcgregor/Documents/Mendeley Desktop/01532781 17-55-16.pdf:pdf},
organization = {IEEE},
pages = {79--86},
title = {{The value of visualization}},
year = {2005}
}
@article{VanWijk2005,
abstract = {The field of visualization is getting mature. Many problems have been solved, and new directions are sought for. In order to make good choices, an understanding of the purpose and meaning of visualization is needed. Especially, it would be nice if we could assess what a good visualization is. In this paper an attempt is made to determine the value of visualization. A technological viewpoint is adopted, where the value of visualization is measured based on effectiveness and efficiency. An economic model of visualization is presented, and benefits and costs are established. Next, consequences (brand limitations of visualization are discussed (including the use of alternative methods, high initial costs, subjective/less, and the role of interaction), as well as examples of the use of the model for the judgement of existing classes of methods and understanding why they are or are not used in practice. Furthermore, two alternative views on visualization are presented and discussed: viewing visualization as an art or as a scientific discipline. Implications and future directions are identified.},
author = {van Wijk, J.J.},
doi = {10.1109/VIS.2005.102},
file = {:Users/smcgregor/Documents/Mendeley Desktop/01532781 17-55-16.pdf:pdf},
isbn = {0-7803-9462-3},
journal = {IEEE Visualization 2005 - (VIS'05)},
keywords = {evaluation,visualization},
pages = {11--11},
publisher = {Ieee},
title = {{The Value of Visualization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1565993},
year = {2005}
}
@misc{Victor2012,
author = {Victor, Bret},
title = {{Tangle}},
url = {http://worrydream.com/Tangle/},
urldate = {0012-12-12},
year = {2012}
}
@article{W.BergerH.PiringerP.Filzmoser2011,
annote = {Coping with slow simulators.},
author = {{W. Berger, H. Piringer, P. Filzmoser}, and M. E. Groller},
file = {:Users/smcgregor/Documents/Mendeley Desktop/PubDat_203913.pdf:pdf},
journal = {Computer Graphics Forum},
number = {3},
pages = {911--920},
title = {{Uncertainty-aware exploration of continnuous parameter spaces using multivariate prediction.}},
volume = {30(3)},
year = {2011}
}
@article{wang2011branching,
abstract = {Large observations and simulations in scientific research give rise to high-dimensional data sets that present many challenges and opportunities in data analysis and visualization. Researchers in application domains such as engineering, computational biology, climate study, imaging and motion capture are faced with the problem of how to discover compact representations of highdimensional data while preserving their intrinsic structure. In many applications, the original data is projected onto low-dimensional space via dimensionality reduction techniques prior to modeling. One problem with this approach is that the projection step in the process can fail to preserve structure in the data that is only apparent in high dimensions. Conversely, such techniques may create structural illusions in the projection, implying structure not present in the original high-dimensional data. Our solution is to utilize topological techniques to recover important structures in high-dimensional data that contains non-trivial topology. Specifically, we are interested in high-dimensional branching structures. We construct local circle-valued coordinate functions to represent such features. Subsequently, we perform dimensionality reduction on the data while ensuring such structures are visually preserved. Additionally, we study the effects of global circular structures on visualizations. Our results reveal never-before-seen structures on real-world data sets from a variety of applications.},
author = {Wang, B and Summa, B and Pascucci, V and Vejdemo-Johansson, M},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Wang et al. - 2011 - Branching and circular features in high dimensional data.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {1902--1911},
publisher = {IEEE},
title = {{Branching and circular features in high dimensional data}},
volume = {17},
year = {2011}
}
@article{waser2011nodes,
abstract = {Flood disasters are the most common natural risk and tremendous efforts are spent to improve their simulation and management. However, simulation-based investigation of actions that can be taken in case of flood emergencies is rarely done. This is in part due to the lack of a comprehensive framework which integrates and facilitates these efforts. In this paper, we tackle several problems which are related to steering a flood simulation. One issue is related to uncertainty. We need to account for uncertain knowledge about the environment, such as levee-breach locations. Furthermore, the steering process has to reveal how these uncertainties in the boundary conditions affect the confidence in the simulation outcome. Another important problem is that the simulation setup is often hidden in a black-box. We expose system internals and show that simulation steering can be comprehensible at the same time. This is important because the domain expert needs to be able to modify the simulation setup in order to include local knowledge and experience. In the proposed solution, users steer parameter studies through the World Lines interface to account for input uncertainties. The transport of steering information to the underlying data-flow components is handled by a novel meta-flow. The meta-flow is an extension to a standard data-flow network, comprising additional nodes and ropes to abstract parameter control. The meta-flow has a visual representation to inform the user about which control operations happen. Finally, we present the idea to use the data-flow diagram itself for visualizing steering information and simulation results. We discuss a case-study in collaboration with a domain expert who proposes different actions to protect a virtual city from imminent flooding. The key to choosing the best response strategy is the ability to compare different regions of the parameter space while retaining an understanding of what is happening inside the data-flow system.},
author = {Waser, J and Ribicic, H and Fuchs, R and Hirsch, C and Schindler, B and Bloschl, G and Groller, M E},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Waser et al. - 2011 - Nodes on ropes A comprehensive data and control flow for steering ensemble simulations.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {12},
pages = {1872--1881},
publisher = {IEEE},
title = {{Nodes on ropes: A comprehensive data and control flow for steering ensemble simulations}},
volume = {17},
year = {2011}
}
@article{Waser2010,
author = {Waser, J{\"{u}}rgen and Fuchs, Raphael and Ribicic, Hrvoje and Schindler, Benjamin and Bloschl, Gunther and Groller, M. Eduard},
file = {:Users/smcgregor/Documents/Mendeley Desktop/05613487.pdf:pdf},
journal = {IEEE transactions on visualization and computer graphics},
number = {6},
pages = {1458--1467},
title = {{World Lines}},
volume = {16},
year = {2010}
}
@article{Waser2014,
author = {Waser, J{\"{u}}rgen and Konev, A and Sadransky, B and Horv{\'{a}}th, Z and Ribicic, Hrvoje and Carnecky, R. and Kluding, P. and Schindler, B.},
file = {:Users/smcgregor/Documents/Mendeley Desktop/ManyPlans.pdf:pdf},
journal = {Eurographic Conference on Visualization (EuroVis)},
number = {3},
title = {{Many Plans: Multidimensional Ensembles for Visual Decision Support in Flood Management}},
volume = {33},
year = {2014}
}
@article{Waser2011,
abstract = {Flood disasters are the most common natural risk and tremendous efforts are spent to improve their simulation and management. However, simulation-based investigation of actions that can be taken in case of flood emergencies is rarely done. This is in part due to the lack of a comprehensive framework which integrates and facilitates these efforts. In this paper, we tackle several problems which are related to steering a flood simulation. One issue is related to uncertainty. We need to account for uncertain knowledge about the environment, such as levee-breach locations. Furthermore, the steering process has to reveal how these uncertainties in the boundary conditions affect the confidence in the simulation outcome. Another important problem is that the simulation setup is often hidden in a black-box. We expose system internals and show that simulation steering can be comprehensible at the same time. This is important because the domain expert needs to be able to modify the simulation setup in order to include local knowledge and experience. In the proposed solution, users steer parameter studies through the World Lines interface to account for input uncertainties. The transport of steering information to the underlying data-flow components is handled by a novel meta-flow. The meta-flow is an extension to a standard data-flow network, comprising additional nodes and ropes to abstract parameter control. The meta-flow has a visual representation to inform the user about which control operations happen. Finally, we present the idea to use the data-flow diagram itself for visualizing steering information and simulation results. We discuss a case-study in collaboration with a domain expert who proposes different actions to protect a virtual city from imminent flooding. The key to choosing the best response strategy is the ability to compare different regions of the parameter space while retaining an understanding of what is happening inside the data-flow system.},
author = {Waser, J{\"{u}}rgen and Ribi{\v{c}}i{\'{c}}, Hrvoje and Fuchs, Raphael and Hirsch, Christian and Schindler, Benjamin and Bl{\"{o}}schl, G{\"{u}}nther and Gr{\"{o}}ller, M Eduard},
doi = {10.1109/TVCG.2011.225},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Waser et al. - 2011 - Nodes on ropes A comprehensive data and control flow for steering ensemble simulations.pdf:pdf},
issn = {1941-0506},
journal = {IEEE transactions on visualization and computer graphics},
month = {dec},
number = {12},
pages = {1872--81},
pmid = {22034304},
title = {{Nodes on ropes: a comprehensive data and control flow for steering ensemble simulations.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22034304},
volume = {17},
year = {2011}
}
@article{Waterland2013,
author = {Waterland, Amos and Angelino, Elaine and Cubuk, Ekin D. and Kaxiras, Efthimios and Adams, Ryan P. and Appavoo, Jonathan and Seltzer, Margo},
doi = {10.1145/2485732.2485749},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Waterland13.pdf:pdf},
isbn = {9781450321167},
journal = {Proceedings of the 6th International Systems and Storage Conference on - SYSTOR '13},
pages = {1},
title = {{Computational caches}},
url = {http://dl.acm.org/citation.cfm?doid=2485732.2485749},
year = {2013}
}
@inproceedings{weaver2010multidimensional,
abstract = {Visual exploration and analysis is a process of discovering and dissecting the abundant and complex attribute relationships that pervade multidimensional data. Recent research has identified and characterized patterns of multiple coordinated views, such as cross-filtered views, in which rapid sequences of simple interactions can be used to express queries on subsets of attribute values. In visualizations designed around these patterns, for the most part, distinct views serve to visually isolate each attribute from the others. Although the brush-and-click simplicity of visual isolation facilitates discovery of many-to-many relationships between attributes, dissecting these relationships into more fine-grained one-to-many relationships is interactively tedious and, worse, visually fragmented over prolonged sequences of queries. This paper describes: (1) a method for interactively dissecting multidimensional data by iteratively slicing and manipulating a multigraph representation of data values and value co-occurrences; and (2) design strategies for extending the construction of coordinated multiple view interfaces for dissection as well as discovery of attribute relationships in multidimensional data sets. Using examples from different domains, we describe how attribute relationship graphs can be combined with cross-filtered views, modularized for reuse across designs, and integrated into broader visual analysis tools. The exploratory and analytic utility of these examples suggests that an attribute relationship graph would be a useful addition to a wide variety of visual analysis tools.},
author = {Weaver, C},
booktitle = {Visual Analytics Science and Technology (VAST), 2010 IEEE Symposium on},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Weaver - 2010 - Multidimensional data dissection using attribute relationship graphs.pdf:pdf},
organization = {IEEE},
pages = {75--82},
title = {{Multidimensional data dissection using attribute relationship graphs}},
year = {2010}
}
@inproceedings{weaver2008multidimensional,
abstract = {Analysis of multidimensional data often requires careful examination of relationships across dimensions. Coordinated multiple view approaches have become commonplace in visual analysis tools because they directly support expression of complex multidimensional queries using simple interactions. However, generating such tools remains difficult because of the need to map domain-specific data structures and semantics into the idiosyncratic combinations of interdependent data and visual abstractions needed to reveal particular patterns and distributions in cross-dimensional relationships. This paper describes: (1) a method for interactively expressing sequences of multidimensional set queries by cross-filtering data values across pairs of views, and (2) design strategies for constructing coordinated multiple view interfaces for cross-filtered visual analysis of multidimensional data sets. Using examples of cross-filtered visualizations of data from several different domains, we describe how cross-filtering can be modularized and reused across designs, flexibly customized with respect to data types across multiple dimensions, and incorporated into more wide-ranging multiple view designs. The demonstrated analytic utility of these examples suggest that cross-filtering is a suitable design pattern for instantiation in a wide variety of visual analysis tools.},
author = {Weaver, C},
booktitle = {Visual Analytics Science and Technology, 2008. VAST'08. IEEE Symposium on},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Weaver - 2008 - Multidimensional visual analysis using cross-filtered views.pdf:pdf},
organization = {IEEE},
pages = {163--170},
title = {{Multidimensional visual analysis using cross-filtered views}},
year = {2008}
}
@inproceedings{weber2002exploring,
abstract = {Isosurfaces are commonly used to visualize scalar fields. Critical isovalues indicate isosurface topology changes: the creation of new surface components, merging of surface components or the formation of holes in a surface component. Therefore, they highlight interesting isosurface behavior and are helpful in exploration of large trivariate data sets. We present a method that detects critical isovalues in a scalar field defined by piecewise trilinear interpolation over a rectilinear grid and describe how to use them when examining volume data. We further review varieties of the marching cubes (MC) algorithm, with the intention of preserving topology of the trilinear interpolant when extracting an isosurface. We combine and extend two approaches in such a way that it is possible to extract meaningful isosurfaces even when a critical value is chosen as the isovalue.},
author = {Weber, G H and Scheuermann, G and Hagen, H and Hamann, B},
booktitle = {Visualization, 2002. VIS 2002. IEEE},
file = {:Users/smcgregor/Documents/Mendeley Desktop/01183772.pdf:pdf},
organization = {IEEE},
pages = {171--178},
title = {{Exploring scalar fields using critical isovalues}},
year = {2002}
}
@misc{Wee2011,
address = {Beijing/San Francisco},
author = {Wee, Sui-Lee and Oreskovic, Alexei},
booktitle = {Reuters},
month = {jun},
title = {{Google reveals Gmail hacking, says likely from China}},
url = {http://www.reuters.com/article/2011/06/02/us-google-hacking-idUSTRE7506U320110602},
year = {2011}
}
@inproceedings{wei2002simulating,
abstract = {We propose the use of textured splats as the basic display primitives for an open surface fire model. The high-detail textures help to achieve a smooth boundary of the fire and gain the small-scale turbulence appearance. We utilize the Lattice Boltzmann Model (LBM) to simulate physically-based equations describing the fire evolution and its interaction with the environment (e.g., obstacles, wind and temperature). The property of fuel and non-burning objects are defined on the lattice of the computation domain. A temperature field is also incorporated to model the generation of smoke from the fire due to incomplete combustion. The linear and local characteristics of the LBM enable us to accelerate the computation with graphics hardware to reach real-time simulation speed, while the texture splat primitives enable interactive rendering frame rates.},
author = {Wei, X and Li, W and Mueller, K and Kaufman, A},
booktitle = {Visualization, 2002. VIS 2002. IEEE},
file = {:Users/smcgregor/Documents/Mendeley Desktop/01183779.pdf:pdf},
organization = {IEEE},
pages = {227--234},
title = {{Simulating fire with texture splats}},
year = {2002}
}
@article{Wei2008,
author = {Wei, Yu and Rideout, Douglas and Kirsch, Andy},
doi = {10.1139/X07-162},
file = {:Users/smcgregor/Documents/Mendeley Desktop/wei-rideout-kirsch-optimization-model-locating-fuel-treatments-across-landscape-reduce-expected-fire-losses-cjfr2008.pdf:pdf},
issn = {0045-5067},
journal = {Canadian Journal of Forest Research},
month = {apr},
number = {4},
pages = {868--877},
title = {{An optimization model for locating fuel treatments across a landscape to reduce expected fire losses}},
url = {http://www.nrcresearchpress.com/doi/abs/10.1139/X07-162},
volume = {38},
year = {2008}
}
@article{Weinberger2006,
annote = {LMNN
900 citations for this.},
author = {Weinberger, K and Blitzer, John and Saul, L},
file = {:Users/smcgregor/Documents/Mendeley Desktop/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1473},
title = {{Distance metric learning for large margin nearest neighbor classification}},
url = {https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf},
volume = {18},
year = {2006}
}
@article{Weissman2003,
author = {Weissman, Tsachy and Ordentlich, Erik},
file = {:Users/smcgregor/Documents/Mendeley Desktop/HPL-2003-97R1.pdf:pdf},
institution = {HP Labs},
journal = {Hewlett-Packard Labs},
title = {{Inequalities for the L1 deviation of the empirical distribution}},
url = {http://www.hpl.hp.com/techreports/2003/HPL-2003-97R1.pdf?origin=publicationDetail},
year = {2003}
}
@article{wen2008evaluating,
abstract = {Data transformation, the process of preparing raw data for effective visualization, is one of the key challenges in information visualization. Although researchers have developed many data transformation techniques, there is little empirical study of the general impact of data transformation on visualization. Without such study, it is difficult to systematically decide when and which data transformation techniques are needed. We thus have designed and conducted a two-part empirical study that examines how the use of common data transformation techniques impacts visualization quality, which in turn affects user task performance. Our first experiment studies the impact of data transformation on user performance in single-step, typical visual analytic tasks. The second experiment assesses the impact of data transformation in multi-step analytic tasks. Our results quantify the benefits of data transformation in both experiments. More importantly, our analyses reveal that (1) the benefits of data transformation vary significantly by task and by visualization, and (2) the use of data transformation depends on a user's interaction context. Based on our findings, we present a set of design recommendations that help guide the development and use of data transformation techniques.},
author = {Wen, Z and Zhou, M X},
file = {:Users/smcgregor/Documents/Mendeley Desktop//Wen, Zhou - 2008 - Evaluating the use of data transformation for information visualization.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1309--1316},
publisher = {IEEE},
title = {{Evaluating the use of data transformation for information visualization}},
volume = {14},
year = {2008}
}
@article{Wender2012,
author = {Wender, Stefan and Watson, Ian},
doi = {10.1109/CIG.2012.6374183},
file = {:Users/smcgregor/Documents/Mendeley Desktop/paper44.pdf:pdf},
isbn = {9781467311922},
journal = {2012 IEEE Conference on Computational Intelligence and Games, CIG 2012},
pages = {402--408},
title = {{Applying reinforcement learning to small scale combat in the real-time strategy game StarCraft:Broodwar}},
year = {2012}
}
@article{Whitten1999,
author = {Whitten, Alma and Tygar, JD},
file = {:Users/smcgregor/Documents/Mendeley Desktop/HelpingJohnnyEncrypt.pdf:pdf},
journal = {Proceedings of the 8th USENIX Security Symposium},
title = {{Why Johnny can't encrypt: A usability evaluation of PGP 5.0}},
url = {http://www.usenix.org/events/sec99/full_papers/whitten/whitten.ps},
year = {1999}
}
@article{Williams1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {Williams, Ronald J.},
doi = {10.1007/BF00992696},
file = {:Users/smcgregor/Documents/Mendeley Desktop/williams-reinforce.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
pages = {229--256},
title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
volume = {8},
year = {1992}
}
@article{wold1987principal,
author = {Wold, S and Esbensen, K and Geladi, P},
journal = {Chemometrics and intelligent laboratory systems},
number = {1},
pages = {37--52},
publisher = {Elsevier},
title = {{Principal component analysis}},
volume = {2},
year = {1987}
}
@article{Wu2012,
abstract = {Learning distance functions with side information plays a key role in many data mining applications. Conventional distance metric learning approaches often assume that the target distance function is represented in some form of Mahalanobis distance. These approaches usually work well when data are in low dimensionality, but often become computationally expensive or even infeasible when handling high-dimensional data. In this paper, we propose a novel scheme of learning nonlinear distance functions with side information. It aims to learn a Bregman distance function using a nonparametric approach that is similar to Support Vector Machines. We emphasize that the proposed scheme is more general than the conventional approach for distance metric learning, and is able to handle high-dimensional data efficiently. We verify the efficacy of the proposed distance learning method with extensive experiments on semi-supervised clustering. The comparison with state-of-the-art approaches for learning distance functions with side information reveals clear advantages of the proposed technique. © 2006 IEEE.},
author = {Wu, Lei and Hoi, Steven C H and Jin, Rong and Zhu, Jianke and Yu, Nenghai},
doi = {10.1109/TKDE.2010.215},
file = {:Users/smcgregor/Documents/Mendeley Desktop/3678-learning-bregman-distance-functions-and-its-application-for-semi-supervised-clustering.pdf:pdf},
isbn = {9781615679119},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Bregman distance,convex functions,distance functions,metric learning},
number = {3},
pages = {478--491},
title = {{Learning Bregman distance functions for semi-supervised clustering}},
volume = {24},
year = {2012}
}
@article{Xing2003,
abstract = {Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many &034;plausible&034; ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider &034;similar.&034; For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in , learns a distance metric over that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance.},
annote = {They solve a convex optimization attempting to minimize the squared distance between points, subject to a minimal sum of distances. I think this is desireable because it standardizes the distances on each of the dimensions. It makes the whole thing less noisy, but it is not ultimately more similar unless the side information is included.

todo: look at how the side information is incorporated.},
author = {Xing, Eric P and Ng, Andrew Y and Jordan, Michael I and Russell, Stuart},
doi = {10.1.1.58.3667},
file = {:Users/smcgregor/Documents/Mendeley Desktop/AA03.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {2},
pages = {505--512},
pmid = {19276889},
title = {{Distance metric learning with application to clustering with side-information}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.7952&amp;rep=rep1&amp;type=pdf\nhttp://books.nips.cc/papers/files/nips15/AA03.pdf},
volume = {15},
year = {2003}
}
@article{Yang2006,
abstract = {Many machine learning algorithms, such as K Nearest Neighbor (KNN), heav- ily rely on the distance metric for the input data patterns. Distance Metric learning is to learn a distance metric for the input space of data from a given collection of pair of similar/dissimilar points that preserves the distance relation among the training data. In recent years, many studies have demonstrated, both empirically and theoretically, that a learned metric can significantly improve the performance in classification, clustering and retrieval tasks. This paper surveys the field of dis- tance metric learning from a principle perspective, and includes a broad selection of recent work. In particular, distance metric learning is reviewed under different learning conditions: supervised learning versus unsupervised learning, learning in a global sense versus in a local sense; and the distance matrix based on linear kernel versus nonlinear kernel. In addition, this paper discusses a number of techniques that is central to distance metric learning, including convex programming, posi- tive semi-definite programming, kernel learning, dimension reduction, K Nearest Neighbor, large margin classification, and graph-based approaches.},
author = {Yang, L and Jin, R},
doi = {10.1073/pnas.0809777106},
file = {:Users/smcgregor/Documents/Mendeley Desktop/frame_survey_v2.pdf:pdf},
issn = {00401951},
journal = {Unpublished Manuscript},
pages = {1--51},
pmid = {19342485},
title = {{Distance metric learning: A comprehensive survey}},
url = {http://www.cs.cmu.edu/$\sim$liuy/frame_survey_v2.pdf\npapers2://publication/uuid/AAD11FCE-1563-4566-BC0F-88222DD8F1C4},
year = {2006}
}
@article{Yang2010,
abstract = {Similarity measurement is a critical component in content-based image retrieval systems, and learning a good distance metric can significantly improve retrieval performance. However, despite extensive study, there are several major shortcomings with the existing approaches for distance metric learning that can significantly affect their application to medical image retrieval. In particular, "similarity" can mean very different things in image retrieval: resemblance in visual appearance (e.g., two images that look like one another) or similarity in semantic annotation (e.g., two images of tumors that look quite different yet are both malignant). Current approaches for distance metric learning typically address only one goal without consideration of the other. This is problematic for medical image retrieval where the goal is to assist doctors in decision making. In these applications, given a query image, the goal is to retrieve similar images from a reference library whose semantic annotations could provide the medical professional with greater insight into the possible interpretations of the query image. If the system were to retrieve images that did not look like the query, then users would be less likely to trust the system; on the other hand, retrieving images that appear superficially similar to the query but are semantically unrelated is undesirable because that could lead users toward an incorrect diagnosis. Hence, learning a distance metric that preserves both visual resemblance and semantic similarity is important. We emphasize that, although our study is focused on medical image retrieval, the problem addressed in this work is critical to many image retrieval systems. We present a boosting framework for distance metric learning that aims to preserve both visual and semantic similarities. The boosting framework first learns a binary representation using side information, in the form of labeled pairs, and then computes the distance as a weighted Hamming distance using the learned binary representation. A boosting algorithm is presented to efficiently learn the distance function. We evaluate the proposed algorithm on a mammographic image reference library with an Interactive Search-Assisted Decision Support (ISADS) system and on the medical image data set from ImageCLEF. Our results show that the boosting framework compares favorably to state-of-the-art approaches for distance metric learning in retrieval accuracy, with much lower computational cost. Additional evaluation with the COREL collection shows that our algorithm works well for regular image data sets.},
author = {Yang, Liu and Jin, Rong and Mummert, Lily and Sukthankar, Rahul and Goode, Adam and Zheng, Bin and Hoi, Steven C H and Satyanarayanan, Mahadev},
doi = {10.1109/TPAMI.2008.273},
file = {:Users/smcgregor/Documents/Mendeley Desktop/0deec52dd1564538b9000000.pdf:pdf},
isbn = {0162-8828 VO - 32},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Boosting,Distance metric learning,Image retrieval,Machine learning},
number = {1},
pages = {30--44},
pmid = {19926897},
title = {{A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval}},
volume = {32},
year = {2010}
}
@inproceedings{Yannakakis2004,
author = {Yannakakis, Mihalis},
booktitle = {Symposium on Logic in Computer Science},
file = {:Users/smcgregor/Documents/Mendeley Desktop/816068.pdf:pdf},
pages = {78--88},
title = {{Testing, Optimizaton, and Games}},
year = {2004}
}
@article{Yogatama2014,
abstract = {In many high-dimensional learning problems, only some parts of an observation are important to the prediction task; for example, the cues to correctly categorizing a document may lie in a handful of its sentences. We introduce a learning algorithm that exploits this intuition by encod- ing it in a regularizer. Specifically, we apply the sparse overlapping group lasso with one group for every bundle of features occurring together in a training-data sentence, leading to thousands to millions of overlapping groups. We show how to efficiently solve the resulting optimization chal- lenge using the alternating directions method of multipliers. We find that the resulting method significantly outperforms competitive baselines (standard ridge, lasso, and elastic net regulariz- ers) on a suite of real-world text categorization problems. 1.},
author = {Yogatama, Dani and Smith, Noah a},
file = {:Users/smcgregor/Documents/Mendeley Desktop/making-the-most-of-bag-of-words-sentence-regularization-with-alternating-direction-of-multipliers-yogatama-smith-2014.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of the 31st International Conference on Machine Learning},
title = {{Making the Most of Bag of Words: Sentence Regularization with Alternating Direction Method of Multipliers}},
volume = {32},
year = {2014}
}
@article{Yosinski2015,
annote = {A potential method of inspecting the features that are generated for image classification is to follow this methodology. This could be a way to offer interpretability of DNNs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.1897v3},
author = {Yosinski, Jason and Clune, Jeff},
eprint = {arXiv:1412.1897v3},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1412.1897v3.pdf:pdf},
title = {{Deep Neural Networks are Easily Fooled : High Confidence Predictions for Unrecognizable Images}},
year = {2015}
}
@article{Youdao2010,
author = {Youdao, NE},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Duan.pdf:pdf},
title = {{P4P: Practical large-scale privacy-preserving distributed computation robust against malicious users}},
url = {http://www.usenix.org/event/sec10/tech/full_papers/Duan.pdf},
year = {2010}
}
@article{yuan2009scattering,
abstract = {In this paper, we present a novel parallel coordinates design integrated with points (scattering points in parallel coordinates, SPPC), by taking advantage of both parallel coordinates and scatterplots. Different from most multiple views visualization frameworks involving parallel coordinates where each visualization type occupies an individual window, we convert two selected neighboring coordinate axes into a scatterplot directly. Multidimensional scaling is adopted to allow converting multiple axes into a single subplot. The transition between two visual types is designed in a seamless way. In our work, a series of interaction tools has been developed. Uniform brushing functionality is implemented to allow the user to perform data selection on both points and parallel coordinate polylines without explicitly switching tools. A GPU accelerated dimensional incremental multidimensional scaling (DIMDS) has been developed to significantly improve the system performance. Our case study shows that our scheme is more efficient than traditional multi-view methods in performing visual analysis tasks.},
author = {Yuan, X and Guo, P and Xiao, H and Zhou, H and Qu, H},
file = {:Users/smcgregor/Documents/Mendeley Desktop/05290705.pdf:pdf},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {6},
pages = {1001--1008},
publisher = {IEEE},
title = {{Scattering points in parallel coordinates}},
volume = {15},
year = {2009}
}
@article{Zadrozny,
author = {Zadrozny, Bianca and Langford, John and Abe, Naoki},
file = {:Users/smcgregor/Documents/Mendeley Desktop/zadrozny-langford-abe-cost-sensitive-learning-by-cost-proportionate-example-weighting.pdf:pdf},
journal = {Test},
title = {{Cost-Sensitive Learning by Cost-Proportionate Example Weighting}}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:Users/smcgregor/Documents/Mendeley Desktop/zeilerECCV2014.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {Computer Vision–ECCV 2014},
pages = {818--833},
title = {{Visualizing and understanding convolutional networks}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-10590-1_53},
volume = {8689},
year = {2014}
}
@book{zeller2009programs,
author = {Zeller, Andreas},
publisher = {Elsevier},
title = {{Why programs fail: a guide to systematic debugging}},
year = {2009}
}
@book{Zhang2006,
address = {Berlin, Heidelberg},
author = {Zhang, Ke-Bing and Orgun, Mehmet A. and Zhang, Kang},
doi = {10.1007/11811305},
editor = {Li, Xue and Za{\"{\i}}ane, Osmar R. and Li, Zhan-huai},
isbn = {978-3-540-37025-3},
month = {aug},
pages = {316--327},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Advanced Data Mining and Applications}},
url = {http://dl.acm.org/citation.cfm?id=2123665.2123704},
volume = {4093},
year = {2006}
}
@inproceedings{zhao2003voxels,
abstract = {We introduce a method for the animation of fire propagation and the burning consumption of objects represented as volumetric data sets. Our method uses a volumetric fire propagation model based on an enhanced distance field. It can simulate the spreading of multiple fire fronts over a specified isosurface without actually having to create that isosurface. The distance field is generated from a specific shell volume that rapidly creates narrow spatial bands around the virtual surface of any given isovalue. The complete distance field is then obtained by propagation from the initial bands. At each step multiple fire fronts can evolve simultaneously on the volumetric object. The flames of the fire are constructed from streams of particles whose movement is regulated by a velocity field generated with the hardware-accelerated Lattice Boltzmann Model (LBM). The LBM provides a physically-based simulation of the air flow around the burning object. The object voxels and the splats associated with the flame particles are rendered in the same pipeline so that the volume data with its external and internal structures can be displayed along with the fire.},
author = {Zhao, Y and Wei, X and Fan, Z and Kaufman, A and Qin, H},
booktitle = {Visualization, 2003. VIS 2003. IEEE},
organization = {IEEE},
pages = {271--278},
title = {{Voxels on fire [computer animation]}},
year = {2003}
}
@inproceedings{zhao2003voxels,
abstract = {We introduce a method for the animation of fire propagation and the burning consumption of objects represented as volumetric data sets. Our method uses a volumetric fire propagation model based on an enhanced distance field. It can simulate the spreading of multiple fire fronts over a specified isosurface without actually having to create that isosurface. The distance field is generated from a specific shell volume that rapidly creates narrow spatial bands around the virtual surface of any given isovalue. The complete distance field is then obtained by propagation from the initial bands. At each step multiple fire fronts can evolve simultaneously on the volumetric object. The flames of the fire are constructed from streams of particles whose movement is regulated by a velocity field generated with the hardware-accelerated Lattice Boltzmann Model (LBM). The LBM provides a physically-based simulation of the air flow around the burning object. The object voxels and the splats associated with the flame particles are rendered in the same pipeline so that the volume data with its external and internal structures can be displayed along with the fire.},
author = {Zhao, Y and Wei, X and Fan, Z and Kaufman, A and Qin, H},
booktitle = {Visualization, 2003. VIS 2003. IEEE},
organization = {IEEE},
pages = {271--278},
title = {{Voxels on fire [computer animation]}},
year = {2003}
}
@article{Zhao2003,
author = {Zhao, Ye},
file = {:Users/smcgregor/Documents/Mendeley Desktop/01250382.pdf:pdf},
title = {{Voxels on Fire}},
year = {2003}
}
@misc{,
keywords = {Error bounds,multi-task learning},
title = {{Learning from Multiple Sources}},
url = {http://www.cis.upenn.edu/$\sim$mkearns/papers/multisource-jmlr.pdf},
urldate = {2014-11-12}
}
@article{,
file = {:Users/smcgregor/Documents/Mendeley Desktop/carlson-doyle-highly-optimized-tolerance-a-mechanism-for-power-laws-in-designed-systems1999.pdf:pdf},
title = {{Highly Optimized Tolerance: A Mechanism for Power Laws in Designed Systems}}
}
@article{Dann2015,
abstract = {Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound $\tilde O(\frac{|\mathcal S|^2 |\mathcal A| H^2}{\epsilon^2} \ln\frac 1 \delta)$ and a lower PAC bound $\tilde \Omega(\frac{|\mathcal S| |\mathcal A| H^2}{\epsilon^2} \ln \frac 1 {\delta + c})$ that match up to log-terms and an additional linear dependency on the number of states $|\mathcal S|$. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency of at least $H^3$.},
archivePrefix = {arXiv},
arxivId = {1510.08906},
author = {Dann, Christoph and Brunskill, Emma},
eprint = {1510.08906},
file = {:Users/smcgregor/Documents/Mendeley Desktop/5827-sample-complexity-of-episodic-fixed-horizon-reinforcement-learning.pdf:pdf},
journal = {Neural Information Processing Systems},
pages = {1--9},
title = {{Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning}},
url = {http://arxiv.org/abs/1510.08906},
year = {2015}
}
@online{RLPy,
author = {Geramifard, Alborz and Klein, Robert H and Dann, Christoph and Dabney, William and How, Jonathan P},
howpublished = {\url{http://acl.mit.edu/RLPy}},
title = {{RLPy: The Reinforcement Learning Library for Education and Research}},
year = {2013}
}
@article{Obermaier2015,
author = {Obermaier, Harald and Bensema, Kevin and Joy, Kenneth},
doi = {10.1109/TVCG.2015.2507592},
file = {:Users/smcgregor/Documents/Mendeley Desktop/07352365.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {1},
pages = {1--1},
title = {{Visual Trends Analysis in Time-Varying Ensembles}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7352365},
volume = {6},
year = {2015}
}
@misc{Jones,
annote = {[Online; accessed 2016-02-27]},
author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and Others},
title = {{{SciPy}: Open source scientific tools for {Python}}},
url = {http://www.scipy.org/}
}
@article{powell1964efficient,
author = {Powell, Michael J D},
journal = {The computer journal},
number = {2},
pages = {155--162},
publisher = {Br Computer Soc},
title = {{An efficient method for finding the minimum of a function of several variables without calculating derivatives}},
volume = {7},
year = {1964}
}
@article{scikit-learn,
author = {Pedregosa, F and Varoquaux, G and Gramfort, A and Michel, V and Thirion, B and Grisel, O and Blondel, M and Prettenhofer, P and Weiss, R and Dubourg, V and Vanderplas, J and Passos, A and Cournapeau, D and Brucher, M and Perrot, M and Duchesnay, E},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in {P}ython}},
volume = {12},
year = {2011}
}
@book{omohundro1989five,
author = {Omohundro, Stephen Malvern},
publisher = {International Computer Science Institute Berkeley},
title = {{Five balltree construction algorithms}},
year = {1989}
}
@article{Ernst2006,
abstract = {This paper addresses the problem of computing optimal structured treatment interruption strategies for HIV infected patients. We show that reinforcement learning may be useful to extract such strategies directly from clinical data, without the need of an accurate mathematical model of HIV infection dynamics. To support our claims, we report simulation results obtained by running a recently proposed batch-mode reinforcement learning algorithm, known as fitted Q iteration, on numerically generated data},
author = {Ernst, Damien and Stan, {Guy-Bart} and Goncalves, Jorge and Wehenkel, Louis},
doi = {10.1109/CDC.2006.377527},
file = {:Users/smcgregor/Documents/Mendeley Desktop/04177178.pdf:pdf},
isbn = {1-4244-0171-2},
issn = {01912216},
journal = {Proceedings of the 45th IEEE Conference on Decision and Control},
pages = {667--672},
title = {{Clinical data based optimal STI strategies for HIV: a reinforcement learning approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4177178},
year = {2006}
}
@article{Obermaier2015,
author = {Obermaier, Harald and Bensema, Kevin and Joy, Kenneth},
doi = {10.1109/TVCG.2015.2507592},
file = {:Users/smcgregor/Documents/Mendeley Desktop/07352365.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {1},
pages = {1--1},
title = {{Visual Trends Analysis in Time-Varying Ensembles}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7352365},
volume = {6},
year = {2015}
}
@article{Booshehrian2012a,
abstract = {... Article first published online: 25 JUN 2012. DOI: 10.1111 / j . 1467 - 8659.2012 . 03116 . x . © 2012 The Author(s) Computer Graphics Forum © 2012 The Eurographics Association and Blackwell Publishing Ltd. Issue. Computer Graphics Forum. ... \n},
author = {Booshehrian, Maryam and M{\"{o}}ller, Torsten and Peterman, Randall M. and Munzner, Tamara},
doi = {10.1111/j.1467-8659.2012.03116.x},
file = {:Users/smcgregor/Documents/Mendeley Desktop/etd7610_MBooshehrian.pdf:pdf},
isbn = {01677055},
issn = {01677055},
journal = {Computer Graphics Forum},
number = {3},
pages = {1235--1244},
title = {{Vismon: Facilitating Analysis of Trade-Offs, Uncertainty, and Sensitivity In Fisheries Management Decision Making}},
url = {http://doi.wiley.com/10.1111/j.1467-8659.2012.03116.x},
volume = {31},
year = {2012}
}
@article{Jouhaud2007,
abstract = {A surrogate-model based shape optimization method is presented and applied to the case of the multidisciplinary shape optimization of a 2D NACA subsonic airfoil. The cost function is designed so that both the far-field radiated noise and the aerodynamic forces are controlled. The surrogate model is based on the Kriging optimal interpolation technique. In order to increase the efficiency of the method, a dynamic Kriging method is developed, which can be interpreted as an Adaptive Mesh Refinement method in the shape optimization parameters. ?? 2006 Elsevier Ltd. All rights reserved.},
author = {Jouhaud, J. C. and Sagaut, P. and Montagnac, M. and Laurenceau, J.},
doi = {10.1016/j.compfluid.2006.04.001},
file = {:Users/smcgregor/Documents/Mendeley Desktop/1-s2.0-S0045793006000582-main.pdf:pdf},
isbn = {0045-7930},
issn = {00457930},
journal = {Computers and Fluids},
number = {3},
pages = {520--529},
title = {{A surrogate-model based multidisciplinary shape optimization method with application to a 2D subsonic airfoil}},
volume = {36},
year = {2007}
}
@article{Liefooghe2008,
abstract = {Although many accounts of task switching emphasize the importance of working memory as a substantial source of the switch cost, there is a lack of evidence demonstrating that task switching actually places additional demands on working memory. The present study addressed this issue by implementing task switching in continuous complex span tasks with strictly controlled time parameters. A series of 4 experiments demonstrate that recall performance decreased as a function of the number of task switches and that the concurrent load of item maintenance had no influence on task switching. These results indicate that task switching induces a cost on working memory functioning. Implications for theories of task switching, working memory, and resource sharing are addressed},
author = {Liefooghe, Baptist and Barrouillet, Pierre and Vandierendonck, Andr{\'{e}} and Camos, Val{\'{e}}rie},
doi = {10.1037/0278-7393.34.3.478},
file = {:Users/smcgregor/Documents/Mendeley Desktop/10.1.1.379.4974.pdf:pdf},
isbn = {0278-7393 (Print)\r0278-7393 (Linking)},
issn = {1939-1285},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
keywords = {complex span tasks,resource sharing,task switching,working memory},
number = {3},
pages = {478--494},
pmid = {18444750},
title = {{Working memory costs of task switching.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.34.3.478},
volume = {34},
year = {2008}
}
@article{Miller1968,
abstract = {The literature concerning man-computer transactions abounds in controversy about the limits of "system response time" to a user's command or inquiry at a terminal. Two major semantic issues prohibit resolving this controversy. One issue centers around the question of "Response time to what?" The implication is that different human purposes and actions will have different acceptable or useful response times.},
author = {Miller, R.B.},
doi = {10.1145/1476589.1476628},
file = {:Users/smcgregor/Documents/Mendeley Desktop/50720267.pdf:pdf},
journal = {AFIPS'68 Proceedings of the December 9-11, 1968, fall joint computer conference, part I},
pages = {267--277},
title = {{Response time in man-computer conversational transactions}},
url = {http://portal.acm.org/citation.cfm?id=1476628},
year = {1968}
}
@article{Keim2008,
author = {Keim, Daniel and Andrienko, Gennady and Fekete, Jean-daniel and Carsten, G and Melan, Guy and Keim, Daniel and Andrienko, Gennady and Fekete, Jean-daniel and Carsten, G},
doi = {10.1007/978-3-540-70956-5_7},
file = {:Users/smcgregor/Documents/Mendeley Desktop/80.pdf:pdf},
isbn = {978-3-540-70955-8},
journal = {Information Visualization - Human-Centered Issues and Perspectives},
pages = {154--175},
title = {{Visual Analytics: Definition, Process, and Challenges}},
url = {http://hal-lirmm.ccsd.cnrs.fr/lirmm-00272779},
year = {2008}
}
@inproceedings{Authors2016,
author = {Authors, Anonymized},
booktitle = {(under review, see supplementary materials) IEEE Transactions on Visualization and Computer Graphics},
title = {{Fast MDP Visual Exploration via Trajectory Synthesis Using Learned Similarity Functions}},
year = {2016}
}
@article{Fonteneau2010c,
author = {Fonteneau, Raphael and Murphy, Susan A and Wehenkel, Louis and Ernst, Damien},
file = {:Users/smcgregor/Documents/Mendeley Desktop/Fonteneau2010AISTATS.pdf:pdf},
journal = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010)},
pages = {217--224},
title = {{Model-Free Monte Carlo-like Policy Evaluation}},
year = {2010}
}
@article{UnitedStatesForestDepartment2015,
author = {{United States Forest Department}},
file = {:Users/smcgregor/Documents/Mendeley Desktop/2015-Fire-Budget-Report.pdf:pdf},
journal = {United States Department of Agriculture},
pages = {13},
title = {{The Rising Cost of Fire Operations: Effects on the Forest Service’s Non-Fire Work}},
url = {http://www.fs.fed.us/sites/default/files/media/2014/34/nr-firecostimpact-082014.pdf},
year = {2015}
}
@article{Tephens2005,
author = {Tephens, Scott L and Ruth, Lawrence W},
file = {:Users/smcgregor/Documents/Mendeley Desktop/writing_assess_2013_stephens_and_ruth_2005_000.pdf:pdf},
journal = {Ecological Applications},
number = {March 2004},
pages = {532--542},
title = {{Federal Forest-Fire Policy in the United States}},
volume = {15},
year = {2005}
}

